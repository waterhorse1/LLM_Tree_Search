{
    "600": {
        "file_id": 71,
        "content": "/tsllm/offline_rl/generate_data.py",
        "type": "filepath"
    },
    "601": {
        "file_id": 71,
        "content": "This code converts Llama models, generates responses, and compares them with ground truth using ThreadPoolExecutor for efficiency. It also sets command line arguments for environment name, GPU usage, batch size, output directory, tokenizer path, and test mode.",
        "type": "summary"
    },
    "602": {
        "file_id": 71,
        "content": "from pathlib import Path\nfrom typing import Callable\nimport ctranslate2\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nimport sentencepiece as spm\nimport os\nfrom tqdm import tqdm\nimport argparse\nimport jsonlines\nfrom tsllm.distributed.utils import print_rank_0\nfrom tsllm.llm.text_generation import llm_gen_ct2\nfrom tsllm.llm.ct2_utils import OnlineHfConverter\nfrom tsllm.envs import get_env_datasets, get_default_query_str_builder, get_env_answer_checker\nfrom tsllm.argparse_utils import list_of_ints, str2bool\nfrom importlib import import_module\ndef _cot_gen(\n    ct2_generator,\n    tokenizer,\n    query_str_build_fn: Callable,\n    problem,\n    n=100,\n    stop=2,\n    max_new_tokens=256,\n    **kwargs,\n):\n    # prompt = \"Question: \" + problem[\"question\"] + \"\\nAnswer: Let's think step by step\\n\"\n    # if use_prefix:\n    #     prompt = prefix + \"\\n\" + prompt\n    prompt = query_str_build_fn(problem[\"question\"])",
        "type": "code",
        "location": "/tsllm/offline_rl/generate_data.py:1-34"
    },
    "603": {
        "file_id": 71,
        "content": "This code is importing necessary libraries and defining a function called `_cot_gen`. The function takes in a ct2 generator, tokenizer, query string builder, problem, number of outputs, stop sequence, max new tokens, and optional keyword arguments. It generates responses using the given parameters and a provided prompt based on the problem's question.",
        "type": "comment"
    },
    "604": {
        "file_id": 71,
        "content": "    texts, logps = llm_gen_ct2(\n        ct2_generator,\n        tokenizer,\n        static_prompt=None,\n        prompt=prompt,\n        num_sequence=n,\n        stop=stop,\n        max_new_tokens=max_new_tokens,\n        **kwargs,\n    )\n    return texts\n\"\"\"convert hf model to ct2 model\"\"\"\n# base_llm_dir = \"huggyllama/llama-7b\"\n# cache_dir =\n# ct2_dir =\n# base_llm = AutoModelForCausalLM.from_pretrained(base_llm_dir,\n#                                                 cache_dir=cache_dir)\n# tokenizer = AutoTokenizer.from_pretrained(base_llm_dir, cache_dir=cache_dir)\n# cvt = OnlineHfConverter(model=base_llm,\n#                         model_name_or_path=base_llm_dir,\n#                         copy_files=[\"tokenizer.model\"])\n# cvt.convert(ct2_dir, force=True, quantization=\"bfloat16\")\ndef check_answers(check_fn, problem_inst, texts):\n    # groundtruth = extract_groundtruth(problem_inst[\"answer\"])\n    write_obj = {\n        \"question\": problem_inst[\"question\"],\n        \"groundtruth\": problem_inst[\"answer\"],\n    }\n    ans_list = []",
        "type": "code",
        "location": "/tsllm/offline_rl/generate_data.py:35-70"
    },
    "605": {
        "file_id": 71,
        "content": "The code converts a Hugging Face LLama model to CT2 format, generates texts using the generated CT2 model, and checks answers against ground truth. It initializes an AutoModelForCausalLM from pre-trained model, tokenizer, uses OnlineHfConverter to convert the model to CT2 format, and defines a check_answers function to compare generated responses with ground truth for each problem instance.",
        "type": "comment"
    },
    "606": {
        "file_id": 71,
        "content": "    cnt = 0\n    for txt in texts:\n        correct = check_fn(problem_inst[\"question\"], problem_inst[\"answer\"], txt)\n        if correct:\n            cnt += 1\n        ans_list.append({\"text\": txt, \"correct\": correct})\n    write_obj[\"answer\"] = ans_list\n    return write_obj, cnt, len(texts)\ndef main(args):\n    train_ds, test_ds = get_env_datasets(args.env_name)\n    if args.test:\n        ds = test_ds\n    else:\n        ds = train_ds\n    args.output_path = Path(args.output_path)\n    if not args.output_path.parent.exists():\n        args.output_path.parent.mkdir(parents=True)\n    ct2_generator = ctranslate2.Generator(\n        args.ct2_dir, device=\"cuda\", device_index=args.gpu_ids, compute_type=\"bfloat16\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n    print(\n        \"LOADING CT2 MODEL AT: {}. DEVICES={}.\\nOUTPUT_DIR: {}\".format(\n            args.ct2_dir, args.gpu_ids, args.output_path\n        )\n    )\n    query_str_build_fn = partial(\n        get_default_query_str_builder(args.env_name), is_few_shot=args.is_few_shot",
        "type": "code",
        "location": "/tsllm/offline_rl/generate_data.py:72-107"
    },
    "607": {
        "file_id": 71,
        "content": "The code defines a function that takes a list of texts and checks each text against a given problem instance, determining whether it is correct or not. The function appends the correct texts to an answer list and returns this along with counts for correct answers and total number of texts. The main function loads data and generates an output directory if necessary. It then initializes a ctranslate2 generator and tokenizer, and prints out information about their location and device usage. A query string builder is also defined based on the environment name.",
        "type": "comment"
    },
    "608": {
        "file_id": 71,
        "content": "    )\n    cot_gen = partial(\n        _cot_gen,\n        ct2_generator,\n        tokenizer,\n        query_str_build_fn,\n        n=args.k,\n        stop=tokenizer.eos_token_id,  # llama tokenizer: 2 is eos_token_id\n        max_new_tokens=256,\n        temperature=args.t,\n        top_p=1,\n        top_k=100,\n        max_batch_size=args.max_batch_size,\n    )\n    checker_fn = get_env_answer_checker(args.env_name)\n    correct_num, total_num = 0, 0\n    with ThreadPoolExecutor(args.num_workers) as pool:\n        results = pool.map(cot_gen, ds)\n        with jsonlines.open(args.output_path, \"w\") as writer:\n            for i, txts in enumerate(pbar := tqdm(results, total=len(ds))):\n                write_obj, cnt, len_list = check_answers(checker_fn, ds[i], txts)\n                writer.write(write_obj)\n                correct_num += cnt\n                total_num += len_list\n                pbar.set_description(\n                    \"{}-corrent: {:.3%}[{}/{}]\".format(\n                        i + 1, correct_num / total_num, correct_num, total_num",
        "type": "code",
        "location": "/tsllm/offline_rl/generate_data.py:108-135"
    },
    "609": {
        "file_id": 71,
        "content": "This code generates data for an environment using LLAMa, a large language model. It utilizes a ThreadPoolExecutor to generate results from a dataset (ds) and checks answers using a given checker function. The results are then written to a file specified by args.output_path. The progress is tracked in a tqdm progress bar.",
        "type": "comment"
    },
    "610": {
        "file_id": 71,
        "content": "                    )\n                )\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    # env/task config\n    parser.add_argument(\"--is_few_shot\", type=str2bool, default=False)\n    parser.add_argument(\"--test\", type=str2bool, default=False)\n    parser.add_argument(\"--env_name\", type=str, required=True)\n    parser.add_argument(\"-k\", type=int, default=1)\n    parser.add_argument(\"-t\", type=float, default=0.7)\n    parser.add_argument(\"--ct2_dir\", type=str, required=True)\n    parser.add_argument(\"--tokenizer_path\", type=str, required=True)\n    parser.add_argument(\"--output_path\", type=str, required=True)\n    parser.add_argument(\n        \"--gpu_ids\", type=list_of_ints, default=[0, 1, 2, 3, 4, 5, 6, 7]\n    )\n    parser.add_argument(\"--max_batch_size\", type=int, default=50)\n    parser.add_argument(\"--num_workers\", type=int, default=8)\n    args = parser.parse_args()\n    main(args)",
        "type": "code",
        "location": "/tsllm/offline_rl/generate_data.py:136-160"
    },
    "611": {
        "file_id": 71,
        "content": "This code is setting up command line arguments for the program. It requires an environment name, specifies a few default values (e.g., few-shot learning is disabled by default), defines several optional parameters such as number of GPUs to use and batch size, and then calls the main function with these arguments. The code also includes options for an output directory, tokenizer path, and whether or not the program should run in test mode.",
        "type": "comment"
    },
    "612": {
        "file_id": 72,
        "content": "/tsllm/offline_rl/gsm8k_data/gen_3.sh",
        "type": "filepath"
    },
    "613": {
        "file_id": 72,
        "content": "The code executes a Python script named 'generate_data.py' three times with different parameters for each run. The aim is to generate data for an environment called gsm8k, and the generated data will be stored in different JSONL files. The code uses different CT2 directories for each run, indicating that it is generating data for different episodes of the gsm8k environment.",
        "type": "summary"
    },
    "614": {
        "file_id": 72,
        "content": "set -e\nK=100\nT=0.7\nN_WORKER=16\nOUTPUT_DIR=./gsm8k_data/cot_sample/\nCUDA_DEVICES=0,1,2,3,4,5,6,7\nCT2_CACHE=$1\nTOKENIZER_PATH=$2\npython generate_data.py \\\n    -k $K \\\n    -t $T \\\n    --num_workers $N_WORKER \\\n    --gpu_ids $CUDA_DEVICES \\\n    --ct2_dir ${CT2_CACHE}/llama2_sft_ep1_ct2 \\\n    --tokenizer_path $TOKENIZER_PATH \\\n    --output_path ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k${K}_ep1.jsonl \\\n    --env_name gsm8k\npython generate_data.py \\\n    -k $K \\\n    -t $T \\\n    --num_workers $N_WORKER \\\n    --gpu_ids $CUDA_DEVICES \\\n    --ct2_dir ${CT2_CACHE}/llama2_sft_ep2_ct2 \\\n    --tokenizer_path $TOKENIZER_PATH \\\n    --output_path ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k${K}_ep2.jsonl \\\n    --env_name gsm8k\npython generate_data.py \\\n    -k $K \\\n    -t $T \\\n    --num_workers $N_WORKER \\\n    --gpu_ids $CUDA_DEVICES \\\n    --ct2_dir ${CT2_CACHE}/llama2_sft_ep3_ct2 \\\n    --tokenizer_path $TOKENIZER_PATH \\\n    --output_path ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k${K}_ep3.jsonl \\\n    --env_name gsm8k",
        "type": "code",
        "location": "/tsllm/offline_rl/gsm8k_data/gen_3.sh:1-41"
    },
    "615": {
        "file_id": 72,
        "content": "The code executes a Python script named 'generate_data.py' three times with different parameters for each run. The aim is to generate data for an environment called gsm8k, and the generated data will be stored in different JSONL files. The code uses different CT2 directories for each run, indicating that it is generating data for different episodes of the gsm8k environment.",
        "type": "comment"
    },
    "616": {
        "file_id": 73,
        "content": "/tsllm/offline_rl/gsm8k_data/process.sh",
        "type": "filepath"
    },
    "617": {
        "file_id": 73,
        "content": "This script processes GSM8K data, deduplicates and samples data files, creates three output episodes named with \"gsm8k_train_cot_sample_offline_sft_k100_ep\". It uses split_two_test.py for splitting into training and testing sets, and merge.py for merging training files. The scripts utilize environment variables such as ${OUTPUT_DIR}, ${file_prefix}, $N, sample17x3.jsonl.",
        "type": "summary"
    },
    "618": {
        "file_id": 73,
        "content": "set -e\nINPUT_DIR=\"gsm8k_data/cot_sample\"\nOUTPUT_DIR=\"gsm8k_data/processed\"\nmkdir -p $OUTPUT_DIR\nN=17\nfile_prefix=\"gsm8k_train_cot_sample_offline_sft_k100_ep1\"\npython dedup.py \\\n    --input_path ${INPUT_DIR}/${file_prefix}.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl\npython sample.py \\\n    --input_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup_sample${N}.jsonl \\\n    -n $N\nfile_prefix=\"gsm8k_train_cot_sample_offline_sft_k100_ep2\"\npython dedup.py \\\n    --input_path ${INPUT_DIR}/${file_prefix}.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl\npython sample.py \\\n    --input_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup_sample${N}.jsonl \\\n    -n $N\nfile_prefix=\"gsm8k_train_cot_sample_offline_sft_k100_ep3\"\npython dedup.py \\\n    --input_path ${INPUT_DIR}/${file_prefix}.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl\n# split_two_test.py will choose files encswith \"dedup\"",
        "type": "code",
        "location": "/tsllm/offline_rl/gsm8k_data/process.sh:1-32"
    },
    "619": {
        "file_id": 73,
        "content": "This script processes GSM8K data by deduplicating and sampling data files. It creates processed output files for three different episodes, each containing deduplicated and sampled data. The resulting files are named with the prefix \"gsm8k_train_cot_sample_offline_sft_k100_ep\" followed by the episode number, deduped, and then sampled with a sample size of 17. These files will be chosen by split_two_test.py if their names contain \"dedup\".",
        "type": "comment"
    },
    "620": {
        "file_id": 73,
        "content": "python split_two_test.py \\\n    --train_data_prefix ${OUTPUT_DIR}/${file_prefix} \\\n    --train_num $N \\\n    --train_test_num 3\npython merge.py \\\n    --input_paths ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k100_ep1_dedup_sample17.jsonl \\\n    ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k100_ep2_dedup_sample17.jsonl \\\n    ${OUTPUT_DIR}/gsm8k_train_cot_sample_offline_sft_k100_ep3_dedup_sample17.jsonl \\\n    --output_path ${OUTPUT_DIR}/gsm8k_train_cot_sample_sft_k100_merged_dedup_sample17x3.jsonl",
        "type": "code",
        "location": "/tsllm/offline_rl/gsm8k_data/process.sh:33-42"
    },
    "621": {
        "file_id": 73,
        "content": "This code executes two Python scripts: \"split_two_test.py\" and \"merge.py\". The former splits the data into training and testing sets, and the latter merges three training files into one. It uses environment variables such as ${OUTPUT_DIR}, ${file_prefix}, $N, and sample17x3.jsonl.",
        "type": "comment"
    },
    "622": {
        "file_id": 74,
        "content": "/tsllm/offline_rl/merge.py",
        "type": "filepath"
    },
    "623": {
        "file_id": 74,
        "content": "This code merges and deduplicates JSONL question-answer pairs, filters duplicate answers, and outputs the final result to a new JSONL file.",
        "type": "summary"
    },
    "624": {
        "file_id": 74,
        "content": "import json\nfrom argparse import ArgumentParser\nfrom tsllm.argparse_utils import str2bool\nfrom tsllm.offline_rl.utils import load_jsonl, write_to_jsonl\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--input_paths\", action=\"store\", nargs=\"+\")\n    parser.add_argument(\"--output_path\", type=str)\n    parser.add_argument(\"--deduplicate\", type=str2bool, default=True)\n    args = parser.parse_args()\n    print(args.input_paths)\n    data = []\n    for input_path in args.input_paths:\n        data.extend(load_jsonl(input_path))\n    d_by_q = {}\n    for d in data:\n        q_str = d[\"question\"]\n        if q_str not in d_by_q:\n            d_by_q[q_str] = {\n                \"question\": q_str,\n                \"answer\": d[\"answer\"],\n                \"groundtruth\": d.get(\"groundtruth\", \"[DUMMY]\"),\n            }\n        else:\n            d_by_q[q_str][\"answer\"].extend(d[\"answer\"])\n    if args.deduplicate:\n        cnt, total_cnt, correct_cnt = 0, 0, 0\n        data = list(d_by_q.values())\n        print(len(data))",
        "type": "code",
        "location": "/tsllm/offline_rl/merge.py:1-33"
    },
    "625": {
        "file_id": 74,
        "content": "This code reads in multiple JSONL files containing question-answer pairs, merges them into a single dictionary (d_by_q), and deduplicates the questions. It also provides an option to output the final result to a new JSONL file.",
        "type": "comment"
    },
    "626": {
        "file_id": 74,
        "content": "        for d in data:\n            answer = d[\"answer\"]\n            unique_data = []\n            seen_texts = []\n            for item in answer:\n                if item[\"text\"] not in seen_texts:\n                    seen_texts.append(item[\"text\"])\n                    unique_data.append(item)\n                    cnt += 1\n                    if item[\"correct\"]:\n                        correct_cnt += 1\n                total_cnt += 1\n            d[\"answer\"] = unique_data\n        print(cnt, total_cnt, correct_cnt)\n    write_to_jsonl(data, args.output_path)",
        "type": "code",
        "location": "/tsllm/offline_rl/merge.py:34-49"
    },
    "627": {
        "file_id": 74,
        "content": "This code filters duplicate texts from the \"answer\" list in each dictionary of a data list. It counts and prints total, correct, and unique answer count before writing the modified data to a JSONL file.",
        "type": "comment"
    },
    "628": {
        "file_id": 75,
        "content": "/tsllm/offline_rl/prontoqa/gen_3.sh",
        "type": "filepath"
    },
    "629": {
        "file_id": 75,
        "content": "The code is setting environment variables, defining parameters (K, T, N_WORKER), specifying output and cache directories, and running a Python script (generate_data.py) to generate data for offline RL with ProntoQA. The Python script uses LLama2 model, SFTP, tokenizer, and generates output in JSONL format for training.",
        "type": "summary"
    },
    "630": {
        "file_id": 75,
        "content": "set -e\nK=100\nT=0.7\nN_WORKER=16\nOUTPUT_DIR=./prontoqa/cot_sample/\nCUDA_DEVICES=0,1,2,3,4,5,6,7\nCT2_CACHE=$1\nTOKENIZER_PATH=$2\npython generate_data.py \\\n    -k $K \\\n    -t $T \\\n    --num_workers $N_WORKER \\\n    --gpu_ids $CUDA_DEVICES \\\n    --ct2_dir ${CT2_CACHE}/llama2_sft_ep1_ct2 \\\n    --tokenizer_path $TOKENIZER_PATH \\\n    --output_path ${OUTPUT_DIR}/prontoqa_train_cot_sample_offline_sft_k${K}_ep1.jsonl \\\n    --env_name prontoqa",
        "type": "code",
        "location": "/tsllm/offline_rl/prontoqa/gen_3.sh:1-20"
    },
    "631": {
        "file_id": 75,
        "content": "The code is setting environment variables, defining parameters (K, T, N_WORKER), specifying output and cache directories, and running a Python script (generate_data.py) to generate data for offline RL with ProntoQA. The Python script uses LLama2 model, SFTP, tokenizer, and generates output in JSONL format for training.",
        "type": "comment"
    },
    "632": {
        "file_id": 76,
        "content": "/tsllm/offline_rl/prontoqa/process.sh",
        "type": "filepath"
    },
    "633": {
        "file_id": 76,
        "content": "The code sets up a directory, creates a dedup file, and then splits it into training and testing data for offline RL.",
        "type": "summary"
    },
    "634": {
        "file_id": 76,
        "content": "set -e\nINPUT_DIR=\"prontoqa/cot_sample\"\nOUTPUT_DIR=\"prontoqa/processed\"\nmkdir -p $OUTPUT_DIR\nN=50\nfile_prefix=\"prontoqa_train_cot_sample_offline_sft_k100_ep1\"\npython dedup.py \\\n    --input_path ${INPUT_DIR}/${file_prefix}.jsonl \\\n    --output_path ${OUTPUT_DIR}/${file_prefix}_dedup.jsonl\n# split_two_test.py will choose files encswith \"dedup\"\npython split_two_test.py \\\n    --train_data_prefix ${OUTPUT_DIR}/${file_prefix} \\\n    --train_num $N \\\n    --train_test_num 3",
        "type": "code",
        "location": "/tsllm/offline_rl/prontoqa/process.sh:1-16"
    },
    "635": {
        "file_id": 76,
        "content": "The code sets up a directory, creates a dedup file, and then splits it into training and testing data for offline RL.",
        "type": "comment"
    },
    "636": {
        "file_id": 77,
        "content": "/tsllm/offline_rl/rlhf/gen_3.sh",
        "type": "filepath"
    },
    "637": {
        "file_id": 77,
        "content": "This script sets environment variables and runs a test for RLHF using specific paths, cache, and settings. It uses 8 processes, tokenizer path, CT2 cache, critic model path, save directory, environment name (rlhf), and trains the model.",
        "type": "summary"
    },
    "638": {
        "file_id": 77,
        "content": "set -e\n# export TEST_NO_TERMINAL=1\n# export TEST_WITH_TERMINAL=1\n# export TEST_COT_GREEDY=1\nexport TEST_COT_SC=1\n# export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nCT2_CACHE=$1\nTOKENIZER_PATH=$2\n# just None critic because we only use cot-sc sample\nCRITIC_PATH=\"None\"\n# also don't forget to set k_maj as 50 in test_sft_and_v_rlhf.py\ntorchrun --nproc_per_node=8 --master-port 29503 ../test_sft_and_v_rlhf.py \\\n    --critic_model_path $CRITIC_PATH \\\n    --tokenizer_path $TOKENIZER_PATH \\\n    --ct2_dir $CT2_CACHE \\\n    --save_dir ./rlhf/cot_sample \\\n    --env_name rlhf\n    --train",
        "type": "code",
        "location": "/tsllm/offline_rl/rlhf/gen_3.sh:1-21"
    },
    "639": {
        "file_id": 77,
        "content": "This script sets environment variables and runs a test for RLHF using specific paths, cache, and settings. It uses 8 processes, tokenizer path, CT2 cache, critic model path, save directory, environment name (rlhf), and trains the model.",
        "type": "comment"
    },
    "640": {
        "file_id": 78,
        "content": "/tsllm/offline_rl/rlhf/process.py",
        "type": "filepath"
    },
    "641": {
        "file_id": 78,
        "content": "This code loads and merges data from JSONL files, allowing users to specify input and output directories through command-line arguments. It processes all 8 JSONL files in the input directory, creating a new JSONL file in the specified output directory with each entry separated by newline characters.",
        "type": "summary"
    },
    "642": {
        "file_id": 78,
        "content": "import json\nfrom argparse import ArgumentParser\ndef load_jsonl(file_path):\n    data_list = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            data = json.loads(line.strip())\n            data_list.append(data)\n    return data_list\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--input_dir\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    config = parser.parse_args()\n    all_data = []\n    for i in range(8):\n        # merge all data, you can modify the path to your save path\n        all_data.extend(load_jsonl(f\"{config.input_dir}/rlhf/args0/cot_sc/{i}.jsonl\"))\n    d = []\n    for data_dict in all_data:\n        question = data_dict['prompt']\n        answer_list = []\n        for o in data_dict['output']:\n            answer_list.append({'text': o[0], 'reward':o[1]})\n        data_dict_new = {'question': question, 'answer': answer_list}\n        d.append(data_dict_new)\n    save_dir = config.output_dir + \"/rlhf_data.jsonl\"",
        "type": "code",
        "location": "/tsllm/offline_rl/rlhf/process.py:1-32"
    },
    "643": {
        "file_id": 78,
        "content": "This code loads JSONL files, merges data into a list, and then formats it for saving. The user specifies the input and output directories using command-line arguments, and the code processes all 8 JSONL files in the input directory to create a new JSONL file in the specified output directory.",
        "type": "comment"
    },
    "644": {
        "file_id": 78,
        "content": "    with open(save_dir, 'w') as file:\n        for data in d:\n            json_str = json.dumps(data)\n            file.write(json_str + '\\n')",
        "type": "code",
        "location": "/tsllm/offline_rl/rlhf/process.py:33-36"
    },
    "645": {
        "file_id": 78,
        "content": "Writes data to file in JSON format with newline character separating each entry.",
        "type": "comment"
    },
    "646": {
        "file_id": 79,
        "content": "/tsllm/offline_rl/rlhf/process.sh",
        "type": "filepath"
    },
    "647": {
        "file_id": 79,
        "content": "This code sets up environment variables, creates an output directory if it doesn't exist, and runs a Python script to process input files in the specified directory. The processed data is saved in the output directory.",
        "type": "summary"
    },
    "648": {
        "file_id": 79,
        "content": "set -e\nINPUT_DIR=\"./rlhf/cot_sample\"\nOUTPUT_DIR=\"./rlhf/processed\"\nmkdir -p $OUTPUT_DIR\npython3 process.py --input_dir=$INPUT_DIR --output_dir=$OUTPUT_DIR",
        "type": "code",
        "location": "/tsllm/offline_rl/rlhf/process.sh:1-6"
    },
    "649": {
        "file_id": 79,
        "content": "This code sets up environment variables, creates an output directory if it doesn't exist, and runs a Python script to process input files in the specified directory. The processed data is saved in the output directory.",
        "type": "comment"
    },
    "650": {
        "file_id": 80,
        "content": "/tsllm/offline_rl/sample.py",
        "type": "filepath"
    },
    "651": {
        "file_id": 80,
        "content": "This Python script takes a JSONL file (--input_path) as input, samples 'n' number of items randomly from each answer in the input data, and writes the modified data to another JSONL file (--output_path). The sampled answers are used for training or testing purposes.",
        "type": "summary"
    },
    "652": {
        "file_id": 80,
        "content": "import json\nimport random\nfrom argparse import ArgumentParser\nfrom tsllm.offline_rl.utils import load_jsonl, write_to_jsonl\ndef sample_dicts(data, n):\n    if n > len(data):\n        n = len(data)\n    return random.sample(data, n)\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--input_path\", type=str)\n    parser.add_argument(\"--output_path\", type=str)\n    parser.add_argument(\"-n\", type=int, default=10)\n    args = parser.parse_args()\n    data = load_jsonl(args.input_path)\n    for d in data:\n        d[\"answer\"] = sample_dicts(d[\"answer\"], args.n)\n    write_to_jsonl(data, args.output_path)",
        "type": "code",
        "location": "/tsllm/offline_rl/sample.py:1-25"
    },
    "653": {
        "file_id": 80,
        "content": "This Python script takes a JSONL file (--input_path) as input, samples 'n' number of items randomly from each answer in the input data, and writes the modified data to another JSONL file (--output_path). The sampled answers are used for training or testing purposes.",
        "type": "comment"
    },
    "654": {
        "file_id": 81,
        "content": "/tsllm/offline_rl/split_two_test.py",
        "type": "filepath"
    },
    "655": {
        "file_id": 81,
        "content": "The code loads JSONL data, splits it into training and evaluation sets, and samples specified numbers of items for the training set using ArgumentParser. It then deduplicates and writes train and test data to JSONL files with specific prefixes.",
        "type": "summary"
    },
    "656": {
        "file_id": 81,
        "content": "import copy\nimport json\nimport random\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nfrom tsllm.offline_rl.utils import load_jsonl, write_to_jsonl\ndef sample_dicts(data, n):\n    if n > len(data):\n        n = len(data)\n    return random.sample(data, n)\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--train_num\", type=int, default=20)\n    parser.add_argument(\"--train_test_num\", type=int, default=2)\n    parser.add_argument(\"--train_data_prefix\", type=str)\n    args = parser.parse_args()\n    train_data = load_jsonl(f\"{args.train_data_prefix}_dedup.jsonl\")\n    train_data_eval = copy.deepcopy(train_data)\n    for d1, d2 in zip(train_data, train_data_eval):\n        picked_ans = sample_dicts(d1[\"answer\"], args.train_num + args.train_test_num)\n        if len(picked_ans) > args.train_num:\n            d1[\"answer\"] = picked_ans[: args.train_num]\n            d2[\"answer\"] = picked_ans[args.train_num :]\n        else:\n            d1[\"answer\"] = picked_ans\n            d2[\"answer\"] = []",
        "type": "code",
        "location": "/tsllm/offline_rl/split_two_test.py:1-34"
    },
    "657": {
        "file_id": 81,
        "content": "This code loads JSONL data, splits it into training and evaluation sets, and samples a specified number of items for the training set. It uses the ArgumentParser to take input parameters such as train_num and train_data_prefix. The loaded data is then split into two parts: one with answers included and another with empty answers.",
        "type": "comment"
    },
    "658": {
        "file_id": 81,
        "content": "    write_to_jsonl(\n        train_data, f\"{args.train_data_prefix}_dedup_sample{args.train_num}.jsonl\"\n    )\n    write_to_jsonl(\n        train_data_eval,\n        f\"{args.train_data_prefix}_dedup_sample{args.train_num}_train_test_sample_{args.train_test_num}.jsonl\",\n    )",
        "type": "code",
        "location": "/tsllm/offline_rl/split_two_test.py:36-42"
    },
    "659": {
        "file_id": 81,
        "content": "Writes train and test data to JSONL files with specific prefixes for deduplicated samples.",
        "type": "comment"
    },
    "660": {
        "file_id": 82,
        "content": "/tsllm/offline_rl/test_sft_and_v.py",
        "type": "filepath"
    },
    "661": {
        "file_id": 82,
        "content": "The code imports necessary modules, defines function for accuracy evaluation, and configures hyperparameters. It uses distributed learning techniques such as MCTS, RAP, and Beam Search, and saves output as specified writers or JSON files. Additionally, it tests scenarios, generates logs, raises ValueError, updates a dictionary of correct counts for a distributed system, gathers results from each process, prints with timing information, and handles both integer and nested dictionary values.",
        "type": "summary"
    },
    "662": {
        "file_id": 82,
        "content": "from pathlib import Path\nfrom typing import Dict, List, Optional\nimport torch.distributed as dist\nfrom tsllm.argparse_utils import str2bool\nfrom tsllm.distributed.utils import (\n    print_rank_0,\n    print_with_rank,\n    init_distributed,\n    gather_scalar,\n)\nfrom tsllm.envs import get_env_datasets, get_default_query_str_builder\nfrom tsllm.inference.trajectory_collector import _mcts_rollout_v1\nfrom tsllm.inference.value import value_fn\nfrom tsllm.inference.lm_self_value import tot_value_fn\nfrom tsllm.llm.ct2_utils import load_ct2_model\nfrom tsllm.mcts.utils import get_root\nfrom tsllm.model import load_critic_model\nfrom tsllm.llm.text_generation import llm_gen_ct2\nfrom tsllm.mcts.tree import MCTS\nfrom tsllm.inference.evaluation.vote_utils import (\n    AGG_FN_MAP,\n    MAJORITY_VOTE,\n    ORM_VOTE,\n    ORM_MAX,\n)\nfrom tsllm.envs.base_env import INVALID_ANS\nfrom transformers import AutoTokenizer\nimport torch\nfrom functools import partial\nimport json\nimport jsonlines\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nfrom dataclasses import dataclass",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:1-35"
    },
    "663": {
        "file_id": 82,
        "content": "The code imports necessary modules and functions for distributed learning, environment configurations, model loading, text generation, MCTS tree, evaluation voting methods, tokenization, torch tensors, json handling, progress bars, and dataclasses.",
        "type": "comment"
    },
    "664": {
        "file_id": 82,
        "content": "from argparse import ArgumentParser\nimport os\nimport importlib\nimport random\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\nCHOSEN_AGGR_METHODS = [MAJORITY_VOTE, ORM_VOTE, ORM_MAX]\ndef judge_ans(\n    problem_str: str,\n    extracted_groundtruth: str,\n    output_list: List[str],\n    v_list: List[float],\n    aggration_mode: str,\n    extract_answer_fn,\n    judge_correct_fn,\n):\n    ans_list = [extract_answer_fn(txt) for txt in output_list]\n    valid_ans_list, valid_v_list = [], []\n    for i, ans in enumerate(ans_list):\n        if ans != INVALID_ANS:\n            valid_ans_list.append(ans)\n            valid_v_list.append(v_list[i])\n    if len(valid_ans_list) == 0:\n        return 0\n    # score_normalization: this is only necessary for [-1, 1] values\n    valid_v_list = np.array(valid_v_list, dtype=float)\n    valid_v_list -= valid_v_list.min()\n    valid_v_list /= valid_v_list.max() + 1e-3",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:36-76"
    },
    "665": {
        "file_id": 82,
        "content": "The code defines a function `judge_ans` for evaluating the accuracy of extracted answers. It takes in various parameters such as `problem_str`, `extracted_groundtruth`, `output_list`, and `v_list`. The function extracts answers from the `output_list`, filters out invalid answers, normalizes the scores, and applies an aggregation mode (`aggration_mode`) to determine the final result.",
        "type": "comment"
    },
    "666": {
        "file_id": 82,
        "content": "    valid_v_list = valid_v_list.tolist()\n    aggregated_ans = AGG_FN_MAP[aggration_mode](valid_ans_list, valid_v_list)\n    return (\n        1 if judge_correct_fn(problem_str, extracted_groundtruth, aggregated_ans) else 0\n    )\ndef get_correct_proportion(\n    problem_str: str,\n    extracted_groundtruth: str,\n    output_list: List[str],\n    extract_answer_fn,\n    judge_correct_fn,\n):\n    correct_list = [\n        1.0\n        if judge_correct_fn(problem_str, extracted_groundtruth, extract_answer_fn(txt))\n        else 0.0\n        for txt in output_list\n    ]\n    if len(correct_list) > 0:\n        return np.mean(correct_list).item()\n    else:\n        return 0.0\n@dataclass\nclass SearchArgs:\n    # temperature used for llm generation in CoT(-SC) and MCTS tree expansion\n    temperature: float = 1.0\n    # COT-SC number\n    k_maj: int = 100\n    # MCTS aggregation number\n    num_mcts_aggregation: int = 5\n    # which tree search methods to use\n    #  [\"mcts.get_next_action\", \"mcts.rap\", \"mcts.rollout\", \"dfs\", \"bfs\"]\n    # \"mcts.get_next_action\" is MCTS-$\\alpha$ in paper",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:77-116"
    },
    "667": {
        "file_id": 82,
        "content": "This code snippet defines a function `get_correct_proportion` that takes in parameters like `problem_str`, `extracted_groundtruth`, `output_list`, `extract_answer_fn`, and `judge_correct_fn`. It uses the `judge_correct_fn` to determine if an answer is correct, creating a list of 1s or 0s based on the correctness. The function then calculates the mean of the list to find the correct proportion.",
        "type": "comment"
    },
    "668": {
        "file_id": 82,
        "content": "    # \"mcts.rap\" is MCTS in paper\n    # \"mcts.rollout\" is MCTS-Rollout in paper\n    rollout_method: str = None\n    # Tree Search building configs\n    max_length: int = 8\n    max_action: int = 6\n    # general mcts hyperparameters for MCTS-alpha, MCTS, MCTS-Rollout\n    # Tree basic configs\n    pb_c_init: float = 10\n    # MCTS-alpha hyperparamerters\n    num_simulations: int = 10\n    reset_total_tree: bool = False\n    mcts_sample: bool = False\n    clear_tree: bool = False\n    # MCTS-Rollout Hyperparameters\n    max_simulation: Optional[int] = None\n    max_token: Optional[int] = None\n    # DFS hyperparameters\n    prune_ratio: Optional[float] = None\n    prune_value: Optional[float] = None\n    # if set method to be mcts.rap and set this to be True, then\n    #  it samples with llm's prior on the tree space, which is\n    #  CoT-SC-Tree\n    select_by_prior: bool = False\n    seed: int = 7\nif __name__ == \"__main__\":\n    TEST_NO_TERMINAL = int(os.getenv(\"TEST_NO_TERMINAL\", 0))\n    TEST_WITH_TERMINAL = int(os.getenv(\"TEST_WITH_TERMINAL\", 0))",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:117-153"
    },
    "669": {
        "file_id": 82,
        "content": "This code configures hyperparameters and settings for different tree search methods in a machine learning model. The options include MCTS, MCTS-Rollout, MCTS-alpha, and DFS. It also allows sampling from the llm's prior on the tree space by setting `select_by_prior` to True. The code is used in an offline reinforcement learning context for model testing.",
        "type": "comment"
    },
    "670": {
        "file_id": 82,
        "content": "    TEST_COT_GREEDY = int(os.getenv(\"TEST_COT_GREEDY\", 0))\n    TEST_COT_SC = int(os.getenv(\"TEST_COT_SC\", 0))\n    assert TEST_NO_TERMINAL + TEST_WITH_TERMINAL + TEST_COT_SC + TEST_COT_GREEDY > 0\n    parser = ArgumentParser()\n    parser.add_argument(\"--ct2_dir\", type=str, required=True)\n    parser.add_argument(\"--critic_model_path\", type=str, required=True)\n    parser.add_argument(\"--tokenizer_path\", type=str, required=True)\n    parser.add_argument(\"--state_dict_path\", type=str, default=None)\n    parser.add_argument(\"--save_dir\", type=str, required=True)\n    parser.add_argument(\"--env_name\", type=str, default=\"gsm8k\")\n    parser.add_argument(\"--test\", type=str2bool, default=True)\n    parser.add_argument(\"--is_few_shot\", type=str2bool, default=False)\n    config = parser.parse_args()\n    TREE_MAX_LENGTH = 4\n    TREE_MAX_ACTIONS = 20\n    # RANDOM_SEEDS = [x * 10009 + 7 for x in [0, 1, 2]]\n    args_list = [\n        {\n            \"temperature\": 1.0,\n            \"max_length\": TREE_MAX_LENGTH,\n            \"max_action\": TREE_MAX_ACTIONS,",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:154-176"
    },
    "671": {
        "file_id": 82,
        "content": "This code is setting environment variables and parsing command line arguments for the tree search algorithm. It sets the test, few shot learning, environment name, save directory, critic model path, tokenizer path, and whether to load a specific state dictionary. The code also defines TREE_MAX_LENGTH, TREE_MAX_ACTIONS, and a list of random seeds (though commented out).",
        "type": "comment"
    },
    "672": {
        "file_id": 82,
        "content": "            \"pb_c_init\": 3,\n            \"num_simulations\": 5,\n            \"k_maj\": 10,\n            \"num_mcts_aggregation\": 1,\n            \"max_simulation\": None,\n            \"max_token\": 51200,\n            \"rollout_method\": \"mcts.rap\",\n            \"select_by_prior\": False,\n            \"reset_total_tree\": False,\n            \"mcts_sample\": False,\n            \"clear_tree\": True,\n            \"prune_ratio\": 0.7,\n            \"prune_value\": None,\n            \"seed\": 7,\n        },\n    ]\n    use_llm_self_eval: bool = False\n    # whether to use LM self-evaluation, this is supported only for game24 now.\n    if use_llm_self_eval:\n        assert (\n            config.env_name == \"game24\"\n        ), \"llm self-evaluation is now only supported for Game24\"\n    task_module = importlib.import_module(f\"tsllm.envs.{config.env_name}\")\n    extract_answer = task_module.extract_answer\n    extract_groundtruth = task_module.extract_groundtruth\n    judge_correct = task_module.judge_correct\n    save_dir = Path(config.save_dir) / config.env_name",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:177-206"
    },
    "673": {
        "file_id": 82,
        "content": "The code imports necessary modules, sets configuration options, and ensures that the LLM self-evaluation is supported for Game24 environment. It also imports specific functions related to extracting answers and ground truths from the task module. The save directory is constructed based on the given save_dir and env_name values.",
        "type": "comment"
    },
    "674": {
        "file_id": 82,
        "content": "    local_rank, world_size = init_distributed()\n    print_rank_0(\"ENV: {}, test set: {}\".format(config.env_name, config.test))\n    train_ds, test_ds = get_env_datasets(config.env_name)\n    if not config.test:\n        test_ds = train_ds\n    device = torch.device(f\"cuda:{local_rank}\")\n    if use_llm_self_eval:\n        tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)\n        critic, _ = load_ct2_model(\n            config.critic_model_path,\n            device=\"cuda\",\n            device_index=local_rank,\n            compute_type=\"bfloat16\",\n        )\n        policy_forward_value = partial(tot_value_fn, critic, tokenizer, config.env_name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)\n        critic = load_critic_model(\n            config.critic_model_path, config.state_dict_path, device\n        )\n        policy_forward_value = partial(value_fn, critic, tokenizer)\n    # # fake value\n    # def policy_forward_value(x):\n    #     return np.array([0.0] * len(x))\n    ############ CONVERT MODEL to CT2 files ###################",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:208-237"
    },
    "675": {
        "file_id": 82,
        "content": "This code initializes distributed settings, prints the environment and test set configuration, retrieves training and testing datasets, assigns a GPU device for model execution, loads a language model with tokenizer, and sets up either a critic model or a value function depending on if the self-evaluation by the LLM is enabled.",
        "type": "comment"
    },
    "676": {
        "file_id": 82,
        "content": "    ct2_generator, ct2_sp = load_ct2_model(\n        config.ct2_dir, device=\"cuda\", device_index=local_rank, compute_type=\"bfloat16\"\n    )\n    def prompt_fn(problem_input: str):\n        return get_default_query_str_builder(config.env_name)(\n            problem_input, is_few_shot=config.is_few_shot\n        )\n    def cot_direct_output(args, problem_inst, stop, **kwargs):\n        prompt = prompt_fn(problem_inst[\"question\"])\n        max_new_tokens = kwargs.pop(\"max_new_tokens\", 256)\n        max_new_tokens = max(256, max_new_tokens)\n        texts, logps = llm_gen_ct2(\n            ct2_generator,\n            tokenizer,\n            static_prompt=None,\n            prompt=prompt,\n            num_sequence=1,\n            stop=tokenizer.eos_token_id,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        )\n        extracted_groundtruth = extract_groundtruth(problem_inst[\"answer\"])\n        value_list = policy_forward_value(\n            [prompt + txt + task_module.SEP for txt in texts]\n        ).tolist()\n        output_list = [",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:238-266"
    },
    "677": {
        "file_id": 82,
        "content": "This code block loads a CT2 model, defines prompt and output functions, and generates texts using the CT2 generator with a given prompt. It then extracts ground truth and calculates forward policy values for each text generated. These outputs can be used to evaluate performance in a few-shot learning task.",
        "type": "comment"
    },
    "678": {
        "file_id": 82,
        "content": "            {\"path_idx\": i, \"text\": txt, \"value\": v}\n            for i, (txt, v) in enumerate(zip(texts, value_list))\n        ]\n        return (\n            judge_ans(\n                problem_inst[\"question\"],\n                extracted_groundtruth,\n                texts,\n                value_list,\n                MAJORITY_VOTE,\n                extract_answer,\n                judge_correct,\n            ),\n            output_list,\n        )\n    def cot_sc_output(args, problem_inst, stop, **kwargs):\n        prompt = prompt_fn(problem_inst[\"question\"])\n        max_new_tokens = kwargs.pop(\"max_new_tokens\", 256)\n        max_new_tokens = max(256, max_new_tokens)\n        texts, logps = llm_gen_ct2(\n            ct2_generator,\n            tokenizer,\n            static_prompt=None,\n            prompt=prompt,\n            num_sequence=args.k_maj,\n            stop=tokenizer.eos_token_id,\n            max_new_tokens=max_new_tokens,\n            max_batch_size=50,\n            **kwargs,\n        )\n        extracted_groundtruth = extract_groundtruth(problem_inst[\"answer\"])",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:267-299"
    },
    "679": {
        "file_id": 82,
        "content": "This code snippet defines a function `cot_sc_output` that takes input arguments and processes them to generate output. It first generates a prompt, sets the maximum number of new tokens allowed, then uses LLM (Large Language Model) generation function (`llm_gen_ct2`) to generate texts and log probability scores (logps). It also extracts ground truth information from the input answer and returns the result of `judge_ans` function along with the output list.",
        "type": "comment"
    },
    "680": {
        "file_id": 82,
        "content": "        value_list = []\n        for i in range(0, len(texts), 25):\n            value_list.extend(\n                policy_forward_value(\n                    [prompt + txt + task_module.SEP for txt in texts[i : i + 25]]\n                ).tolist()\n            )\n        judge_results = {\n            f\"{k}@{args.k_maj}\": judge_ans(\n                problem_inst[\"question\"],\n                extracted_groundtruth,\n                texts,\n                value_list,\n                k,\n                extract_answer,\n                judge_correct,\n            )\n            for k in CHOSEN_AGGR_METHODS\n        }\n        judge_results[\"c%\"] = get_correct_proportion(\n            problem_inst[\"question\"],\n            extracted_groundtruth,\n            texts,\n            extract_answer,\n            judge_correct,\n        )\n        output_list = [\n            {\"path_idx\": i, \"text\": txt, \"value\": v}\n            for i, (txt, v) in enumerate(zip(texts, value_list))\n        ]\n        return judge_results, output_list\n    def mcts_multi_search(",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:300-334"
    },
    "681": {
        "file_id": 82,
        "content": "The code iterates through a list of texts in batches, calculates the policy forward value for each batch, stores these values in a list. It then evaluates the judge results based on the ground truth and text inputs. The function returns a dictionary containing the judge results and a list with path index, text, and value for each text.",
        "type": "comment"
    },
    "682": {
        "file_id": 82,
        "content": "        args: \"SearchArgs\", problem, no_terminal_reward=True, tree_path=None\n    ):\n        env = task_module.Env(\n            config={\n                \"max_actions\": args.max_action,\n                \"max_length\": args.max_length,\n                \"stop_str\": \"The answer is \",\n                \"generation_config\": {\n                    \"max_new_tokens\": 64,\n                    \"do_sample\": True,\n                    \"temperature\": args.temperature,\n                    \"top_p\": 1.0,\n                    \"top_k\": 100,\n                    \"return_dict_in_generate\": True,\n                    \"output_scores\": True,\n                    \"use_cache\": True,\n                },\n            },\n            math_problems=[\n                {\n                    \"question\": problem[\"question\"],\n                    \"answer\": extract_groundtruth(problem[\"answer\"]),\n                }\n            ],\n            llm_gen_fn=partial(llm_gen_ct2, ct2_generator, tokenizer),\n            tokenizer=tokenizer,\n        )\n        # llm_gen_fn=partial(llm_gen_with_logp_v1, model, tokenizer),",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:335-362"
    },
    "683": {
        "file_id": 82,
        "content": "This code initializes an environment using the given configuration and problem, setting up parameters for maximum actions, length, stop string, and generation settings. It also includes a function for generating LLM responses and the tokenizer, creating an environment suitable for running the task.",
        "type": "comment"
    },
    "684": {
        "file_id": 82,
        "content": "        cfg = {\n            \"num_simulations\": args.num_simulations,\n            \"pb_c_base\": 19652,\n            \"pb_c_init\": args.pb_c_init,\n            \"root_dirichlet_alpha\": 0.3,\n            \"root_noise_weight\": 0.25,\n            \"no_terminal_reward\": no_terminal_reward,\n        }\n        if tree_path and tree_path.exists():\n            mcts = MCTS.from_json(cfg, tree_path, reset_visit_info=True)\n        else:\n            mcts = MCTS(cfg=cfg)\n        if args.rollout_method == \"mcts.rollout\":\n            assert args.max_token is not None and args.max_simulation is None\n            output_list, _, _ = mcts.rollout(\n                env,\n                args.num_mcts_aggregation,\n                policy_forward_value,\n                max_num_simulation=args.max_simulation,\n                max_token=args.max_token,\n                return_tree=True,\n            )\n        elif args.rollout_method == \"mcts.get_next_action\":\n            output_list = _mcts_rollout_v1(\n                mcts,\n                env,\n                policy_forward_value,",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:363-390"
    },
    "685": {
        "file_id": 82,
        "content": "This code initializes an MCTS (Monte Carlo Tree Search) object with configuration settings specified in the \"cfg\" dictionary. If a pre-existing tree file is found at the given \"tree_path\", it loads the tree from that file, otherwise, it creates a new MCTS instance. The rollout method for the MCTS object is determined by the \"args.rollout_method\" argument, and the MCTS object's methods are then called to perform either a full rollout or get the next action for further processing.",
        "type": "comment"
    },
    "686": {
        "file_id": 82,
        "content": "                args.num_mcts_aggregation,\n                args.reset_total_tree,\n                sample=args.mcts_sample,\n                clear_total_tree=args.clear_tree,\n            )\n            prompt = prompt_fn(problem[\"question\"])\n            texts = [o[\"text\"] for o in output_list]\n            if len(texts) > 0:\n                value_list = policy_forward_value(\n                    # add a .strip() in case mistakes happens when copy this line to other place\n                    [prompt + txt.strip() + task_module.SEP for txt in texts]\n                ).tolist()\n            else:\n                value_list = []\n            for o, v in zip(output_list, value_list):\n                o[\"value\"] = v\n        elif args.rollout_method == \"mcts.rap\":\n            output_list = mcts.rap(\n                env,\n                args.num_mcts_aggregation,\n                policy_forward_value,\n                args.select_by_prior,\n            )\n        elif args.rollout_method == \"mcts.beam_search\":\n            output_list = mcts.beam_search(",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:391-416"
    },
    "687": {
        "file_id": 82,
        "content": "This code is responsible for executing a test on the Soft-Target and Value functions in a reinforcement learning model. It uses different rollout methods such as MCTS, RAP, and Beam Search to generate output lists. The function applies a policy forward value calculation to each element in the list of texts generated from the problem's question. Depending on the selected rollout method, the output is obtained using different functions: mcts.rap, mcts.beam_search or the MCTS module.",
        "type": "comment"
    },
    "688": {
        "file_id": 82,
        "content": "                env, args.num_mcts_aggregation, args.max_length, policy_forward_value\n            )\n        elif args.rollout_method == \"mcts.dfs\":\n            # here the num_mcts_aggregation is the step_limit which indicate how many nodes\n            # will be visited in the tree.\n            output_list = mcts.dfs(\n                env,\n                args.num_mcts_aggregation,\n                policy_forward_value,\n                prune_value=args.prune_value,\n                prune_ratio=args.prune_ratio,\n            )\n        else:\n            raise ValueError(\"Unknow rollout method: {}\".format(args.rollout_method))\n        texts = [o[\"text\"] for o in output_list]\n        value_list = [o[\"value\"] for o in output_list]\n        extracted_groundtruth = extract_groundtruth(problem[\"answer\"])\n        judge_results = {\n            f\"{k}@{args.num_mcts_aggregation}\": judge_ans(\n                problem[\"question\"],\n                extracted_groundtruth,\n                texts,\n                value_list,\n                k,",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:417-442"
    },
    "689": {
        "file_id": 82,
        "content": "The code determines the rollout method based on the input argument 'args.rollout_method'. If it's \"mcts.assistant",
        "type": "comment"
    },
    "690": {
        "file_id": 82,
        "content": "                extract_answer,\n                judge_correct,\n            )\n            for k in CHOSEN_AGGR_METHODS\n        }\n        judge_results[\"c%\"] = get_correct_proportion(\n            problem[\"question\"],\n            extracted_groundtruth,\n            texts,\n            extract_answer,\n            judge_correct,\n        )\n        if output_list and args.rollout_method != \"mcts.rollout\":\n            num_token = output_list[-1][\"num_generated_token\"]\n        else:\n            num_token = mcts.num_generated_token\n        judge_results[\"#token\"] = num_token\n        return mcts, judge_results, output_list\n    def test_problem(\n        args,\n        idx,\n        problem_inst,\n        cot_writer,\n        cot_sc_writer,\n        mcts_no_term_writer,\n        mcts_w_term_writer,\n    ):\n        results = {}\n        def save_fn(writer, output, result: Dict):\n            if writer is not None:\n                obj = {\n                    \"i\": idx,\n                    \"question\": problem_inst[\"question\"],\n                    \"groundtruth\": problem_inst[\"answer\"],",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:443-479"
    },
    "691": {
        "file_id": 82,
        "content": "This code defines a function `test_problem` that takes arguments, a problem instance, and various writers as input. It returns the MCTS algorithm, judge results, and output list. The function uses nested loops to evaluate problems using different rollout methods. It calculates correct proportions, number of tokens generated, and saves results to specified writers.",
        "type": "comment"
    },
    "692": {
        "file_id": 82,
        "content": "                    \"output\": output,\n                    \"result\": result,\n                }\n                writer.write(obj)\n        if TEST_NO_TERMINAL:\n            # save_tree_path = save_dir / f\"tmp_tree\"\n            # if not save_tree_path.exists():\n            #     save_tree_path.mkdir(parents=True)\n            mcts, r_no_terminal, no_terminal_episodes = mcts_multi_search(\n                args, problem_inst, True\n            )\n            # json.dump(\n            #     get_root(mcts.root).to_json(),\n            #     open(save_tree_path / f\"tree_{idx}.json\", \"w\"),\n            #     indent=2,\n            # )\n            save_fn(mcts_no_term_writer, no_terminal_episodes, r_no_terminal)\n            results[\"w/o-terminal\"] = r_no_terminal\n        if TEST_WITH_TERMINAL:\n            _, r_with_terminal, with_terminal_episodes = mcts_multi_search(\n                args, problem_inst, False\n            )\n            save_fn(mcts_w_term_writer, with_terminal_episodes, r_with_terminal)\n            results[\"w/-terminal\"] = r_with_terminal",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:480-505"
    },
    "693": {
        "file_id": 82,
        "content": "This code is saving a tree using MCTS multi-search for cases with and without terminal, writing the results to JSON files. It uses a writer object to store the output, and defines the search parameters in args and problem_inst variables. The TEST_NO_TERMINAL and TEST_WITH_TERMINAL flags determine which type of search is performed.",
        "type": "comment"
    },
    "694": {
        "file_id": 82,
        "content": "        if TEST_COT_GREEDY:\n            r_cot_greedy, cot_episodes = cot_direct_output(\n                args,\n                problem_inst,\n                stop=tokenizer.eos_token_id,\n                max_new_tokens=256,\n                temperature=args.temperature,\n                top_k=1,\n            )\n            save_fn(cot_writer, cot_episodes, r_cot_greedy)\n            results[\"cot-greedy\"] = r_cot_greedy\n        if TEST_COT_SC:\n            r_cot_sc, cot_sc_episodes = cot_sc_output(\n                args,\n                problem_inst,\n                stop=tokenizer.eos_token_id,\n                max_new_tokens=256,\n                temperature=args.temperature,\n                top_k=100,\n            )\n            save_fn(cot_sc_writer, cot_sc_episodes, r_cot_sc)\n            results[\"cot-sc\"] = r_cot_sc\n        return results\n    def _result_str(results, cnt, join_str=\"\\n\"):\n        res = \"\"\n        for k, v in results.items():\n            if isinstance(v, int):\n                res += f\"{k}: {v/cnt:.2%}\"\n            elif isinstance(v, dict):",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:507-537"
    },
    "695": {
        "file_id": 82,
        "content": "The code performs two tests, TEST_COT_GREEDY and TEST_COT_SC. For the first test, it generates cot_episodes and r_cot_greedy using cot_direct_output function and saves them. The results are then stored in \"cot-greedy\". Similarly, for the second test, cot_sc_episodes and r_cot_sc are generated using cot_sc_output, saved, and stored in \"cot-sc\". Both sets of results are returned.",
        "type": "comment"
    },
    "696": {
        "file_id": 82,
        "content": "                res += f\"{k}: \"\n                res += \", \".join(\n                    [\n                        (\n                            f\"{sub_k}: {sub_v/cnt:.2f}\"\n                            if sub_k == \"#token\"\n                            else f\"{sub_k}: {sub_v/cnt:.2%}\"\n                        )\n                        for sub_k, sub_v in v.items()\n                    ]\n                )\n            else:\n                raise ValueError\n            res += join_str\n        res += f\"cnt: {cnt}\"\n        return res\n    for i_arg, cur_args in enumerate(args_list):\n        args = SearchArgs(**cur_args)\n        seed = args.seed\n        setup_seed(seed)\n        writer_dir = save_dir / (f\"args{i_arg}_seed{seed}/\")\n        if local_rank == 0:\n            print(\"Search args: {}, SEED={}\".format(args, seed))\n            if not writer_dir.exists():\n                writer_dir.mkdir(parents=True)\n            json.dump(cur_args, open(writer_dir / \"args.json\", \"w\"))\n        if TEST_COT_GREEDY:\n            cot_save_path = writer_dir / \"cot\"",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:538-568"
    },
    "697": {
        "file_id": 82,
        "content": "This code appears to be part of a larger script that performs some form of tree search and language modeling. The code snippet is responsible for creating summary logs for each set of arguments (args_list). It calculates various statistics, such as token counts, percentages, and joins them into a formatted string. If the given arguments don't meet certain conditions, it raises a ValueError. The script also writes these summaries to a file within the specified save directory for each set of arguments.",
        "type": "comment"
    },
    "698": {
        "file_id": 82,
        "content": "            if local_rank == 0 and not cot_save_path.exists():\n                cot_save_path.mkdir(parents=True)\n            dist.barrier()\n            cot_writer = jsonlines.open(cot_save_path / f\"{local_rank}.jsonl\", \"a\")\n        else:\n            cot_writer = None\n        if TEST_COT_SC:\n            cot_sc_save_path = writer_dir / \"cot_sc\"\n            if local_rank == 0 and not cot_sc_save_path.exists():\n                cot_sc_save_path.mkdir(parents=True)\n            dist.barrier()\n            cot_sc_writer = jsonlines.open(\n                cot_sc_save_path / f\"{local_rank}.jsonl\", \"a\"\n            )\n        else:\n            cot_sc_writer = None\n        if TEST_NO_TERMINAL:\n            mcts_no_term_save_path = writer_dir / \"no_terminal_reward\"\n            if local_rank == 0 and not mcts_no_term_save_path.exists():\n                mcts_no_term_save_path.mkdir(parents=True)\n            dist.barrier()\n            mcts_no_term_writer = jsonlines.open(\n                mcts_no_term_save_path / f\"{local_rank}.jsonl\", \"a\"",
        "type": "code",
        "location": "/tsllm/offline_rl/test_sft_and_v.py:569-592"
    },
    "699": {
        "file_id": 82,
        "content": "This code is initializing writer objects for different types of data based on flags. If the local rank is 0 and the save path does not exist, it creates the directory using mkdir. Then, dist.barrier() ensures all processes reach this point before proceeding further. The code then opens jsonlines writers for each local rank and appends to them if TEST_COT_SC or TEST_NO_TERMINAL is set. If not, the writer is set to None. This code seems to be part of a distributed system where each process has its own local rank and writes data to separate files for further processing.",
        "type": "comment"
    }
}