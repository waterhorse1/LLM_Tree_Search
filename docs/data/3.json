{
    "300": {
        "file_id": 45,
        "content": "            test_dataset = full_dataset.select(\n                [i for i in range(num_train_data, len(full_dataset))]\n            )\n    else:\n        train_data_pre = kwargs.pop(\"train_data_pre\")\n        train_data_post = kwargs.pop(\"train_data_post\")\n        full_dataset = load_from_disk(**kwargs)[\"train\"]\n        train_dataset = full_dataset.select(\n            [i for i in range(train_data_pre, train_data_post)]\n        )\n        test_dataset = full_dataset.select(\n            [i for i in range(train_data_pre, train_data_post)]\n        )\n    train_dataset = train_dataset.rename_column(\"prompt\", \"question\")\n    test_dataset = test_dataset.rename_column(\"prompt\", \"question\")\n    return train_dataset, test_dataset\ndef build_offline_data_component(path, q2idx_dict, tokenizer, sep):\n    def get_value_index(question, answer):\n        pre_state_token_length = len(tokenizer.encode(question + sep))\n        index = [pre_state_token_length]\n        if not sep == \"\":\n            answer_list = answer.split(sep)\n            for action in answer_list:",
        "type": "code",
        "location": "/tsllm/envs/rlhf/data.py:59-83"
    },
    "301": {
        "file_id": 45,
        "content": "The code defines a function that takes in various parameters such as `train_data_pre`, `train_data_post`, and `full_dataset`. If the `test_dataset` is not specified, it creates both train and test datasets by selecting specific ranges of data from the full dataset. It then renames the \"prompt\" column to \"question\" for both datasets. The code also defines an inner function called `get_value_index()` that takes in a question and answer and calculates the value index based on token length. This function may be used later in the code.",
        "type": "comment"
    },
    "302": {
        "file_id": 45,
        "content": "                action_length = len(\n                    tokenizer.encode(action + sep, add_special_tokens=False)\n                )\n                index.append(action_length)\n                if action_length == 0:\n                    print_with_rank(\n                        \"possbile problems met in online value instance building. {}\".format(\n                            action\n                        )\n                    )\n        else:\n            answer_tokens = tokenizer.encode(answer, add_special_tokens=False)\n            for token in answer_tokens:\n                index.append(1)\n        index = np.cumsum(index) - 1\n        return index\n    predata = load_jsonl(path)\n    traj_dict_list = []\n    for idx, d in enumerate(predata):\n        question = d[\"question\"]\n        if question in q2idx_dict.keys():\n            task_idx = q2idx_dict[question]\n            full_answer_list = d[\"answer\"]\n            # deduplication\n            # note that in Dahoas/synthetic-instruct-gptj-pairwise, these exists several questions that are the same",
        "type": "code",
        "location": "/tsllm/envs/rlhf/data.py:84-110"
    },
    "303": {
        "file_id": 45,
        "content": "Code calculates the action length for each token, appends it to index list. If action length is 0, prints potential issues. Else, encodes answer tokens and appends 1 to index list for each token. Calculates cumulative sum of index list and subtracts 1. Loads jsonl data and creates traj_dict_list by iterating over predata, considering unique questions and their corresponding answers.",
        "type": "comment"
    },
    "304": {
        "file_id": 45,
        "content": "            # but here the deduplication only happens for the answer list given one question\n            # So it is still possible to have sample example when add traj\n            unique_answer_list = list(\n                {item[\"text\"]: item for item in full_answer_list}.values()\n            )\n            answer_list = []\n            for a in unique_answer_list:\n                answer = a[\"text\"]\n                query_str = RLHF_TokenEnv.build_query_str(\n                    cot_task_desc=None,\n                    cot_examples=None,\n                    problem_format_str=PROBLEM_FORMAT_STR,\n                    problem_input=question,\n                    sep=SEP,\n                    is_few_shot=False,\n                )\n                value_index = get_value_index(query_str, answer)\n                # :-1 is value index, -1 is the reward index\n                reward_list = np.zeros(len(value_index) - 1)\n                reward_list[-1] = a[\"reward\"]\n                traj_dict = {\n                    \"idx\": task_idx,",
        "type": "code",
        "location": "/tsllm/envs/rlhf/data.py:111-132"
    },
    "305": {
        "file_id": 45,
        "content": "This code ensures that duplicate answer texts are removed from the full_answer_list, creating a unique list of answers. Then, for each unique answer in the updated list, it builds a query string using the RLHF_TokenEnv class. The function get_value_index is used to find the value index, and np.zeros creates a reward list with all values set to 0 except the last one which gets the \"reward\" value from the unique answer dictionary. Lastly, a traj_dict is created with an 'idx' key for task index.",
        "type": "comment"
    },
    "306": {
        "file_id": 45,
        "content": "                    \"query_str\": query_str,\n                    \"answer\": answer,\n                    \"value_index\": value_index,\n                    \"reward_list\": reward_list,\n                }\n                traj_dict_list.append(traj_dict)\n                answer_list.append(answer)\n    return traj_dict_list\n# def build_query_str(problem_input, config=None):\n#     from .prompt import PROBLEM_FORMAT_STR\n#     return PROBLEM_FORMAT_STR.format(problem_input)",
        "type": "code",
        "location": "/tsllm/envs/rlhf/data.py:133-146"
    },
    "307": {
        "file_id": 45,
        "content": "This function takes in a problem input and returns a list of dictionaries, where each dictionary contains the generated query string, answer, value index, and reward list. The build_query_str function formats the problem input using the PROBLEM_FORMAT_STR prompt from the prompt module.",
        "type": "comment"
    },
    "308": {
        "file_id": 46,
        "content": "/tsllm/envs/rlhf/env.py",
        "type": "filepath"
    },
    "309": {
        "file_id": 46,
        "content": "The code defines a reinforcement learning environment class, extending TokenEnv, for language model tasks. It initializes with parameters and includes properties for actions, termination conditions, updates, reward calculation, and environment copying.",
        "type": "summary"
    },
    "310": {
        "file_id": 46,
        "content": "import copy\nimport re\nfrom typing import List\nimport numpy as np\nfrom transformers import PreTrainedTokenizer\nfrom tsllm.envs.base_env import TokenEnv\nfrom .prompt import PROBLEM_FORMAT_STR, SEP\nclass RLHF_TokenEnv(TokenEnv):\n    sep=SEP\n    def __init__(\n        self,\n        config,\n        problems,\n        llm_forward_fn,\n        tokenizer,\n        reward_fn,\n        task_desc_str= None,\n        cot_example_str = None,\n        problem_format_str = PROBLEM_FORMAT_STR,\n        reset=True,\n    ):\n        super().__init__(\n            config,\n            problems,\n            llm_forward_fn,\n            tokenizer,\n            task_desc_str,\n            cot_example_str,\n            problem_format_str,\n            reset,\n        )\n        self.reward_fn = reward_fn\n    @staticmethod\n    def build_response_str(\n        answer_str: str, tokenizer: PreTrainedTokenizer, add_eos_token: bool\n    ):\n        if (\n            add_eos_token\n            and len(tokenizer.encode(answer_str, add_special_tokens=False)) < 64\n        ):\n            answer_str += tokenizer.eos_token",
        "type": "code",
        "location": "/tsllm/envs/rlhf/env.py:1-43"
    },
    "311": {
        "file_id": 46,
        "content": "This code defines a class called RLHF_TokenEnv that extends the TokenEnv class. It initializes an object with various parameters such as config, problems, llm_forward_fn, tokenizer, reward_fn, task_desc_str, cot_example_str, problem_format_str, and reset. The class also defines a static method build_response_str that takes in answer_str, tokenizer, and add_eos_token as parameters. This method adds the eos_token to the answer if the answer length is less than 64 and add_eos_token is True.",
        "type": "comment"
    },
    "312": {
        "file_id": 46,
        "content": "        return answer_str\n    @property\n    def stop_str(self):\n        return self.tokenizer.eos_token\n    # def init_action_history(self):\n    #     # add the first prompted questions\n    #     return ([self.task_prefix] if self.task_prefix is not None else []) + [\n    #         self._problem_format_str.format(self.problem['prompt'])\n    #     ]\n    def step(self, action, update_legal_action=True):\n        terminated = False\n        if not self.stop_str == action:\n            # remove the final stop string like eos token\n            self.action_history.append(action)\n        else:\n            terminated = True\n        state = self.get_state()\n        truncated = len(self.action_history) >= self.config[\"max_length\"] + (\n            2 if self.task_prefix is not None else 1\n        )\n        reward = self.get_reward(terminated, truncated)\n        # update legal actions\n        if not (terminated or truncated) and update_legal_action:\n            self._legal_actions = self.update_legal_actions()\n        else:\n            self._legal_actions = None",
        "type": "code",
        "location": "/tsllm/envs/rlhf/env.py:44-72"
    },
    "313": {
        "file_id": 46,
        "content": "The code defines a class with properties stop_str and step, which is used in a reinforcement learning environment. The stop_str returns the end of sequence token, and the step function handles actions taken by the environment, termination conditions, and updates legal actions. It also retrieves the state and reward based on the action taken.",
        "type": "comment"
    },
    "314": {
        "file_id": 46,
        "content": "        return state, reward, terminated, truncated, {'winner': None, 'reward': reward}\n    def get_reward(self, terminated, truncated):\n        \"\"\"To implement based on learned reward model\"\"\"\n        if terminated or truncated:\n            reward = self.reward_fn(self.question, self.answer)\n        else:\n            reward = 0\n        return reward\n    def copy(self):\n        env = self.__class__(\n            self.config,\n            self.problems,\n            self.llm_forward_fn,\n            self.tokenizer,\n            self.reward_fn,\n            self._task_desc_str,\n            self._cot_example_str,\n            self._problem_format_str,\n            reset=False,\n        )\n        env.problem = copy.deepcopy(self.problem)\n        env._legal_actions = copy.deepcopy(self._legal_actions)\n        env.action_history = copy.deepcopy(self.action_history)\n        return env\nif \"__name__\" == '__main__':\n    pass",
        "type": "code",
        "location": "/tsllm/envs/rlhf/env.py:73-102"
    },
    "315": {
        "file_id": 46,
        "content": "The code defines an environment (env.py) for a reinforcement learning task involving language models. It returns state, reward, termination, and truncation status, along with additional information. The get_reward function calculates the reward based on a learned reward model, while the copy function creates a copy of the environment with the same configuration and current state.",
        "type": "comment"
    },
    "316": {
        "file_id": 47,
        "content": "/tsllm/envs/rlhf/prompt.py",
        "type": "filepath"
    },
    "317": {
        "file_id": 47,
        "content": "The code snippet defines constants related to the instruction and response format of a task. COT_EXAMPLES and COT_TASK_DESC are set to None, suggesting they might be used later in the program. The variable PROBLEM_FORMAT_STR provides a template for displaying instructions and requests, with placeholders for the question and response. SEP is an empty string possibly used for separating elements in the code.",
        "type": "summary"
    },
    "318": {
        "file_id": 47,
        "content": "COT_EXAMPLES = None\nCOT_TASK_DESC = None\nPROBLEM_FORMAT_STR = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{question}\n### Response:\n\"\"\"\nSEP=\"\"",
        "type": "code",
        "location": "/tsllm/envs/rlhf/prompt.py:1-12"
    },
    "319": {
        "file_id": 47,
        "content": "The code snippet defines constants related to the instruction and response format of a task. COT_EXAMPLES and COT_TASK_DESC are set to None, suggesting they might be used later in the program. The variable PROBLEM_FORMAT_STR provides a template for displaying instructions and requests, with placeholders for the question and response. SEP is an empty string possibly used for separating elements in the code.",
        "type": "comment"
    },
    "320": {
        "file_id": 48,
        "content": "/tsllm/envs/tests/test_game24.py",
        "type": "filepath"
    },
    "321": {
        "file_id": 48,
        "content": "The code initializes a Game24Env environment, tests answers, and prints the state of the environment. It also loads data from a JSONL file, tokenizes it, and displays lengths of critic_data, query string, and answer.",
        "type": "summary"
    },
    "322": {
        "file_id": 48,
        "content": "from tsllm.envs.game24.env import (\n    Game24Env,\n    COT_EXAMPLES,\n    COT_TASK_DESC,\n    PROBLEM_FORMAT_STR,\n    SEP,\n)\nimport pytest\nif __name__ == \"__main__\":\n    problem_input = \"1 3 3 4\"\n    env = Game24Env(\n        config={},\n        math_problems=[{\"question\": \"1 3 3 4\", \"answer\": \"\"}],\n        tokenizer=None,\n        llm_gen_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    print(env._is_correct(\"The answer is (3 * 4) * (3 - 1) = 24\"))\n    print(env._is_correct(\"\\n\\nThe answer is (3 * 4) * (3 - 1) = 24\"))\n    print(env._is_correct(\"The answer is (3 * 3) * (3 - 1) = 24\"))\n    print(env._is_correct(\"The answer is (3 * 4) * (3 - 1) = 23\"))\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    build_query_str = Game24Env.build_query_str\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    print(\"\\n\\n====== FEW SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, True",
        "type": "code",
        "location": "/tsllm/envs/tests/test_game24.py:1-38"
    },
    "323": {
        "file_id": 48,
        "content": "This code is initializing a Game24Env environment and testing different answers to see if they are correct. It also prints the state of the environment and uses build_query_str function for zero shot and few shot contextual understanding tasks.",
        "type": "comment"
    },
    "324": {
        "file_id": 48,
        "content": "        )\n    )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _  = get_env_datasets(\"game24\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx\n    sft_data = get_default_sft_data_builder(\n        \"game24\")(\n        \"tsllm/envs/game24/train_data/train_dedup.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")\n    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"game24\")(",
        "type": "code",
        "location": "/tsllm/envs/tests/test_game24.py:39-68"
    },
    "325": {
        "file_id": 48,
        "content": "The code imports necessary libraries, defines a tokenizer, prepares a default small few-shot task dataset, and prints the lengths and samples of both the training data and the default critic dataset for \"game24\".",
        "type": "comment"
    },
    "326": {
        "file_id": 48,
        "content": "        \"tsllm/envs/game24/train_data/train_dedup.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False\n    )\n    print(\"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(len(critic_data), critic_data[0]))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"]+critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))",
        "type": "code",
        "location": "/tsllm/envs/tests/test_game24.py:69-76"
    },
    "327": {
        "file_id": 48,
        "content": "This code is loading data from a JSONL file, using a provided dictionary and tokenizer. It then prints the length of the critic_data, and the encoded lengths of the query string and answer in that data.",
        "type": "comment"
    },
    "328": {
        "file_id": 49,
        "content": "/tsllm/envs/tests/test_gsm8k.py",
        "type": "filepath"
    },
    "329": {
        "file_id": 49,
        "content": "The code initializes modules, retrieves the GSM8k dataset, builds a few-shot learning task dataset, and tests the model by creating an instance of GSMLLMTreeSearch. It prints dataset information and lengths of encoded text data for analysis.",
        "type": "summary"
    },
    "330": {
        "file_id": 49,
        "content": "from tsllm.envs.gsm8k.env import (\n    Gsm8kEnv,\n    COT_EXAMPLES,\n    COT_TASK_DESC,\n    PROBLEM_FORMAT_STR,\n    SEP,\n)\nimport pytest\nif __name__ == \"__main__\":\n    problem_input = \"1 3 3 4\"\n    env = Gsm8kEnv(\n        config={},\n        math_problems=[{\"question\": \"1 3 3 4\", \"answer\": \"3\"}],\n        tokenizer=None,\n        llm_gen_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    print(env._is_correct(\"The answer is 3\"))\n    print(env._is_correct(\"\\n\\nThe answer is 3.\"))\n    print(env._is_correct(\"The answer is 4\"))\n    print(env._is_correct(\"The answer is x\"))\n    build_query_str = Gsm8kEnv.build_query_str\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    # print(\"\\n\\n====== FEW SHOT COT ============\")\n    # print(\n    #     build_query_str(\n    #         COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, True\n    #     )",
        "type": "code",
        "location": "/tsllm/envs/tests/test_gsm8k.py:1-39"
    },
    "331": {
        "file_id": 49,
        "content": "The code imports necessary modules and defines the environment for a math problem game. It creates an instance of Gsm8kEnv with a given math problem, resets the game state, checks correct answers, and prints the current game state. It also demonstrates zero-shot contextualized ordinal regression task description using provided functions.",
        "type": "comment"
    },
    "332": {
        "file_id": 49,
        "content": "    # )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _ = get_env_datasets(\"gsm8k\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx\n    sft_data = get_default_sft_data_builder(\"gsm8k\")(\n        \"tsllm/envs/gsm8k/train_data/sft_init.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")\n    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"gsm8k\")(\n        \"tsllm/envs/gsm8k/train_data/sft_init.jsonl\",",
        "type": "code",
        "location": "/tsllm/envs/tests/test_gsm8k.py:40-68"
    },
    "333": {
        "file_id": 49,
        "content": "This code initializes a tokenizer, retrieves the training dataset for an 8k General Science Multitask (GSMT) dataset, creates a dictionary mapping questions to indices, builds a few-shot learning task dataset using the GSM8K dataset, and prints information about the datasets.",
        "type": "comment"
    },
    "334": {
        "file_id": 49,
        "content": "        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False,\n    )\n    print(\n        \"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(\n            len(critic_data), critic_data[0]\n        )\n    )\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"] + critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))",
        "type": "code",
        "location": "/tsllm/envs/tests/test_gsm8k.py:69-79"
    },
    "335": {
        "file_id": 49,
        "content": "This code tests the GSM8k dataset by creating an instance of the GSMLLMTreeSearch model, setting tokenizer and few-shot parameters to False, then prints various lengths related to encoded text data for further analysis.",
        "type": "comment"
    },
    "336": {
        "file_id": 50,
        "content": "/tsllm/envs/tests/test_prontoqa.py",
        "type": "filepath"
    },
    "337": {
        "file_id": 50,
        "content": "The code sets up an environment for PrOntoQA question answering task, using Llama-2-7b-hf model for tokenization. It creates and prints the length of different datasets, with 'sft_data' and 'critic_data' generated by get_default_sft_data_builder and get_default_critic_data_builder functions respectively. The code also performs additional print statements on selected elements from critic_data.",
        "type": "summary"
    },
    "338": {
        "file_id": 50,
        "content": "from tsllm.envs.prontoqa.env import (\n    PrOntoQAEnv,\n    COT_EXAMPLES,\n    COT_TASK_DESC,\n    PROBLEM_FORMAT_STR,\n    SEP,\n)\nif __name__ == \"__main__\":\n    problem_input = 'Butterflies are lepidopterans. Every arthropod is small. Whales are not small. Invertebrates are animals. Every insect is an arthropod. Lepidopterans are insects. Every insect is six-legged. Every arthropod is an invertebrate. Animals are multicellular. Polly is a lepidopteran. Is the statement \"Polly is not small\" true or false?'\n    env = PrOntoQAEnv(\n        config={},\n        math_problems=[\n            {\n                \"question\": 'Butterflies are lepidopterans. Every arthropod is small. Whales are not small. Invertebrates are animals. Every insect is an arthropod. Lepidopterans are insects. Every insect is six-legged. Every arthropod is an invertebrate. Animals are multicellular. Polly is a lepidopteran. Is the statement \"Polly is not small\" true or false?',\n                \"answer\": False,\n            }\n        ],\n        tokenizer=None,",
        "type": "code",
        "location": "/tsllm/envs/tests/test_prontoqa.py:1-19"
    },
    "339": {
        "file_id": 50,
        "content": "This code imports necessary modules and defines a PrOntoQAEnv class, which is an environment for a question answering task. The code then creates an instance of this class to solve a specific problem input and check if the statement \"Polly is not small\" is true or false.",
        "type": "comment"
    },
    "340": {
        "file_id": 50,
        "content": "        llm_gen_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    print(\"correct: \", env._is_correct(\"The answer is false.\"))\n    build_query_str = PrOntoQAEnv.build_query_str\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    print(\"\\n\\n====== FEW SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, True\n        )\n    )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _ = get_env_datasets(\"prontoqa\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx",
        "type": "code",
        "location": "/tsllm/envs/tests/test_prontoqa.py:20-52"
    },
    "341": {
        "file_id": 50,
        "content": "This code initializes an environment for PrOntoQA, prints its state, and demonstrates zero-shot and few-shot capabilities. It then tokenizes using the Llama-2-7b-hf model and retrieves default SFG data builder for \"prontoqa\".",
        "type": "comment"
    },
    "342": {
        "file_id": 50,
        "content": "    sft_data = get_default_sft_data_builder(\"prontoqa\")(\n        \"tsllm/envs/prontoqa/train_data/train.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")\n    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"prontoqa\")(\n        \"tsllm/envs/prontoqa/train_data/train.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False,\n    )\n    print(\n        \"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(\n            len(critic_data), critic_data[0]\n        )\n    )\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"] + critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))",
        "type": "code",
        "location": "/tsllm/envs/tests/test_prontoqa.py:53-79"
    },
    "343": {
        "file_id": 50,
        "content": "This code snippet is responsible for creating and printing the length of different datasets. It uses the get_default_sft_data_builder and get_default_critic_data_builder functions to generate 'sft_data' and 'critic_data' respectively, which contain training data for a model. The code then prints the lengths of these datasets (train_ds, sft_data, critic_data) and performs some additional print statements on selected elements from critic_data.",
        "type": "comment"
    },
    "344": {
        "file_id": 51,
        "content": "/tsllm/envs/tests/test_rlhf.py",
        "type": "filepath"
    },
    "345": {
        "file_id": 51,
        "content": "The code imports necessary modules and functions, sets up an environment for a contextual understanding task, fine-tunes pretrained models, and prints critical data details.",
        "type": "summary"
    },
    "346": {
        "file_id": 51,
        "content": "from tsllm.envs.rlhf.env import RLHF_TokenEnv, PROBLEM_FORMAT_STR, SEP\nfrom tsllm.envs.rlhf.prompt import COT_EXAMPLES, COT_TASK_DESC\nimport pytest\nif __name__ == \"__main__\":\n    problem_input = \"1 3 3 4\"\n    env = RLHF_TokenEnv(\n        config={},\n        problems=[{\"question\": \"1 3 3 4\", \"answer\": \"\"}],\n        tokenizer=None,\n        llm_forward_fn=None,\n        reward_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    # print(env._is_correct(\"The answer is (3 * 4) * (3 - 1) = 24\"))\n    # print(env._is_correct(\"\\n\\nThe answer is (3 * 4) * (3 - 1) = 24\"))\n    # print(env._is_correct(\"The answer is (3 * 3) * (3 - 1) = 24\"))\n    # print(env._is_correct(\"The answer is (3 * 4) * (3 - 1) = 23\"))\n    build_query_str = RLHF_TokenEnv.build_query_str\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    # print(\"\\n\\n====== FEW SHOT COT ============\")",
        "type": "code",
        "location": "/tsllm/envs/tests/test_rlhf.py:1-29"
    },
    "347": {
        "file_id": 51,
        "content": "The code imports necessary classes and functions from related modules, creates an RLHF_TokenEnv environment with a single problem input, resets the environment, prints its state, and then builds a query string for a zero-shot contextual understanding task using provided descriptions, examples, and problem input.",
        "type": "comment"
    },
    "348": {
        "file_id": 51,
        "content": "    # print(\n    #     build_query_str(\n    #         COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, True\n    #     )\n    # )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"vicgalle/gpt2-open-instruct-v1\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _  = get_env_datasets(\"game24\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx\n    sft_data = get_default_sft_data_builder(\n        \"rlhf\")(\n        \"tsllm/envs/game24/train_data/train_dedup.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")",
        "type": "code",
        "location": "/tsllm/envs/tests/test_rlhf.py:30-56"
    },
    "349": {
        "file_id": 51,
        "content": "This code imports necessary libraries and builds the default supervised fine-tuning (sft) dataset using a pretrained tokenizer. It then prints details about the sft_data and train_ds, including their lengths and examples.",
        "type": "comment"
    },
    "350": {
        "file_id": 51,
        "content": "    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"rlhf\")(\n        \"tsllm/envs/game24/train_data/train_dedup.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False\n    )\n    print(\"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(len(critic_data), critic_data[0]))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"]+critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))",
        "type": "code",
        "location": "/tsllm/envs/tests/test_rlhf.py:57-66"
    },
    "351": {
        "file_id": 51,
        "content": "The code imports the `get_default_critic_data_builder` function from `tsllm.envs` module and uses it to create a `critic_data` object with given parameters: \"rlhf\" mode, path to train data file, query to index dictionary (q2idx_dict), tokenizer, and non-few shot setting. It then prints the length of critic_data, first sample in critic_data, encoded length of the concatenation of query string and answer in critic_data, and lastly the encoded length of just the query string from critic_data.",
        "type": "comment"
    },
    "352": {
        "file_id": 52,
        "content": "/tsllm/envs/utils.py",
        "type": "filepath"
    },
    "353": {
        "file_id": 52,
        "content": "The `build_critic_data_component` function constructs a data component for tasks, utilizes tokenization and generates query and response strings from JSONL files. It returns a list of dictionaries containing information like index, query, answer, response string, handles optional parameters and initializes reward_list with zeros. It assigns rewards to the correct answers and appends it to traj_dict_list before returning it.",
        "type": "summary"
    },
    "354": {
        "file_id": 52,
        "content": "from typing import Dict, Optional, Union, Callable\nimport numpy as np\nfrom tsllm.distributed.utils import print_with_rank\nfrom tsllm.offline_rl.utils import load_jsonl\nfrom transformers import PreTrainedTokenizer\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nimport jsonlines\ndef build_sft_data_component(\n    jsonl_path: Union[Path, str],\n    q2idx_dict: Dict,\n    tokenizer: PreTrainedTokenizer,\n    add_eos_token: bool,\n    is_few_shot: bool,\n    build_query_str_fn: Callable,\n    build_response_str_fn: Callable,\n    sep: str,\n    cot_task_desc_str: Optional[str] = None,\n    cot_example_str: Optional[str] = None,\n    problem_format_str: Optional[str] = None,\n):\n    predata = load_jsonl(jsonl_path)\n    q_r_dict_list = []\n    for idx, d in enumerate(predata):\n        question = d[\"question\"]\n        if question not in q2idx_dict:\n            continue\n        task_idx = q2idx_dict[question]\n        full_answer_list = d[\"answer\"]\n        query_str = build_query_str_fn(\n            cot_task_desc=cot_task_desc_str,",
        "type": "code",
        "location": "/tsllm/envs/utils.py:1-33"
    },
    "355": {
        "file_id": 52,
        "content": "This function builds a data component for the task, taking in various parameters like JSONL path, dictionary mapping questions to indices, tokenizer, boolean values for adding EOS tokens, whether it's few-shot learning or not, and functions for building query and response strings. It loads the JSONL file, filters out irrelevant questions, constructs queries based on optional task descriptions and examples, and populates a list of (question index, full answer) tuples to be used in the training dataset.",
        "type": "comment"
    },
    "356": {
        "file_id": 52,
        "content": "            cot_examples=cot_example_str,\n            problem_format_str=problem_format_str,\n            problem_input=question,\n            sep=sep,\n            is_few_shot=is_few_shot,\n        )\n        for answer_output in full_answer_list:\n            answer_txt = answer_output[\"text\"]\n            response_str = build_response_str_fn(answer_txt, tokenizer, add_eos_token)\n            traj_dict = {\n                \"idx\": task_idx,\n                \"query_str\": query_str,\n                \"answer\": answer_txt,\n                \"response_str\": response_str,\n            }\n            q_r_dict_list.append(traj_dict)\n    return q_r_dict_list\ndef build_critic_data_component(\n    jsonl_path: Union[Path, str],\n    q2idx_dict: Dict,\n    tokenizer: PreTrainedTokenizer,\n    sep: str,\n    is_few_shot: bool,\n    build_query_str_fn: Callable,\n    cot_task_desc_str: Optional[str] = None,\n    cot_example_str: Optional[str] = None,\n    problem_format_str: Optional[str] = None,\n):\n    def get_value_index(q_str: str, answer_str: str):",
        "type": "code",
        "location": "/tsllm/envs/utils.py:34-66"
    },
    "357": {
        "file_id": 52,
        "content": "This code defines a function `build_critic_data_component` that takes in various parameters and returns a list of dictionaries (`q_r_dict_list`). The function seems to build data components for a task, where it uses tokenization, builds response strings, and creates dictionaries with information such as index, query string, answer, and response string. It also takes optional parameters like `cot_task_desc_str`, `cot_example_str`, and `problem_format_str`. The function utilizes other helper functions like `build_response_str_fn` and `get_value_index`.",
        "type": "comment"
    },
    "358": {
        "file_id": 52,
        "content": "        pre_state_token_length = len(tokenizer.encode(q_str))\n        indices = [pre_state_token_length]\n        if sep != \"\":\n            answer_list = answer_str.split(sep)\n            check_indices = [pre_state_token_length - 1]\n            current_str = q_str\n            for action in answer_list:\n                current_str += action + sep\n                if len(action) == 0:\n                    print_with_rank(\n                        \"WARNING: possbile problems met in sft instance building. {}\".format(\n                            action\n                        )\n                    )\n                    continue\n                check_indices.append(len(tokenizer.encode(current_str)) - 1)\n            check_indices = np.array(check_indices)\n            indices = check_indices\n        else:\n            answer_tokens = tokenizer.encode(answer_str, add_special_tokens=False)\n            for token in answer_tokens:\n                indices.append(1)\n            indices = np.cumsum(indices) - 1\n        return indices",
        "type": "code",
        "location": "/tsllm/envs/utils.py:67-94"
    },
    "359": {
        "file_id": 52,
        "content": "The code calculates the tokenized length of the question string and adds it to a list called 'indices'. If there is a separator in the answer string, the answer string is split into a list of actions. For each action, the code appends the action and separator to the current string, updates the check_indices list with the tokenized length minus 1, and prints a warning if the action is empty. After looping through all actions, the check_indices are converted to a numpy array and becomes the new 'indices'. If there's no separator in the answer string, the code encodes the answer string into tokens and appends 1 to the indices list for each token. The indices list is then cumulatively summed and subtracted by 1 to create the final 'indices'.",
        "type": "comment"
    },
    "360": {
        "file_id": 52,
        "content": "    predata = load_jsonl(jsonl_path)\n    traj_dict_list = []\n    for idx, d in enumerate(predata):\n        question = d[\"question\"]\n        if question not in q2idx_dict.keys():\n            continue\n        task_idx = q2idx_dict[question]\n        full_answer_list = d[\"answer\"]\n        query_str = build_query_str_fn(\n            cot_task_desc=cot_task_desc_str,\n            cot_examples=cot_example_str,\n            problem_format_str=problem_format_str,\n            problem_input=question,\n            sep=sep,\n            is_few_shot=is_few_shot,\n        )\n        for answer_output in full_answer_list:\n            \"\"\"answer_output is a dict with keys:\n            \"text\", \"reward\",\n            if there is not \"reward\" key, use \"correct\" key\n            \"\"\"\n            if len(sep) > 1:\n                print_with_rank(\"WARNING: sep is not empty, but {}\".format(sep))\n            answer = answer_output[\"text\"].strip(sep)\n            value_index = get_value_index(query_str, answer)\n            # :-1 is value index, -1 is the reward index",
        "type": "code",
        "location": "/tsllm/envs/utils.py:96-121"
    },
    "361": {
        "file_id": 52,
        "content": "This code reads a list of preloaded JSONL data and iterates through it, filtering out questions not in q2idx_dict. For each question, it builds a query string using provided arguments and then iterates through the answers for that question. If there is no \"reward\" key in the answer dict, it uses the \"correct\" key instead. It retrieves the value index of the answer from the query string and adds a tuple (task_idx, value_index, -1) to traj_dict_list if sep is empty.",
        "type": "comment"
    },
    "362": {
        "file_id": 52,
        "content": "            reward_list = np.zeros(len(value_index) - 1)\n            if \"reward\" not in answer_output:\n                answer_output[\"reward\"] = 1.0 if answer_output[\"correct\"] else -1.0\n            reward_list[-1] = answer_output[\"reward\"]\n            traj_dict = {\n                \"idx\": task_idx,\n                \"query_str\": query_str,\n                \"answer\": answer + sep,\n                \"value_index\": value_index,\n                \"reward_list\": reward_list,\n            }\n            traj_dict_list.append(traj_dict)\n    return traj_dict_list",
        "type": "code",
        "location": "/tsllm/envs/utils.py:122-135"
    },
    "363": {
        "file_id": 52,
        "content": "This code initializes a reward_list with zeros, assigns rewards to the answer output based on correctness, adds reward_list to traj_dict and appends it to traj_dict_list. The function returns traj_dict_list.",
        "type": "comment"
    },
    "364": {
        "file_id": 53,
        "content": "/tsllm/inference/evaluation/vote_utils.py",
        "type": "filepath"
    },
    "365": {
        "file_id": 53,
        "content": "This code defines functions for aggregating votes based on majority and other rules. It uses the Counter class from collections to count votes, and defaultdict from collections for calculating the orm_max. The AGG_FN_MAP dictionary maps different aggregation functions to their corresponding functions in this module.",
        "type": "summary"
    },
    "366": {
        "file_id": 53,
        "content": "from collections import Counter, defaultdict\nfrom typing import List\nMAJORITY_VOTE = \"majority_vote\"\nORM_VOTE = \"orm_vote\"\nORM_MAX = \"orm_max\"\ndef _agg_majority_vote(x_list: List[str], unused_v_list: List[float]):\n    counts = Counter(x_list)\n    most_common = max(counts, key=counts.get)\n    return most_common\ndef _agg_orm_vote(x_list: List[str], v_list: List[float]):\n    assert len(x_list) == len(v_list)\n    x_dict = defaultdict(lambda: 0.0)\n    for x, v in zip(x_list, v_list):\n        x_dict[x] += v\n    highest_x = max(x_dict, key=x_dict.get)\n    return highest_x\ndef _agg_orm_max(x_list: List[str], v_list: List[float]):\n    text_max = x_list[v_list.index(max(v_list))]\n    return text_max\nAGG_FN_MAP = {\n    MAJORITY_VOTE: _agg_majority_vote,\n    ORM_VOTE: _agg_orm_vote,\n    ORM_MAX: _agg_orm_max,\n}",
        "type": "code",
        "location": "/tsllm/inference/evaluation/vote_utils.py:1-34"
    },
    "367": {
        "file_id": 53,
        "content": "This code defines functions for aggregating votes based on majority and other rules. It uses the Counter class from collections to count votes, and defaultdict from collections for calculating the orm_max. The AGG_FN_MAP dictionary maps different aggregation functions to their corresponding functions in this module.",
        "type": "comment"
    },
    "368": {
        "file_id": 54,
        "content": "/tsllm/inference/lm_self_value.py",
        "type": "filepath"
    },
    "369": {
        "file_id": 54,
        "content": "This code defines a function `tot_value_fn` that generates prompts based on inputs and assigns scores to keywords, while another code calculates the mean of a list of values and returns an array containing all computed means.",
        "type": "summary"
    },
    "370": {
        "file_id": 54,
        "content": "import torch\nfrom typing import Union, List\nfrom tsllm.model import ValueHeadedLLM\nfrom transformers import AutoTokenizer\nimport re\nimport numpy as np\n@torch.inference_mode()\ndef tot_value_fn(\n    critic: ValueHeadedLLM,\n    tokenizer: AutoTokenizer,\n    env_name: str,\n    input_str: Union[List[str], str],\n):\n    if env_name == \"game24\":\n        from envs.game24.prompt import VALUE_PROMPT, VALUE_LAST_STEP_PROMPT\n    else:\n        print(\"tot_value_fn does not support env {}.\".format(env_name))\n        raise NotImplementedError\n    token_batch = []\n    for text in input_str:\n        last_line = text.strip().split(\"\\n\")[-1]\n        if \"left\" in last_line:\n            current_numbers = last_line.split(\"left: \")[-1].split(\")\")[0]\n            prompt = VALUE_PROMPT.format(input=current_numbers)\n        else:\n            inp = text.strip().split(\"\\n\")[1].replace(\"Input: \", \"\")\n            ans = last_line.lower().replace(\"The answer is: \", \"\")\n            prompt = VALUE_LAST_STEP_PROMPT.format(input=inp, answer=ans)\n        prompt_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))",
        "type": "code",
        "location": "/tsllm/inference/lm_self_value.py:1-32"
    },
    "371": {
        "file_id": 54,
        "content": "This code defines a function `tot_value_fn` which takes a critic model, tokenizer, environment name and input string as inputs. It then generates prompts for the given input strings based on the environment type (game24) and encodes them using the tokenizer.",
        "type": "comment"
    },
    "372": {
        "file_id": 54,
        "content": "        token_batch.append(prompt_tokens)\n    step_results = critic.generate_batch(\n        token_batch,\n        sampling_temperature=1.0,\n        sampling_topp=1.0,\n        sampling_topk=100,\n        max_length=128,\n        return_scores=True,\n        include_prompt_in_result=False,\n        end_token=[tokenizer.eos_token_id],\n        static_prompt=None,\n        max_batch_size=0,\n        num_hypotheses=3,  # it is the n_evaluate_sample in tot\n    )\n    values = []\n    value_map = {\"impossible\": 0.001, \"likely\": 1, \"sure\": 20}\n    for res in list(step_results):\n        v_res = []\n        for seq in res.sequences_ids:\n            text = tokenizer.decode(seq)\n            matchs = re.findall(r\"\\bimpossible|sure|likely\\b\", text)\n            if len(matchs) > 0:\n                # it will generate too much imagination, thus we chose only the first one.\n                v_seq = value_map[matchs[0]]\n            else:\n                # default to likely\n                v_seq = value_map[\"likely\"]\n            v_res.append(v_seq)",
        "type": "code",
        "location": "/tsllm/inference/lm_self_value.py:33-62"
    },
    "373": {
        "file_id": 54,
        "content": "This code is generating a batch of sequences using a language model and then evaluating each sequence based on specific keywords. It assigns scores to the sequences based on whether they contain certain words (\"impossible\", \"likely\", or \"sure\"). These scores are stored in values, which may be used later in the program.",
        "type": "comment"
    },
    "374": {
        "file_id": 54,
        "content": "        values.append(np.mean(v_res))\n    return np.array(values)",
        "type": "code",
        "location": "/tsllm/inference/lm_self_value.py:63-65"
    },
    "375": {
        "file_id": 54,
        "content": "This code calculates the mean of a list of values and appends it to another list. Finally, it returns an array containing all computed means.",
        "type": "comment"
    },
    "376": {
        "file_id": 55,
        "content": "/tsllm/inference/trajectory_collector.py",
        "type": "filepath"
    },
    "377": {
        "file_id": 55,
        "content": "The given comments describe a Monte Carlo Tree Search (MCTS) function that generates episodes by selecting actions and updating the MCTS tree based on environment responses, while collecting trajectory data. The function is named `_mcts_rollout_v2` and returns a list of outputs containing generated texts, values, and token counts from an MCTS rollout with optional simulation and token parameters.",
        "type": "summary"
    },
    "378": {
        "file_id": 55,
        "content": "from typing import Optional\nfrom tsllm.envs.base_env import CoTEnv\nfrom tsllm.inference.evaluation.vote_utils import MAJORITY_VOTE\nfrom tsllm.mcts.tree import MCTS\nfrom tsllm.mcts.utils import get_root\nimport time\ndef _mcts_rollout_v1(\n    mcts: MCTS,\n    env: CoTEnv,\n    policy_forward_value,\n    n_rollout: int,\n    reset_total_tree: bool,\n    sample: bool,\n    clear_total_tree: bool,\n):\n    \"\"\"MCTS.GET_NEXT_ACTION\"\"\"\n    output_episodes = []\n    num_generated_token = 0\n    env.reset(True)\n    mcts.root = None\n    done = False\n    for i in range(n_rollout):\n        while not done:\n            action, _, current_node = mcts.get_next_action(\n                env,\n                policy_forward_fn=policy_forward_value,\n                sample=sample,\n                return_tree=True,\n            )\n            mcts.root = current_node.children[action]\n            next_state, reward, terminated, truncated, info = env.step(\n                action, update_legal_action=len(mcts.root.children) == 0\n            )\n            done = terminated or truncated",
        "type": "code",
        "location": "/tsllm/inference/trajectory_collector.py:1-36"
    },
    "379": {
        "file_id": 55,
        "content": "This function, _mcts_rollout_v1, performs Monte Carlo Tree Search (MCTS) for a given environment and policy. It resets the environment and MCTS tree at the start of each iteration. It generates episodes by selecting actions using the provided policy until done condition is met. It updates the MCTS tree based on the environment's response to the selected action. It returns the generated episodes and the number of tokens generated.",
        "type": "comment"
    },
    "380": {
        "file_id": 55,
        "content": "            if not done and len(mcts.root.children) > 0:\n                env._legal_actions = [\n                    {\"action\": a, \"prob\": None} for a in mcts.root.children.keys()\n                ]\n        num_generated_token = mcts.num_generated_token\n        traj_data = {\n            \"path_idx\": i,\n            \"text\": env.answer.strip(),  # drop the last \"\\n\"\n            \"value\": mcts.root.value,\n            \"num_generated_token\": num_generated_token,\n        }\n        output_episodes.append(traj_data)\n        assert not (reset_total_tree and clear_total_tree)  # cannot be both true\n        if reset_total_tree:\n            if i < n_rollout - 1:\n                mcts.root = None\n                env.reset(update_legal_action=True)\n        else:\n            mcts.root = get_root(current_node)\n            if clear_total_tree:\n                mcts.clear_node(mcts.root)\n            env.reset(update_legal_action=False)\n            env._legal_actions = [\n                {\"action\": a, \"prob\": None} for a in mcts.root.children.keys()",
        "type": "code",
        "location": "/tsllm/inference/trajectory_collector.py:38-64"
    },
    "381": {
        "file_id": 55,
        "content": "The code handles the trajectory collection during inference. It updates the legal actions, stores trajectory data for each path, and resets or clears the Monte Carlo Tree Search (MCTS) tree based on specified conditions.",
        "type": "comment"
    },
    "382": {
        "file_id": 55,
        "content": "            ]\n        done = False\n    return output_episodes\ndef _mcts_rollout_v2(\n    mcts: MCTS,\n    env: CoTEnv,\n    policy_forward_value,\n    n_rollout: int,\n    max_simulation: Optional[int],\n    max_token: Optional[int],\n):\n    \"\"\"MCTS.ROLLOUT\"\"\"\n    output_list, num_simulation, root = mcts.rollout(\n        env,\n        n_rollout,\n        policy_forward_value,\n        max_num_simulation=max_simulation,\n        max_token=max_token,\n        return_tree=True,\n    )\n    # texts = [x[\"text\"].strip() for x in output_list]\n    # values = [x[\"value\"] for x in output_list]\n    # num_generated_token = mcts.num_generated_token\n    return output_list  # texts, values, num_generated_token",
        "type": "code",
        "location": "/tsllm/inference/trajectory_collector.py:65-94"
    },
    "383": {
        "file_id": 55,
        "content": "This code defines a function `_mcts_rollout_v2` that performs an MCTS rollout with the given `MCTS` object, environment (`env`), and policy forward value. It returns a list of outputs containing the generated texts, values, and the number of generated tokens. The maximum simulation and token parameters are optional for this function call.",
        "type": "comment"
    },
    "384": {
        "file_id": 56,
        "content": "/tsllm/inference/value.py",
        "type": "filepath"
    },
    "385": {
        "file_id": 56,
        "content": "The code defines the `value_fn` function, which takes a critic model, tokenizer, and input text. It prepares the input text, passes it through the critic model to obtain value predictions for each word, gathers the results, and returns them as numpy float arrays in torch inference mode.",
        "type": "summary"
    },
    "386": {
        "file_id": 56,
        "content": "import torch\nfrom typing import Union, List\nfrom tsllm.model import ValueHeadedLLM\nfrom tsllm.model.modeling_actor_critic import AutoModelForCausalLMWithValueHead\nfrom tsllm.llm.text_generation import llm_gen_ct2\nfrom transformers import AutoTokenizer\nimport re\nimport numpy as np\n@torch.inference_mode()\ndef value_fn(\n    critic: ValueHeadedLLM, tokenizer: AutoTokenizer, input_str: Union[List[str], str]\n):\n    if isinstance(input_str, list):\n        indices2pick = torch.LongTensor(\n            [len(tokenizer.encode(txt)) - 1 for txt in input_str]\n        )\n    else:\n        indices2pick = torch.LongTensor([len(tokenizer.encode(input_str)) - 1])\n    # print(input_str)\n    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True).to(critic.device)\n    if \"token_type_ids\" in inputs:\n        inputs.pop(\"token_type_ids\")\n    value = critic(**inputs).value.cpu()\n    value = value.gather(1, indices2pick.unsqueeze_(1)).squeeze_(1).float().numpy()\n    return value\n@torch.inference_mode()\ndef value_fn_rlhf(\n    critic: AutoModelForCausalLMWithValueHead,",
        "type": "code",
        "location": "/tsllm/inference/value.py:1-33"
    },
    "387": {
        "file_id": 56,
        "content": "This code defines the function `value_fn` which takes a critic model, tokenizer, and input text as inputs. It prepares the input text, passes it through the critic model to obtain value predictions for each word in the input text. The returned values are then gathered and converted into numpy float arrays.",
        "type": "comment"
    },
    "388": {
        "file_id": 56,
        "content": "    tokenizer: AutoTokenizer,\n    input_str: Union[List[str], str],\n):\n    if isinstance(input_str, list):\n        indices2pick = torch.LongTensor(\n            [len(tokenizer.encode(txt)) - 1 for txt in input_str]\n        )\n    else:\n        indices2pick = torch.LongTensor([len(tokenizer.encode(input_str)) - 1])\n    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True).to(critic.device)\n    value = critic(**inputs, return_dict=True).value.cpu()\n    value = value.gather(1, indices2pick.unsqueeze_(1)).squeeze_(1).float().numpy()\n    return value\n@torch.inference_mode()\ndef seq_value_fn(critic_model, tokenizer, input_str):\n    input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids.to(\n        critic_model.device\n    )\n    value = critic_model(input_ids, return_dict=True).value\n    return value.cpu().float().numpy()",
        "type": "code",
        "location": "/tsllm/inference/value.py:34-55"
    },
    "389": {
        "file_id": 56,
        "content": "This function takes a critic model, tokenizer, and input string. It converts the input string into tokenized tensors using the tokenizer and moves them to the device of the critic model. Then, it calculates the value using the critic model and returns it as a float numpy array. The `@torch.inference_mode()` decorator enables torch inference mode for this function.",
        "type": "comment"
    },
    "390": {
        "file_id": 57,
        "content": "/tsllm/llm/ct2_utils.py",
        "type": "filepath"
    },
    "391": {
        "file_id": 57,
        "content": "The code defines a model loading function, `load_ct2_model`, that initializes a ctranslate2 model and provides an `OnlineHfConverter` class that extends `TransformersConverter` for translation tasks. The `trust_remote_code` parameter is not utilized in this context.",
        "type": "summary"
    },
    "392": {
        "file_id": 57,
        "content": "import ctranslate2\nfrom ctranslate2.converters import TransformersConverter\nfrom typing import Optional, List\nfrom transformers import PreTrainedModel\nimport os\nimport sentencepiece as spm\ndef load_ct2_model(ct2_model_path, **generator_kwargs):\n    ct2_generator = ctranslate2.Generator(ct2_model_path, **generator_kwargs)\n    ct2_sp = None\n    # spm.SentencePieceProcessor(\n    #     os.path.join(ct2_model_path, \"tokenizer.model\"))\n    return ct2_generator, ct2_sp\nclass OnlineHfConverter(TransformersConverter):\n    \"\"\"Initializes the converter.\n    Arguments:\n      model_name_or_path: Name of the pretrained model to download, or path to the\n        directory containing the pretrained model.\n      activation_scales: Path to the pre-computed activation scales. Models may\n        use them to rescale some weights to smooth the intermediate activations\n        and improve the quantization accuracy. See\n        https://github.com/mit-han-lab/smoothquant.\n      copy_files: List of filenames to copy from the Hugging Face model to the",
        "type": "code",
        "location": "/tsllm/llm/ct2_utils.py:1-27"
    },
    "393": {
        "file_id": 57,
        "content": "Function `load_ct2_model` loads a ctranslate2 model at the specified path with optional generator kwargs, and initializes the SentencePieceProcessor (`ct2_sp`) from the tokenizer.model file in the same directory. The `OnlineHfConverter` class extends `TransformersConverter`, initializing the converter with arguments for model name or path, activation scales, and copying additional files as needed.",
        "type": "comment"
    },
    "394": {
        "file_id": 57,
        "content": "        converted model directory.\n      load_as_float16: Load the model weights as float16. More precisely, the model\n        will be loaded with ``from_pretrained(..., torch_dtype=torch.float16)``.\n      revision: Revision of the model to download from the Hugging Face Hub.\n      low_cpu_mem_usage: Enable the flag ``low_cpu_mem_usage`` when loading the model\n        with ``from_pretrained``.\n      trust_remote_code: Allow converting models using custom code.\n    \"\"\"\n    def __init__(\n        self,\n        model: Optional[PreTrainedModel],\n        model_name_or_path: str,\n        activation_scales: Optional[str] = None,\n        copy_files: Optional[List[str]] = None,\n        load_as_float16: bool = False,\n        revision: Optional[str] = None,\n        low_cpu_mem_usage: bool = False,\n        trust_remote_code: bool = False,\n    ):\n        super().__init__(\n            model_name_or_path,\n            activation_scales,\n            copy_files,\n            load_as_float16,\n            revision,\n            low_cpu_mem_usage,",
        "type": "code",
        "location": "/tsllm/llm/ct2_utils.py:28-54"
    },
    "395": {
        "file_id": 57,
        "content": "The code defines a class with an __init__ method that initializes an instance of the class. It takes in parameters like model, model_name_or_path, activation_scales, copy_files, load_as_float16, revision, low_cpu_mem_usage, and trust_remote_code. The class inherits from a superclass, and these parameters are used to configure the instance.",
        "type": "comment"
    },
    "396": {
        "file_id": 57,
        "content": "            trust_remote_code,\n        )\n        self.model = model\n    def load_model(self, model_class, model_name_or_path, **kwargs):\n        if self.model is None:\n            return model_class.from_pretrained(model_name_or_path, **kwargs)\n        else:\n            return self.model",
        "type": "code",
        "location": "/tsllm/llm/ct2_utils.py:55-63"
    },
    "397": {
        "file_id": 57,
        "content": "This code defines a class that initializes a model and provides a method to load the model. The `trust_remote_code` parameter is used for trusting remote code, but it is not utilized in this context. If the model is None, it loads the model using the provided `model_class` and `model_name_or_path`. Otherwise, it returns the already initialized model.",
        "type": "comment"
    },
    "398": {
        "file_id": 58,
        "content": "/tsllm/llm/text_generation.py",
        "type": "filepath"
    },
    "399": {
        "file_id": 58,
        "content": "The code utilizes the ChatGLM model for text generation, and handles cases where a specific substring is not found in the original text by tokenizing EOS tokens, checking for duplicates, and updating log probabilities.",
        "type": "summary"
    }
}