{
    "800": {
        "file_id": 88,
        "content": "/tsllm/rl/data/sft_buffer.py",
        "type": "filepath"
    },
    "801": {
        "file_id": 88,
        "content": "The code defines a `SFTBuffer` class for storing `SftInstance` objects with methods for batching, padding, and collating data. It also includes a function `collate_fn` that pads input IDs and labels before returning an instance of `SftBatch`. The class inherits from `Dataset` and takes a pad token id and ignore index as parameters.",
        "type": "summary"
    },
    "802": {
        "file_id": 88,
        "content": "from typing import List, Sequence\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom functools import partial\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tsllm.rl.data.node_types import SftBatch, SftInstance\ndef collate_fn(\n    pad_token_id: int,\n    IGNORE_INDEX: int,\n    elems: Sequence[SftInstance],\n) -> SftBatch:\n    input_ids = pad_sequence(\n        [elem.input_ids for elem in elems],\n        padding_value=pad_token_id,\n        batch_first=True,\n    )\n    label = pad_sequence(\n        [elem.label for elem in elems],\n        padding_value=IGNORE_INDEX,\n        batch_first=True,\n    )\n    attn_mask = input_ids.ne(pad_token_id)\n    returns = pad_sequence(\n        [elem.returns for elem in elems],\n        padding_value=0.0,\n        batch_first=True,\n    )\n    mask = pad_sequence(\n        [elem.mask for elem in elems],\n        padding_value=0,\n        batch_first=True,\n    )\n    return SftBatch(input_ids, label, attn_mask, returns, mask)\nclass SFTBuffer(Dataset):\n    def __init__(self, pad_token_id, IGNORE_INDEX=-100) -> None:",
        "type": "code",
        "location": "/tsllm/rl/data/sft_buffer.py:1-43"
    },
    "803": {
        "file_id": 88,
        "content": "The code defines a function `collate_fn` that takes a sequence of `SftInstance` objects, pads the input IDs and labels with specified values, and returns an instance of `SftBatch` containing the padded tensors. The class `SFTBuffer` inherits from `Dataset`, indicating it implements methods to load and return data in batches. The constructor takes a pad token id and an optional ignore index for label padding.",
        "type": "comment"
    },
    "804": {
        "file_id": 88,
        "content": "        super().__init__()\n        self.history: List[SftInstance] = []\n        self.IGNORE_INDEX = IGNORE_INDEX\n        self.pad_token_id = pad_token_id\n    def push(self, exps: Sequence[SftInstance]):\n        self.history += exps\n    def add(self, inst: SftInstance):\n        self.history.append(inst)\n    def clear(self):\n        self.history = []\n    def __len__(self):\n        return len(self.history)\n    def __getitem__(self, index):\n        return self.history[index]\n    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n        return DataLoader(\n            self,\n            batch_size,\n            shuffle=shuffle,\n            collate_fn=partial(collate_fn, self.pad_token_id, self.IGNORE_INDEX),\n        )",
        "type": "code",
        "location": "/tsllm/rl/data/sft_buffer.py:44-71"
    },
    "805": {
        "file_id": 88,
        "content": "This code defines a buffer class for storing instances of SftInstance. The class has methods for pushing instances, adding single instance, clearing all instances, retrieving the length and accessing specific instances. It also includes a method to create a DataLoader for batching and collating data with padding and ignore index specified.",
        "type": "comment"
    },
    "806": {
        "file_id": 89,
        "content": "/tsllm/rl/data/traj_buffer.py",
        "type": "filepath"
    },
    "807": {
        "file_id": 89,
        "content": "The code includes a `collate_fn` function and TrajBuffer class for managing trajectory instances, along with the MultiTrajBuffer class to handle multiple buffers for different tasks. The code uses a dataloader to iterate over data and prepares it for model training or evaluation by loading, batching, shuffling, clearing history, and retrieving elements.",
        "type": "summary"
    },
    "808": {
        "file_id": 89,
        "content": "from typing import List, Sequence, Dict\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom functools import partial\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nfrom tsllm.rl.data.node_types_new import TrajBatch, TrajInstance\nimport bisect\nimport json\nimport os\ndef collate_fn(\n    pad_token_id: int,\n    IGNORE_INDEX: int,\n    elems: Sequence[TrajInstance],\n) -> TrajBatch:\n    input_ids = pad_sequence(\n        [elem.input_ids for elem in elems],\n        padding_value=pad_token_id,\n        batch_first=True,\n    )\n    label = pad_sequence(\n        [elem.label for elem in elems],\n        padding_value=IGNORE_INDEX,\n        batch_first=True,\n    )\n    attn_mask = input_ids.ne(pad_token_id)\n    # result = torch.cat([elem.result for elem in elems])\n    returns = pad_sequence(\n        [elem.returns for elem in elems],\n        padding_value=0.0,\n        batch_first=True,\n    )\n    mask = pad_sequence(\n        [elem.mask for elem in elems],",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:1-39"
    },
    "809": {
        "file_id": 89,
        "content": "This code defines a function called \"collate_fn\" that takes in a padding token ID, an ignore index, and a sequence of TrajInstance objects. It then pads the input IDs, labels, attn_mask, and returns using the provided padding values and returns a TrajBatch object containing the padded data.",
        "type": "comment"
    },
    "810": {
        "file_id": 89,
        "content": "        padding_value=0,\n        batch_first=True,\n    )\n    return TrajBatch(input_ids, label, attn_mask, returns, mask)\nclass TrajBuffer(Dataset):\n    def __init__(self, max_size, pad_token_id, IGNORE_INDEX=-100) -> None:\n        super().__init__()\n        self.history: List[TrajInstance] = []\n        self.max_size = max_size\n        self.IGNORE_INDEX = IGNORE_INDEX\n        self.pad_token_id = pad_token_id\n    def push(self, exps: Sequence[TrajInstance]):\n        self.history += exps\n    def add(self, inst: TrajInstance):\n        # add example\n        # check whether the traj is the same with history\n        # check whether the buffer is full\n        result = \"\"\n        for d in self.history:\n            # need double check\n            if inst.response == d.response:\n                result = \"repeat\"\n                break\n        if not result == \"repeat\":\n            if len(self.history) == self.max_size:\n                self.history.pop(0)\n                result = \"full\"\n            self.history.append(inst)\n        return result",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:40-74"
    },
    "811": {
        "file_id": 89,
        "content": "This code defines a class `TrajBuffer` for managing a buffer of trajectory instances. It keeps a list of these instances in `self.history`, has a maximum size (`self.max_size`), and ignores specific values (`IGNORE_INDEX`). The `push` method appends new instances, while the `add` method checks for duplicates before adding new instances or removing old ones if the buffer is full.",
        "type": "comment"
    },
    "812": {
        "file_id": 89,
        "content": "    def clear(self):\n        self.history = []\n    def __len__(self):\n        return len(self.history)\n    def __getitem__(self, index):\n        return self.history[index]\n    def save(self, path):\n        # save the databuffer\n        data_dict_list = [\n            {\"question\": data.question, \"response\": data.response}\n            for data in self.history\n        ]\n        directory = os.path.dirname(path)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        with open(path, \"w\") as file:\n            for entry in data_dict_list:\n                json_str = json.dumps(entry)\n                file.write(json_str + \"\\n\")\n    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n        return DataLoader(\n            self,\n            batch_size,\n            shuffle=shuffle,\n            collate_fn=partial(collate_fn, self.pad_token_id, self.IGNORE_INDEX),\n        )\nclass MultiTrajBuffer(Dataset):\n    def __init__(\n        self,\n        num,\n        per_problem_max_size,\n        pad_token_id,",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:76-113"
    },
    "813": {
        "file_id": 89,
        "content": "This code defines a class for a dataset buffer containing multiple trajectory data. It has methods to clear, get item length, retrieve items, save the data, and create a DataLoader object for efficient batch processing during training. The dataset is represented by instances of the MultiTrajBuffer class.",
        "type": "comment"
    },
    "814": {
        "file_id": 89,
        "content": "        IGNORE_INDEX=-100,\n        buffer_name=\"SFT\",\n    ) -> None:\n        super().__init__()\n        self.num = num\n        self.buffer_name = buffer_name\n        self.IGNORE_INDEX = IGNORE_INDEX\n        self.pad_token_id = pad_token_id\n        self.history: Dict[TrajBuffer] = {\n            i: TrajBuffer(per_problem_max_size, pad_token_id, IGNORE_INDEX)\n            for i in range(num)\n        }\n    def add(self, idx: int, inst: TrajInstance):\n        result = self.history[idx].add(inst)\n        if result == \"full\":\n            print_with_rank(f\"The {idx} buffer is full, pop out the first traj\")\n        elif result == \"repeat\":\n            print_with_rank(f\"The {idx} buffer has the same example\")\n        else:\n            pass\n    def save(self, path):\n        import os\n        for i in range(self.num):\n            self.history[i].save(\n                os.path.join(path, f\"Buffer_{self.buffer_name}_{i}.jsonl\")\n            )\n    def clear_idx(self, idx: int):\n        self.history[idx].clear()\n    def clear_all(self):",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:114-148"
    },
    "815": {
        "file_id": 89,
        "content": "This class represents a collection of `TrajBuffer` instances, each having the same configuration and serving a specific task. The `add` method adds an instance to one of these buffers, and if the buffer is full or contains a repeated example, it prints a message. The `save` method saves all buffer instances as separate JSONL files, and the `clear_idx` and `clear_all` methods clear the specified buffer or all buffers respectively.",
        "type": "comment"
    },
    "816": {
        "file_id": 89,
        "content": "        for i in range(self.num):\n            self.history[i].clear()\n    def cumsum(self, sequence):\n        r, s = [], 0\n        for e in range(self.num):\n            l = len(sequence[e])\n            r.append(l + s)\n            s += l\n        return r\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n    @property\n    def cumulative_sizes(self):\n        return self.cumsum(self.history)\n    def __getitem__(self, idx):\n        if idx < 0:\n            if -idx > len(self):\n                raise ValueError(\n                    \"absolute value of index should not exceed dataset length\"\n                )\n            idx = len(self) + idx\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.history[dataset_idx][sample_idx]\n    def create_loader(self, batch_size: int, shuffle: bool) -> DataLoader:\n        return DataLoader(\n            self,",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:149-183"
    },
    "817": {
        "file_id": 89,
        "content": "This code defines a custom data structure, likely for storing trajectory-like data. It has methods to clear its internal history, calculate cumulative sizes of the stored sequences, and retrieve elements at specific indices. It also provides a DataLoader to create an iterable loader for training or testing.",
        "type": "comment"
    },
    "818": {
        "file_id": 89,
        "content": "            batch_size,\n            shuffle=shuffle,\n            collate_fn=partial(collate_fn, self.pad_token_id, self.IGNORE_INDEX),\n        )\nif __name__ == \"__main__\":\n    # test\n    multi_buffer = MultiTrajBuffer(num=2, per_problem_max_size=3, pad_token_id=0)\n    sample = TrajInstance(\n        torch.ones(2),\n        torch.ones(3),\n        torch.tensor([1]),\n        torch.ones(3),\n        torch.ones(3),\n        \"hello\",\n    )\n    negative_sample = TrajInstance(\n        -torch.ones(3),\n        -torch.ones(3),\n        torch.tensor([0]),\n        -torch.ones(3),\n        -torch.ones(3),\n        \"hello\",\n    )\n    multi_buffer.add(idx=0, inst=sample)\n    multi_buffer.add(idx=0, inst=negative_sample)\n    multi_buffer.add(idx=1, inst=sample)\n    multi_buffer.add(idx=1, inst=negative_sample)\n    print(multi_buffer.history[0].history, multi_buffer.history[1].history)\n    multi_buffer.history[0].history[0].input_ids = torch.zeros(3)\n    # print(multi_buffer[0], multi_buffer[1])\n    dataloader = multi_buffer.create_loader(2, False)",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:184-216"
    },
    "819": {
        "file_id": 89,
        "content": "This code initializes a MultiTrajBuffer object with specified parameters, adds positive and negative samples to it, modifies the input_ids of the first sample, and then creates a DataLoader object for accessing the buffer in batches.",
        "type": "comment"
    },
    "820": {
        "file_id": 89,
        "content": "    for data in dataloader:\n        print(data)",
        "type": "code",
        "location": "/tsllm/rl/data/traj_buffer.py:217-218"
    },
    "821": {
        "file_id": 89,
        "content": "The code iterates over data from a dataloader and prints each data batch. This likely prepares data for model training or evaluation. The dataloader might be responsible for loading, batching, and shuffling the data.",
        "type": "comment"
    },
    "822": {
        "file_id": 90,
        "content": "/tsllm/rl/trainer/base_trainer.py",
        "type": "filepath"
    },
    "823": {
        "file_id": 90,
        "content": "The code snippet includes an abstract class for MCTS-based model training and three methods: get_arch, setup_optimizer, and another unnamed method. The get_arch method returns a model architecture based on the configuration and handles backward compatibility. The setup_optimizer method returns an optimizer based on the trainer's TRLConfig. If a model path is specified, it loads a pre-existing model instead of initializing a new one.",
        "type": "summary"
    },
    "824": {
        "file_id": 90,
        "content": "from abc import abstractmethod, ABC\nfrom ast import List\nfrom functools import partial\nimport json\nimport os\nimport sys\nfrom typing import Callable, Iterable, Optional\nfrom accelerate import Accelerator\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nfrom tsllm.distributed.utils import print_with_rank\nfrom tsllm.rl.trainer.opt_utils import get_optimizer_class\nfrom tsllm.rl.trainer.utils import flatten_dict, get_distributed_config, get_git_tag\nclass BaseMCTSTrainer(ABC):\n    def __init__(self, config):\n        self.config = config\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=config.train.gradient_accumulation_steps,\n            log_with=config.train.tracker,\n            project_dir=config.train.logging_dir,\n            split_batches=True,\n        )\n        self.accelerator.state.deepspeed_plugin.deepspeed_config[\n            \"train_micro_batch_size_per_gpu\"\n        ] = config.train.micro_batch_size\n        if int(os.environ.get(\"WORLD_SIZE\", 1)) > 1:",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:1-32"
    },
    "825": {
        "file_id": 90,
        "content": "BaseMCTSTrainer is an abstract class that serves as the base for training a model using Monte Carlo Tree Search (MCTS) algorithm. It initializes the Accelerator object with gradient accumulation steps, DeepSpeed plugin configuration, and micro-batch size per GPU. If the world size is greater than 1, it handles distributed training settings.",
        "type": "comment"
    },
    "826": {
        "file_id": 90,
        "content": "            torch.distributed.barrier(device_ids=[int(os.environ.get(\"LOCAL_RANK\", 0))])\n        self.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            config.tokenizer.tokenizer_path, cache_dir=self.config.model.cache_dir\n        )\n        self.tokenizer.padding_side = config.tokenizer.padding_side\n        self.tokenizer.truncation_side = config.tokenizer.truncation_side\n        # FIXME: this may not be right and not general for all tokenizer.\n        if not self.tokenizer.pad_token:\n            self.tokenizer.pad_token = \"<|padding|>\"\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n        self.tokenizer.sep_token = \"<sep>\"\n    @abstractmethod\n    def learn(self):\n        raise NotImplementedError\n    @property\n    def unwrap_model(self):\n        # unwrap_model = self.model\n        unwrap_model = self.accelerator.unwrap_model(self.actor_critic_model)  # .module",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:33-57"
    },
    "827": {
        "file_id": 90,
        "content": "The code sets up the environment for distributed training, initializes a tokenizer, and defines an abstract method `learn` to be implemented by derived classes. It also introduces a property `unwrap_model` that returns the unwrapped model using the accelerator's `unwrap_model` function. The code appears to be part of a machine learning model trainer.",
        "type": "comment"
    },
    "828": {
        "file_id": 90,
        "content": "        return unwrap_model\n    def save_pretrained(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Save the underlying Hugging Face model, tokenizer, and configuration files to a directory for\n        later use.\n        Args:\n            directory (str, *optional*): The directory to save the trainer files to.\n                NOTE: If not specified, the model will be saved to a directory named `hf_model` in the\n                checkpoint directory as specified by the Trainer's config.\n            **kwargs: Additional keyword arguments passed to the underlying Hugging Face model's\n                `save_pretrained` method.\n        \"\"\"\n        if directory is None:\n            directory = os.path.join(self.config.train.checkpoint_dir, \"hf_model\")\n        self.accelerator.wait_for_everyone()\n        self.accelerator.unwrap_model(self.model).save_pretrained(\n            directory,\n            save_function=self.accelerator.save,\n            is_main_process=self.accelerator.is_main_process,\n            state_dict=self.accelerator.get_state_dict(self.model),",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:58-79"
    },
    "829": {
        "file_id": 90,
        "content": "This code snippet is a part of the Trainer class in a deep learning model. It defines two methods - `unwrap_model` and `save_pretrained`. The `unwrap_model` method returns the unwrapped model, useful for saving or exporting purposes. The `save_pretrained` method saves the underlying Hugging Face model, tokenizer, and configuration files to a specified directory. If no directory is provided, it uses the checkpoint directory with a \"hf_model\" subdirectory. It also handles distributed training by waiting for all processes to complete before saving and providing necessary arguments to save the model correctly.",
        "type": "comment"
    },
    "830": {
        "file_id": 90,
        "content": "            **kwargs,\n        )\n        if self.accelerator.is_main_process:\n            self.tokenizer.save_pretrained(directory)\n    def save(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Creates a checkpoint of the optimizer, scheduler and model\"\"\"\n        dst_dir = directory or self.config.train.checkpoint_dir\n        self.accelerator.save_state(dst_dir, **kwargs)\n        if (\n            self.config.model.peft_config is not None\n            and self.accelerator.is_main_process\n        ):\n            # Remove \"pytorch_model.bin\" because it contains more than necessary,\n            # let save_pretrained recreate it with just the value heads.\n            model_file = os.path.join(dst_dir, \"pytorch_model.bin\")\n            if os.path.exists(model_file):\n                os.remove(model_file)\n            self.accelerator.unwrap_model(self.model).save_pretrained(dst_dir)\n    def save_config(self, directory: Optional[str] = None):\n        dst_dir = directory or self.config.train.checkpoint_dir\n        config_path = os.path.join(dst_dir, \"trainer_config.json\")",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:80-104"
    },
    "831": {
        "file_id": 90,
        "content": "The code defines three methods for saving the trainer's state, optimizer, scheduler, and model. The `save` method creates a checkpoint in the specified directory or the default one defined in the configuration file. It uses accelerator's save_state function to save the state of the trainer. If PEFT (Parameter-Efficient Fine-Tuning) is enabled, it removes the \"pytorch_model.bin\" file before saving the model again to include only value heads. The `save_config` method saves the trainer's configuration file in the specified directory or default checkpoint directory.",
        "type": "comment"
    },
    "832": {
        "file_id": 90,
        "content": "        if self.accelerator.is_main_process:\n            json.dump(self.config.to_dict(), open(config_path, \"w\"), indent=2)\n            print_with_rank(\"Saving config to {}\".format(config_path))\n    def load(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Load checkpoint of optimizer, scheduler and a model\"\"\"\n        if self.config.model.peft_config is not None:\n            def load_state_hook(models: List[torch.nn.Module], input_dir: str):\n                with self.accelerator.main_process_first():\n                    for model in models:\n                        model.from_pretrained(input_dir)\n            self.accelerator.register_load_state_pre_hook(load_state_hook)\n            strict = False\n        else:\n            strict = True\n        self.accelerator.load_state(\n            directory or self.config.train.checkpoint_dir,\n            load_module_strict=strict,\n            **kwargs,\n        )\n    def setup_tracker(self):\n        script_name = os.path.basename(sys.argv[0]).rsplit(\".\", 1)[0]",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:105-131"
    },
    "833": {
        "file_id": 90,
        "content": "The code saves the config to a specified path, loads checkpoints for optimizer, scheduler, and model from a given directory, and sets up a tracker. The code also includes a condition to only save the config if it's the main process, and it registers a load state pre-hook when using PEFT.",
        "type": "comment"
    },
    "834": {
        "file_id": 90,
        "content": "        if not isinstance(self.config.model.model_path, str):\n            model_name = str(self.config.model.model_path).split()[0]\n        else:\n            model_name = self.config.model.model_path.split(\"/\")[-1]\n        if self.accelerator.num_processes == 1:\n            num_gpus = \"1gpu\"\n        else:\n            num_gpus = f\"{self.accelerator.num_processes}gpus\"\n        branch = get_git_tag()[0]\n        run_name = \"/\".join([script_name, model_name, num_gpus]) + f\":{branch}\"\n        if self.accelerator.is_main_process:\n            config_dict = self.config.to_dict()\n            dist_config = get_distributed_config(self.accelerator)\n            config_dict[\"distributed\"] = dist_config\n            init_trackers_kwargs = {}\n            if self.config.train.tracker == \"wandb\":\n                init_trackers_kwargs[\"wandb\"] = {\n                    \"name\": run_name,\n                    \"entity\": self.config.train.entity_name,\n                    \"group\": self.config.train.group_name,\n                    \"tags\": self.config.train.tags + [\"/\".join(get_git_tag())],",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:132-156"
    },
    "835": {
        "file_id": 90,
        "content": "This code snippet is responsible for generating a run name and configuring distributed training. It checks the model path, number of GPUs, and retrieves the branch from git tag to construct the run name. If in the main process, it converts the configuration to dictionary form, gets distributed configuration, and adds it to the main configuration. Then it initializes trackers based on the specified tracker type (WandB in this case) with the generated run name, entity name, group name, and tags including git branch information.",
        "type": "comment"
    },
    "836": {
        "file_id": 90,
        "content": "                    \"mode\": \"disabled\" if os.environ.get(\"debug\", False) else \"online\",\n                }\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name,\n                    config=config_dict,\n                    init_kwargs=init_trackers_kwargs,\n                )\n            elif self.config.train.tracker == \"tensorboard\":\n                # flatten config for tensorboard, split list in hparams into flatten config\n                if config_dict[\"model\"].get(\n                    \"peft_config\", None\n                ):  # tensorboard does not support peft config type\n                    config_dict[\"model\"][\"peft_config\"] = str(\n                        config_dict[\"model\"][\"peft_config\"]\n                    )\n                config_dict_flat = flatten_dict(config_dict)\n                config_dict_flat[\"optimizer/kwargs/beta_1\"] = config_dict_flat[\n                    \"optimizer/kwargs/betas\"\n                ][0]\n                config_dict_flat[\"optimizer/kwargs/beta_2\"] = config_dict_flat[",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:157-177"
    },
    "837": {
        "file_id": 90,
        "content": "This code checks if the debug environment variable is set, and sets the mode accordingly. It then initializes trackers for training, either using the \"disabled\" or \"online\" mode based on the debug setting. If the tracker is set to \"tensorboard\", it flattens the config dictionary and handles the \"peft_config\" special case before initializing the trackers.",
        "type": "comment"
    },
    "838": {
        "file_id": 90,
        "content": "                    \"optimizer/kwargs/betas\"\n                ][1]\n                config_dict_flat.pop(\"optimizer/kwargs/betas\", None)\n                for ix, tag in enumerate(config_dict_flat.pop(\"train/tags\")):\n                    config_dict_flat[f\"train/tag_{ix}\"] = tag\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name,\n                    config=config_dict_flat,\n                )\n            elif self.config.train.tracker is None:\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name\n                )\n            else:\n                raise ValueError(\n                    f\"Only supported trackers are `wandb` and `tensorboard`. Got: `{self.config.train.tracker}`. \"\n                    \"Set `tracker` to `None` to disable tracking.\"\n                )\n    def setup_model(self, model_class=AutoModelForCausalLM):\n        \"\"\"\n        Returns a model derived from an instance's TRLConfig\n        \"\"\"",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:178-201"
    },
    "839": {
        "file_id": 90,
        "content": "This code initializes trackers for the trainer. If the config has a specified tracker (either \"wandb\" or \"tensorboard\"), it initializes those trackers. If no tracker is specified, it initializes default trackers. If an unsupported tracker is provided, it raises a ValueError with an appropriate error message. The model setup function returns an instance of the AutoModelForCausalLM class derived from the TRLConfig.",
        "type": "comment"
    },
    "840": {
        "file_id": 90,
        "content": "        model = self.get_arch(self.config, model_class)\n        if self.config.model.model_arch_type == \"seq2seq\":\n            raise NotImplementedError\n        return model\n    def get_arch(self, config, model_class=AutoModelForCausalLM):\n        from_fn = partial(\n            model_class.from_pretrained, cache_dir=self.config.model.cache_dir\n        )\n        # backward-compat: Try to create a randomly initialized architecture from a config\n        if issubclass(type(config.model.model_path), transformers.PretrainedConfig):\n            from_fn = model_class.from_config\n        return from_fn(config.model.model_path)\n    def setup_optimizer(self):\n        \"\"\"\n        Returns an optimizer derived from an instance's TRLConfig\n        \"\"\"\n        optimizer_class = get_optimizer_class(self.config.optimizer.name)\n        optimizer = optimizer_class(\n            self.model.parameters(),\n            **self.config.optimizer.kwargs,\n        )\n        return optimizer",
        "type": "code",
        "location": "/tsllm/rl/trainer/base_trainer.py:202-228"
    },
    "841": {
        "file_id": 90,
        "content": "This code snippet contains three methods in the base_trainer.py file: get_arch, setup_optimizer, and an unnamed method. The get_arch method is responsible for returning a model architecture based on the configuration provided. It handles backward compatibility by attempting to create a randomly initialized architecture if the model path is a pre-trained config. The setup_optimizer method returns an optimizer based on the trainer's TRLConfig. If a model path is specified, it will load a pre-existing model instead of initializing a new one.",
        "type": "comment"
    },
    "842": {
        "file_id": 91,
        "content": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py",
        "type": "filepath"
    },
    "843": {
        "file_id": 91,
        "content": "The code initializes a class for training Monte Carlo Tree Search models, sets up devices and data loaders, prepares MCTS trainer model with epochs, iterators, saves optimizers, and logs progress. It also utilizes accelerator, removes unnecessary files, and loads checkpoints using load state hook or setting `strict=False`.",
        "type": "summary"
    },
    "844": {
        "file_id": 91,
        "content": "import gc\nimport math\nimport os\nfrom typing import List, Optional\nimport numpy as np\nimport torch\nimport transformers\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport time\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nfrom tsllm.envs import get_env_datasets, get_default_sft_data_builder\nfrom tsllm.rl.config import TrainConfig\nfrom tsllm.rl.data.node_types_new import TrajBatch, TrajInstance\nfrom tsllm.rl.data.traj_buffer import MultiTrajBuffer\nfrom tsllm.rl.trainer.base_trainer import BaseMCTSTrainer\nfrom tsllm.rl.trainer.opt_utils import get_scheduler_class\nimport tree as dm_tree\nfrom tqdm import tqdm\nimport json\ndef loop_iter(loader):\n    while True:\n        for x in loader:\n            yield x\ndef load_jsonl(file_path):\n    data_list = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            data = json.loads(line.strip())\n            data_list.append(data)\n    return data_list\nclass AccelerateMCTSTrainer(BaseMCTSTrainer):\n    def __init__(self, config, **kwargs):",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:1-38"
    },
    "845": {
        "file_id": 91,
        "content": "This code imports necessary libraries and defines a class named `AccelerateMCTSTrainer` that inherits from `BaseMCTSTrainer`. The class takes in a configuration object (`config`) and additional keyword arguments (`**kwargs`). The class is likely for training a model using Monte Carlo Tree Search algorithm.\n\nThis code defines a function named `loop_iter` which loops over a data loader, yielding each batch of data. It also includes a helper function called `load_jsonl` that loads data from a JSON lines file into a list of dictionaries. The `AccelerateMCTSTrainer` class likely utilizes these functions for training and processing data during the training process.",
        "type": "comment"
    },
    "846": {
        "file_id": 91,
        "content": "        super().__init__(config, **kwargs)\n        self.model = self.setup_model()\n        # run in pure_bf16\n        self.model = self.model.to(self.accelerator.device, torch.bfloat16)\n        self.opt = self.setup_optimizer()\n        (\n            self.model,\n            self.opt,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.opt,\n        )\n        self.train_q_ds, self.test_q_ds = get_env_datasets(\n            self.config.train.env_name, **self.config.train.task_dataset_kwargs\n        )\n        print_with_rank(\n            \"#train tasks: {}, test_tasks: {}\".format(\n                len(self.train_q_ds), len(self.test_q_ds)\n            )\n        )\n        sampler = DistributedSampler(self.train_q_ds, shuffle=False)\n        self.task_train_loader = DataLoader(\n            self.train_q_ds, batch_size=1, sampler=sampler, shuffle=False\n        )\n        test_sampler = DistributedSampler(self.test_q_ds, shuffle=True)\n        self.task_test_loader = DataLoader(\n            self.test_q_ds, batch_size=1, sampler=test_sampler, shuffle=False",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:39-68"
    },
    "847": {
        "file_id": 91,
        "content": "This code initializes an object, sets up the model and optimizer on the accelerator's device, prepares them for training, gets train and test datasets, creates samplers for both datasets, and sets up data loaders for the tasks.",
        "type": "comment"
    },
    "848": {
        "file_id": 91,
        "content": "        )\n        self.problem_train_iter = loop_iter(self.task_train_loader)\n        self.problem_test_iter = loop_iter(self.task_test_loader)\n        # store imitation data\n        self.sft_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.sft_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"sft\",\n        )\n        self.sft_buffer.clear_all()\n        # question2idx\n        self.q2idx_dict = {}\n        for idx, problem_inst in enumerate(self.task_train_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict[question] = idx\n        # init sft buffer with pre-collect examples\n        sft_data_builder_fn = get_default_sft_data_builder(self.config.train.env_name)\n        if self.config.train.pre_sft_datapath is not None:\n            # predata = load_jsonl(self.config.train.pre_sft_datapath)\n            # for d in predata:\n            #     question = d[\"question\"]",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:69-91"
    },
    "849": {
        "file_id": 91,
        "content": "This code initializes the trainer for a machine comprehension task, setting up data buffers and dictionaries for training and testing. It also prepares the Soft-Finetuning (SFt) buffer with pre-collected examples if available. The code loads question-to-index mappings from the train loader and uses a default SFt data builder based on the environment name.",
        "type": "comment"
    },
    "850": {
        "file_id": 91,
        "content": "            #     if question in self.q2idx_dict.keys():\n            #         # because question here is split by distributed training,\n            #         # so ignore those questions not stored in the dictionary\n            #         task_idx = self.q2idx_dict[question]\n            #         for a in d[\"answer\"]:\n            #             result = 1.0 if a[\"correct\"] else 0.0\n            #             if a[\"correct\"]:\n            #                 self.add_traj(\n            #                     task_idx,\n            #                     question,\n            #                     a[\"text\"],\n            #                     result,\n            #                     add_sft_buffer=True,\n            #                 )\n            sft_data_list = sft_data_builder_fn(\n                jsonl_path=self.config.train.pre_sft_datapath,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n                add_eos_token=True,",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:92-111"
    },
    "851": {
        "file_id": 91,
        "content": "This code checks if a given question is present in the q2idx_dict dictionary. If it is, the task index (task_idx) is retrieved from the dictionary and then iterates over each answer in the \"answer\" list. The result variable is set to 1.0 if the answer is correct and 0.0 otherwise. If the answer is correct, the add_traj function is called with appropriate parameters to store the trajectory data. The sft_data_builder_fn function is then called to build the soft prompt data using the provided arguments.",
        "type": "comment"
    },
    "852": {
        "file_id": 91,
        "content": "            )\n            for sft_data in sft_data_list:\n                self.add_traj(\n                    sft_data[\"idx\"],\n                    query_str=sft_data[\"query_str\"],\n                    response_str=sft_data[\"response_str\"],\n                    result=1,\n                )\n            print_with_rank(\n                \"finish sft buffer initialization. size: {}\".format(\n                    len(self.sft_buffer)\n                )\n            )\n        self.scheduler = self.setup_scheduler()\n        self.scheduler = self.accelerator.prepare(self.scheduler)\n        self.setup_tracker()\n        self.ct2_generator = None\n    @property\n    def unwrap_model(self):\n        # unwrap_model = self.model\n        unwrap_model = self.accelerator.unwrap_model(self.model)  # .module\n        return unwrap_model\n    def setup_scheduler(self):\n        \"\"\"\n        Returns a learning rate scheduler derived from an instance's TRLConfig\n        \"\"\"\n        train_config: TrainConfig = self.config.train\n        assert train_config.train_epoch == 1",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:112-143"
    },
    "853": {
        "file_id": 91,
        "content": "Function initializes the scheduler, unwraps the model for training, and sets up a tracker. It also initializes an sft buffer with provided data and finishes with printing its size. The function asserts that the train epoch is set to 1 in the config.",
        "type": "comment"
    },
    "854": {
        "file_id": 91,
        "content": "        len_sft = torch.tensor(len(self.sft_buffer), device=self.accelerator.device)\n        avg_sft = self.accelerator.gather(len_sft).float()\n        sft_buffer_max_length = avg_sft.max().item()\n        total_training_steps = train_config.epochs * int(\n            sft_buffer_max_length\n            / train_config.sft_micro_batch_size\n            / train_config.gradient_accumulation_steps\n            + 1\n        )\n        print_rank_0(\"Total Training Step: {}.\".format(total_training_steps))\n        scheduler_class = get_scheduler_class(self.config.scheduler.name)\n        if \"warmup\" in self.config.scheduler.name:\n            self.config.scheduler.kwargs[\"num_training_steps\"] = int(\n                total_training_steps\n            )\n        print_rank_0(self.config.scheduler.kwargs)\n        scheduler = scheduler_class(self.opt, **self.config.scheduler.kwargs)\n        return scheduler\n    def prepare_training(self):\n        del self.ct2_generator\n        gc.collect()\n        torch.cuda.empty_cache()\n        self.ct2_generator = None",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:144-168"
    },
    "855": {
        "file_id": 91,
        "content": "This code calculates the total training steps based on the maximum length of the soft (sft) buffer and other parameters. It creates a scheduler class for learning rate adjustment during training. The total number of training steps, configuration details, and scheduler are returned. Additionally, the previous ct2_generator is deleted and GPU memory is cleared to prepare for new training.",
        "type": "comment"
    },
    "856": {
        "file_id": 91,
        "content": "    def loss_fn(self, sftbatch: Optional[TrajBatch] = None):\n        stats = {}\n        sftbatch.to(self.accelerator.device)\n        # ============\n        # sft policy loss\n        # ============\n        sft_output = self.model(\n            input_ids=sftbatch.input_ids,\n            labels=sftbatch.label,\n            attention_mask=sftbatch.attn_mask,\n            return_dict=True,\n            use_cache=False,\n        )\n        policy_loss = sft_output.loss\n        loss = self.config.train.sft_loss_coef * policy_loss\n        stats[\"train/sft_policy_loss\"] = policy_loss.detach().item()\n        stats[\"train/total_loss\"] = loss.detach().item()\n        return loss, stats\n    def add_traj(\n        self,\n        idx: int,\n        query_str: str,\n        response_str: str,\n        result: int,\n    ):\n        # result 1 for right, 0 for wrong\n        def policy_forward_seq_value(x):\n            raise RuntimeError(\"IN SFT TRAINER YOU SHOULD NOT USE CRITIC TO COMPUTE V.\")\n        dummy_value_index = np.zeros(1)\n        dummy_reward_list = np.ones(1)",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:170-202"
    },
    "857": {
        "file_id": 91,
        "content": "The code defines a loss function for the SFT policy and adds a traj (trajectory) to the trainer. The loss is computed from the output of the model, and the loss function returns the total loss and some statistics. If the critic is used to compute V in the SFT trainer, an error will be raised. Dummy values are also set for value index and reward list.",
        "type": "comment"
    },
    "858": {
        "file_id": 91,
        "content": "        self.sft_buffer.add(\n            idx,\n            TrajInstance.from_string(\n                query_str,\n                response_str,\n                dummy_value_index,\n                dummy_reward_list,\n                self.tokenizer,\n                policy_forward_seq_value,\n                self.config.train.gamma,\n                self.config.train.gae_lambda,\n                cal_value=False,\n            ),\n        )\n    def learn(self, iter_count=0):\n        train_config: TrainConfig = self.config.train\n        self.iter_count = iter_count\n        self.train_step = 0\n        for i_epoch in range(train_config.epochs):\n            stats = {}\n            print_with_rank(\"TRAINING\")\n            self.prepare_training()\n            self.model.train()\n            train_sft_dataloader = self.sft_buffer.create_loader(\n                train_config.sft_micro_batch_size, shuffle=True\n            )\n            train_sft_data_iter = loop_iter(train_sft_dataloader)\n            gas = self.config.train.gradient_accumulation_steps",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:203-235"
    },
    "859": {
        "file_id": 91,
        "content": "This code initializes and configures the training process for an MCTS trainer model. It sets up epochs, data loaders, and iterators for training, and prepares the model to start learning from the provided data.",
        "type": "comment"
    },
    "860": {
        "file_id": 91,
        "content": "            # currently use the length of sft buffer as the number of training step\n            len_sft = torch.tensor(\n                len(train_sft_dataloader), device=self.accelerator.device\n            )\n            avg_sft = self.accelerator.gather(len_sft).float()\n            # all sft buffer size\n            all_sft_num = torch.sum(\n                avg_sft.sum() * train_config.sft_micro_batch_size\n            ).item()\n            assert self.config.train.train_epoch == 1\n            nga = int(\n                int(math.ceil(avg_sft.max().item() / gas))\n                * self.config.train.train_epoch\n            )\n            train_stats_list = []\n            t0 = time.time()\n            for i_nga in tqdm(range(nga), disable=not self.local_rank == 0):\n                for i_gas in range(gas):\n                    loss, stats = self.train_iter(train_sft_data_iter)\n                    train_stats_list.append(stats)\n                self.opt.step()\n                self.scheduler.step()\n                self.opt.zero_grad()",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:237-263"
    },
    "861": {
        "file_id": 91,
        "content": "This code calculates the number of training steps based on the length of the SFT (Soft-target) buffer and then performs the training iterations. It uses an optimizer and scheduler to update the model's weights and keeps track of the training statistics in the 'train_stats_list'.",
        "type": "comment"
    },
    "862": {
        "file_id": 91,
        "content": "                stats_gas = dm_tree.map_structure(\n                    lambda *xs: np.mean(xs).item(), *train_stats_list[-gas:]\n                )\n                stats_gas[\"train/learning_rate\"] = self.scheduler.get_last_lr()[0]\n                self.accelerator.log(stats_gas, step=self.train_step)\n                self.train_step += 1\n            t1 = time.time()\n            train_stats = dm_tree.map_structure(\n                lambda *xs: np.mean(xs).item(), *train_stats_list\n            )\n            stats.update(train_stats)\n            stats[\"time/training_time\"] = t1 - t0\n            stats[\"train/sft_buffer_size\"] = all_sft_num\n            if self.local_rank == 0:\n                print_rank_0(\"LOSS: {:.4f}, {}\".format(loss, stats))\n            if self.iter_count % train_config.checkpoint_interval == 0:\n                subfolder = f\"checkpoint_{self.iter_count}_ep{i_epoch}\"\n                directory = os.path.join(train_config.checkpoint_dir, subfolder)\n                print_with_rank(f\"Saving intermediate checkpoint into {directory}\")",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:264-289"
    },
    "863": {
        "file_id": 91,
        "content": "Code snippet calculates statistics for training progress, updates logs and prints loss at the end of each epoch. It also saves intermediate checkpoints periodically based on the provided config settings. The code uses mean averaging to gather stats from previous epochs, handles local rank logging and saving, and tracks total training time.",
        "type": "comment"
    },
    "864": {
        "file_id": 91,
        "content": "                if train_config.save_optimizer:\n                    self.save(directory)\n                else:\n                    self.save_pretrained(directory)\n                self.save_pretrained(\n                    os.path.join(train_config.checkpoint_dir, \"last_model_hf\")\n                )\n            # replace all key\n            for key in list(stats.keys()):\n                if \"loss\" in key:\n                    stats[key.replace(\"loss\", \"average_loss\")] = stats.pop(key)\n            self.accelerator.log(stats, step=self.iter_count)\n            self.iter_count += 1\n    def train_iter(self, train_sft_data_iter):\n        forward_time = -time.time()\n        # onpolicy data loss\n        sftbatch = next(train_sft_data_iter)\n        loss, stats = self.loss_fn(sftbatch)\n        forward_time += time.time()\n        backward_time = -time.time()\n        self.accelerator.backward(loss)\n        backward_time += time.time()\n        stats[\"time/forward_time\"] = forward_time\n        stats[\"time/backward_time\"] = backward_time",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:290-317"
    },
    "865": {
        "file_id": 91,
        "content": "This code snippet is for a machine learning model trainer that performs training iterations. It saves the optimizer if specified, saves the pretrained model in a specific directory, updates statistics, logs statistics to track progress, and measures forward and backward propagation time.",
        "type": "comment"
    },
    "866": {
        "file_id": 91,
        "content": "        return loss.item(), stats\n    def save_pretrained(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Save the underlying Hugging Face model, tokenizer, and configuration files to a directory for\n        later use.\n        Args:\n            directory (str, *optional*): The directory to save the trainer files to.\n                NOTE: If not specified, the model will be saved to a directory named `hf_model` in the\n                checkpoint directory as specified by the Trainer's config.\n            **kwargs: Additional keyword arguments passed to the underlying Hugging Face model's\n                `save_pretrained` method.\n        \"\"\"\n        if directory is None:\n            directory = os.path.join(self.config.train.checkpoint_dir, \"hf_model\")\n        self.accelerator.wait_for_everyone()\n        self.accelerator.unwrap_model(self.model).save_pretrained(\n            directory,\n            save_function=self.accelerator.save,\n            is_main_process=self.accelerator.is_main_process,",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:319-339"
    },
    "867": {
        "file_id": 91,
        "content": "This code defines a method `save_pretrained` for the `Trainer` class, which saves the underlying Hugging Face model, tokenizer, and configuration files to a directory. It takes an optional `directory` argument and additional keyword arguments. If no `directory` is specified, it saves the model to a directory named `hf_model` in the checkpoint directory as per the Trainer's config. The method ensures that all processes have finished before saving the model and uses the accelerator's save function for saving.",
        "type": "comment"
    },
    "868": {
        "file_id": 91,
        "content": "            state_dict=self.accelerator.get_state_dict(self.model),\n            **kwargs,\n        )\n        if self.accelerator.is_main_process:\n            self.tokenizer.save_pretrained(directory)\n    def save(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Creates a checkpoint of the optimizer, scheduler and model\"\"\"\n        dst_dir = directory or self.config.train.checkpoint_dir\n        self.accelerator.save_state(dst_dir, **kwargs)\n        if (\n            self.config.model.peft_config is not None\n            and self.accelerator.is_main_process\n        ):\n            # Remove \"pytorch_model.bin\" because it contains more than necessary,\n            # let save_pretrained recreate it with just the value heads.\n            model_file = os.path.join(dst_dir, \"pytorch_model.bin\")\n            if os.path.exists(model_file):\n                os.remove(model_file)\n            self.accelerator.unwrap_model(self.model).save_pretrained(dst_dir)\n    def load(self, directory: Optional[str] = None, **kwargs):",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:340-363"
    },
    "869": {
        "file_id": 91,
        "content": "The code defines a class with methods for saving and loading model checkpoints, optimizer, and scheduler. It uses accelerator to handle state and file operations. If PEFT configuration is provided, it removes unnecessary files during saving and loading processes.",
        "type": "comment"
    },
    "870": {
        "file_id": 91,
        "content": "        \"\"\"Load checkpoint of optimizer, scheduler and a model\"\"\"\n        if self.config.model.peft_config is not None:\n            def load_state_hook(models: List[torch.nn.Module], input_dir: str):\n                with self.accelerator.main_process_first():\n                    for model in models:\n                        model.from_pretrained(input_dir)\n            self.accelerator.register_load_state_pre_hook(load_state_hook)\n            strict = False\n        else:\n            strict = True\n        self.accelerator.load_state(\n            directory or self.config.train.checkpoint_dir,\n            load_module_strict=strict,\n            **kwargs,\n        )",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py:364-382"
    },
    "871": {
        "file_id": 91,
        "content": "This code is loading a checkpoint of an optimizer, scheduler, and a model. If the `config.model.peft_config` is not None, it registers a load state hook to load all models in the list. Otherwise, it sets `strict=False`. Finally, it loads the state from the directory or config's checkpoint_dir with the given parameters.",
        "type": "comment"
    },
    "872": {
        "file_id": 92,
        "content": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py",
        "type": "filepath"
    },
    "873": {
        "file_id": 92,
        "content": "The code sets up PyTorch DataLoader, initializes buffers and schedulers for training/testing, defines loss function, and utilizes transformers, tqdm for RL and language modeling, ultimately training and evaluating models.",
        "type": "summary"
    },
    "874": {
        "file_id": 92,
        "content": "import gc\nfrom functools import partial\nimport math\nimport os\nimport re\nfrom typing import List, Optional\nimport numpy as np\nimport torch\nimport transformers\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport time\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nfrom tsllm.envs import (\n    get_default_critic_data_builder,\n    get_env_datasets,\n)\nfrom tsllm.model import ValueHeadedLLM, AutoModelForCausalLMWithValueHead\nfrom tsllm.rl.config import TrainConfig\nfrom tsllm.rl.data.node_types_new import TrajBatch, TrajInstance\nfrom tsllm.rl.data.traj_buffer import MultiTrajBuffer\nfrom tsllm.rl.trainer.base_trainer import BaseMCTSTrainer\nfrom tsllm.rl.trainer.opt_utils import get_scheduler_class\nimport tree as dm_tree\nfrom tqdm import tqdm\nimport json\ndef loop_iter(loader):\n    while True:\n        for x in loader:\n            yield x\ndef load_jsonl(file_path):\n    data_list = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            data = json.loads(line.strip())",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:1-38"
    },
    "875": {
        "file_id": 92,
        "content": "This code imports necessary libraries and defines a function `loop_iter` for creating an infinite iterator over a PyTorch DataLoader, as well as a function `load_jsonl` to load data from a JSONL file. It also seems to be part of a larger project related to reinforcement learning (RL) and language modeling, involving classes like `BaseMCTSTrainer`, `TrajBatch`, and `MultiTrajBuffer`. The code uses transformers for natural language processing and tqdm for progress bars.",
        "type": "comment"
    },
    "876": {
        "file_id": 92,
        "content": "            data_list.append(data)\n    return data_list\nclass AccelerateMCTSTrainer(BaseMCTSTrainer):\n    def __init__(self, config, **kwargs):\n        super().__init__(config, **kwargs)\n        if config.model.value_model_type_name == \"ValueHeadLLM\":\n            self.model = self.setup_model(ValueHeadedLLM)\n        elif config.model.value_model_type_name == \"AutoModelForCausalLMWithValueHead\":\n            self.model = self.setup_model(AutoModelForCausalLMWithValueHead)\n        else:\n            raise ValueError(\n                \"Unknown value model type name {}.\".format(\n                    config.model.value_model_type_name\n                )\n            )\n        # run in pure_bf16\n        self.model = self.model.to(torch.bfloat16)\n        self.opt = self.setup_optimizer()\n        (\n            self.model,\n            self.opt,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.opt,\n        )\n        self.train_q_ds, self.test_q_ds = get_env_datasets(\n            self.config.train.env_name, **self.config.train.task_dataset_kwargs",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:39-70"
    },
    "877": {
        "file_id": 92,
        "content": "The code defines a class `AccelerateMCTSTrainer` which extends `BaseMCTSTrainer`. It initializes the model and optimizer based on the configuration's value model type name. The model is converted to bfloat16 for acceleration, and then prepared by the accelerator for training and testing datasets.",
        "type": "comment"
    },
    "878": {
        "file_id": 92,
        "content": "        )\n        sampler = DistributedSampler(self.train_q_ds, shuffle=False)\n        self.task_train_loader = DataLoader(\n            self.train_q_ds, batch_size=1, sampler=sampler, shuffle=False\n        )\n        test_sampler = DistributedSampler(self.test_q_ds, shuffle=True)\n        self.task_test_loader = DataLoader(\n            self.test_q_ds, batch_size=1, sampler=test_sampler, shuffle=False\n        )\n        self.problem_train_iter = loop_iter(self.task_train_loader)\n        self.problem_test_iter = loop_iter(self.task_test_loader)\n        # store on_policy data\n        self.onpolicy_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_buffer.clear_all()\n        self.onpolicy_train_test_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:71-95"
    },
    "879": {
        "file_id": 92,
        "content": "Creates distributed samplers for training and testing data, sets up data loaders, initializes on-policy buffer, and clears it.",
        "type": "comment"
    },
    "880": {
        "file_id": 92,
        "content": "            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_train_test_buffer.clear_all()\n        self.onpolicy_test_buffer = MultiTrajBuffer(\n            num=len(self.task_test_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_test_buffer.clear_all()\n        # question2idx\n        self.q2idx_dict = {}\n        for idx, problem_inst in enumerate(self.task_train_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict[question] = idx\n        # question2idx\n        self.q2idx_dict_test = {}\n        for idx, problem_inst in enumerate(self.task_test_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict_test[question] = idx\n        build_env_offline_data_component_fn = get_default_critic_data_builder(\n            self.config.train.env_name",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:96-122"
    },
    "881": {
        "file_id": 92,
        "content": "This code initializes a MultiTrajBuffer for on-policy training and testing, clears the buffers, creates q2idx dictionaries for both train and test data, and gets an env_offline_data_component_fn.",
        "type": "comment"
    },
    "882": {
        "file_id": 92,
        "content": "        )\n        # init onpolicy buffer with pre-collect examples\n        if self.config.train.pre_onpolicy_datapath is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:\n                self.add_traj(buffer_to_add=self.onpolicy_buffer, **traj_dict)\n            print_with_rank(\"finish onpolicy buffer initialization\")\n        if self.config.train.pre_onpolicy_datapath_train_test is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath_train_test,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:123-144"
    },
    "883": {
        "file_id": 92,
        "content": "Initializes onpolicy buffer with pre-collected data for faster training. It loads the data from a specified path and adds it to the onpolicy_buffer. This feature is beneficial in scenarios where initial data collection can improve model performance.",
        "type": "comment"
    },
    "884": {
        "file_id": 92,
        "content": "                self.add_traj(\n                    buffer_to_add=self.onpolicy_train_test_buffer, **traj_dict\n                )\n            print_with_rank(\"finish onpolicy train test buffer initialization\")\n        if self.config.train.pre_onpolicy_datapath_test is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath_test,\n                q2idx_dict=self.q2idx_dict_test,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:\n                self.add_traj(buffer_to_add=self.onpolicy_test_buffer, **traj_dict)\n            print_with_rank(\"finish onpolicy test buffer initialization\")\n        self.scheduler = self.setup_scheduler()\n        self.scheduler = self.accelerator.prepare(self.scheduler)\n        self.setup_tracker()\n        self.ct2_generator = None\n    def setup_scheduler(self):\n        \"\"\"\n        Returns a learning rate scheduler derived from an instance's TRLConfig",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:145-170"
    },
    "885": {
        "file_id": 92,
        "content": "This code initializes the onpolicy train and test buffers, loads pre-existing data for the onpolicy test buffer if specified in the config, sets up a scheduler, and accelerator.",
        "type": "comment"
    },
    "886": {
        "file_id": 92,
        "content": "        \"\"\"\n        train_config: TrainConfig = self.config.train\n        assert train_config.train_epoch == 1\n        len_sft = torch.tensor(\n            len(self.onpolicy_buffer), device=self.accelerator.device\n        )\n        avg_sft = self.accelerator.gather(len_sft).float()\n        buffer_max_length = avg_sft.max().item()\n        total_training_steps = train_config.epochs * int(\n            buffer_max_length\n            / train_config.micro_batch_size\n            / train_config.gradient_accumulation_steps\n            + 1\n        )\n        print_rank_0(\"Total Training Step: {}.\".format(total_training_steps))\n        scheduler_class = get_scheduler_class(self.config.scheduler.name)\n        if \"warmup\" in self.config.scheduler.name:\n            self.config.scheduler.kwargs[\"num_training_steps\"] = int(\n                total_training_steps\n            )\n        print_rank_0(self.config.scheduler.kwargs)\n        scheduler = scheduler_class(self.opt, **self.config.scheduler.kwargs)\n        return scheduler\n    def prepare_training(self):",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:171-196"
    },
    "887": {
        "file_id": 92,
        "content": "The code initializes a scheduler for training. It calculates the total number of training steps based on epochs, buffer length, micro-batch size and gradient accumulation steps. The scheduler class is then obtained from the given config. If the scheduler name contains \"warmup\", the num_training_steps is set to the total training steps. Finally, the scheduler object is created with the optimizer and scheduler kwargs.",
        "type": "comment"
    },
    "888": {
        "file_id": 92,
        "content": "        del self.ct2_generator\n        gc.collect()\n        torch.cuda.empty_cache()\n        self.ct2_generator = None\n    def loss_fn(self, minibatch: Optional[TrajBatch] = None, setting=\"train\"):\n        stats = {}\n        minibatch.to(self.accelerator.device)\n        onpolicy_output = self.model(\n            input_ids=minibatch.input_ids,\n            #  labels=minibatch.label,\n            attention_mask=minibatch.attn_mask,\n        )\n        value = onpolicy_output.value\n        mask = minibatch.mask\n        returns = minibatch.returns\n        # value loss includes value loss + final step reward loss\n        value_delta = (returns[mask != 0] - value[mask != 0]) ** 2\n        value_loss = value_delta.mean()\n        stats[f\"{setting}/value_loss\"] = value_loss.detach().item()\n        # final step reward statistics\n        reward_loss = (returns[mask == 2] - value[mask == 2]) ** 2\n        reward_loss = reward_loss.mean()\n        stats[f\"{setting}/reward_loss\"] = reward_loss.detach().item()\n        loss = self.config.train.value_loss_coef * value_loss",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:197-225"
    },
    "889": {
        "file_id": 92,
        "content": "This code defines a loss function for a model training. It calculates the value loss and reward loss based on the predicted values from the model and the actual returns. The losses are then combined to compute the final loss. This function also collects garbage, empties CUDA cache, and initializes ct2_generator.",
        "type": "comment"
    },
    "890": {
        "file_id": 92,
        "content": "        stats[f\"{setting}/total_loss\"] = loss.detach().item()\n        return loss, stats\n    def add_traj(\n        self,\n        idx: int,\n        query_str: str,\n        answer: str,\n        value_index: np.array,\n        reward_list: np.array,\n        buffer_to_add: MultiTrajBuffer,\n    ):\n        # result 1 for right, -1 for wrong\n        @torch.inference_mode()\n        def policy_forward_seq_value(input_str):\n            input_ids = self.tokenizer(input_str, return_tensors=\"pt\").input_ids.to(\n                self.accelerator.device\n            )\n            value = self.unwrap_critic_model(input_ids=input_ids).value\n            return value.cpu().float().numpy()\n        buffer_to_add.add(\n            idx,\n            TrajInstance.from_string(\n                query_str,\n                answer,\n                value_index,\n                reward_list,\n                self.tokenizer,\n                policy_forward_seq_value,\n                self.config.train.gamma,\n                self.config.train.gae_lambda,\n                use_gae=False,",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:227-259"
    },
    "891": {
        "file_id": 92,
        "content": "This code snippet contains a function that measures the total loss during training and adds a new trajectory instance to a buffer. The policy_forward_seq_value function is used for forward pass in policy network, and the TrajInstance class is instantiated from query_str, answer, value_index, reward_list with tokenizer and policy_forward_seq_value as inputs. The buffer_to_add's add method is called to include this trajectory instance into the buffer.",
        "type": "comment"
    },
    "892": {
        "file_id": 92,
        "content": "                cal_value=True,\n            ),\n        )\n    def learn(self, iter_count=0):\n        train_config: TrainConfig = self.config.train\n        self.iter_count = iter_count\n        self.train_step = 0\n        for i_epoch in range(train_config.epochs):\n            stats = {}\n            print_with_rank(\"TRAINING\")\n            self.model.train()\n            # if train_config.pure_sft:\n            #    assert train_config.sft_loss_coef is not None\n            train_dataloader = self.onpolicy_buffer.create_loader(\n                train_config.micro_batch_size, shuffle=True\n            )\n            # train_sft_dataloader = self.sft_buffer.create_loader(\n            #    train_config.sft_micro_batch_size, shuffle=True)\n            train_data_iter = loop_iter(train_dataloader)\n            # train_sft_data_iter = loop_iter(train_sft_dataloader)\n            gas = self.config.train.gradient_accumulation_steps\n            len_onpolicy = torch.tensor(\n                len(train_dataloader), device=self.accelerator.device",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:260-288"
    },
    "893": {
        "file_id": 92,
        "content": "The code defines a class with a method called \"learn\" which performs training for a certain number of epochs. It creates data loaders, iterators and initializes variables such as the gradient accumulation steps and the length of the on-policy buffer. The purpose is to train a model using the provided configuration.",
        "type": "comment"
    },
    "894": {
        "file_id": 92,
        "content": "            )\n            avg_onpolicy = self.accelerator.gather(len_onpolicy).float()\n            # all sft buffer size\n            all_onpolicy_num = torch.sum(\n                avg_onpolicy.sum() * train_config.micro_batch_size\n            ).item()\n            assert self.config.train.train_epoch == 1\n            nga = int(\n                int(math.ceil(avg_onpolicy.max().item() / gas))\n                * self.config.train.train_epoch\n            )\n            train_stats_list = []\n            t0 = time.time()\n            for i_nga in tqdm(range(nga), disable=not self.local_rank == 0):\n                for i_gas in range(gas):\n                    loss, cur_step_stats = self.train_iter(train_data_iter)\n                    train_stats_list.append(cur_step_stats)\n                self.opt.step()\n                self.scheduler.step()\n                self.opt.zero_grad()\n                stats_gas = dm_tree.map_structure(\n                    lambda *xs: np.mean(xs).item(), *train_stats_list[-gas:]\n                )",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:289-316"
    },
    "895": {
        "file_id": 92,
        "content": "Computes the average on-policy length and calculates the total on-policy number for all soft updates. Then, it asserts that the train epoch is equal to 1. Next, it calculates the number of gradient accumulation steps (nga) based on the maximum on-policy length divided by gas, multiplied by the train epoch. Finally, it loops through nga and gas iterations, computing loss and step statistics for each iteration and updating optimizer accordingly.",
        "type": "comment"
    },
    "896": {
        "file_id": 92,
        "content": "                stats_gas[\"train/learning_rate\"] = self.scheduler.get_last_lr()[0]\n                self.accelerator.log(stats_gas, step=self.train_step)\n                self.train_step += 1\n            t1 = time.time()\n            train_stats = dm_tree.map_structure(\n                lambda *xs: np.mean(xs).item(), *train_stats_list\n            )\n            stats.update(train_stats)\n            stats[\"time/training_time\"] = t1 - t0\n            stats[\"train/sft_buffer_size\"] = all_onpolicy_num\n            if self.local_rank == 0:\n                print_rank_0(\"LOSS: {:.4f}, {}\".format(loss, stats))\n            if self.iter_count % train_config.checkpoint_interval == 0:\n                subfolder = f\"checkpoint_{self.iter_count}_ep{i_epoch}\"\n                directory = os.path.join(train_config.checkpoint_dir, subfolder)\n                print_with_rank(f\"Saving intermediate checkpoint into {directory}\")\n                if train_config.save_optimizer:\n                    self.save(directory)\n                else:",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:318-342"
    },
    "897": {
        "file_id": 92,
        "content": "This code snippet calculates training loss and updates statistics for each step of the training process. It logs these statistics, calculates the training time elapsed, and saves intermediate checkpoints at specified intervals. The loss is printed if the local_rank is 0. Additionally, it checks if the optimizer should be saved when saving a checkpoint.",
        "type": "comment"
    },
    "898": {
        "file_id": 92,
        "content": "                    self.save_pretrained(directory)\n                self.save_pretrained(\n                    os.path.join(train_config.checkpoint_dir, \"last_model_hf\")\n                )\n                self.save_config()\n            if self.iter_count % train_config.eval_interval == 0:\n                print_with_rank(\"EVALUATING iteration:{}\".format(self.iter_count))\n                eval_stats = self.evaluate()\n                stats.update(eval_stats)\n            # replace all key\n            for key in list(stats.keys()):\n                if \"loss\" in key:\n                    stats[key.replace(\"loss\", \"average_loss\")] = stats.pop(key)\n            self.accelerator.log(stats, step=self.iter_count)\n            self.iter_count += 1\n    def train_iter(self, data_iter):\n        forward_time = -time.time()\n        # onpolicy data loss\n        minibatch = next(data_iter)\n        loss, stats = self.loss_fn(minibatch)\n        forward_time += time.time()\n        backward_time = -time.time()\n        self.accelerator.backward(loss)",
        "type": "code",
        "location": "/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py:343-369"
    },
    "899": {
        "file_id": 92,
        "content": "The code saves the model and config, evaluates the model after specified intervals, updates statistics by replacing \"loss\" with \"average_loss\", logs the updated stats, and performs forward and backward passes for calculating loss.",
        "type": "comment"
    }
}