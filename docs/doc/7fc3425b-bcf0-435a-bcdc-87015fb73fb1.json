{
    "summary": "The code sets up PyTorch DataLoader, initializes buffers and schedulers for training/testing, defines loss function, and utilizes transformers, tqdm for RL and language modeling, ultimately training and evaluating models.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a function `loop_iter` for creating an infinite iterator over a PyTorch DataLoader, as well as a function `load_jsonl` to load data from a JSONL file. It also seems to be part of a larger project related to reinforcement learning (RL) and language modeling, involving classes like `BaseMCTSTrainer`, `TrajBatch`, and `MultiTrajBuffer`. The code uses transformers for natural language processing and tqdm for progress bars.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":0-37",
            "content": "import gc\nfrom functools import partial\nimport math\nimport os\nimport re\nfrom typing import List, Optional\nimport numpy as np\nimport torch\nimport transformers\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport time\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nfrom tsllm.envs import (\n    get_default_critic_data_builder,\n    get_env_datasets,\n)\nfrom tsllm.model import ValueHeadedLLM, AutoModelForCausalLMWithValueHead\nfrom tsllm.rl.config import TrainConfig\nfrom tsllm.rl.data.node_types_new import TrajBatch, TrajInstance\nfrom tsllm.rl.data.traj_buffer import MultiTrajBuffer\nfrom tsllm.rl.trainer.base_trainer import BaseMCTSTrainer\nfrom tsllm.rl.trainer.opt_utils import get_scheduler_class\nimport tree as dm_tree\nfrom tqdm import tqdm\nimport json\ndef loop_iter(loader):\n    while True:\n        for x in loader:\n            yield x\ndef load_jsonl(file_path):\n    data_list = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            data = json.loads(line.strip())"
        },
        {
            "comment": "The code defines a class `AccelerateMCTSTrainer` which extends `BaseMCTSTrainer`. It initializes the model and optimizer based on the configuration's value model type name. The model is converted to bfloat16 for acceleration, and then prepared by the accelerator for training and testing datasets.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":38-69",
            "content": "            data_list.append(data)\n    return data_list\nclass AccelerateMCTSTrainer(BaseMCTSTrainer):\n    def __init__(self, config, **kwargs):\n        super().__init__(config, **kwargs)\n        if config.model.value_model_type_name == \"ValueHeadLLM\":\n            self.model = self.setup_model(ValueHeadedLLM)\n        elif config.model.value_model_type_name == \"AutoModelForCausalLMWithValueHead\":\n            self.model = self.setup_model(AutoModelForCausalLMWithValueHead)\n        else:\n            raise ValueError(\n                \"Unknown value model type name {}.\".format(\n                    config.model.value_model_type_name\n                )\n            )\n        # run in pure_bf16\n        self.model = self.model.to(torch.bfloat16)\n        self.opt = self.setup_optimizer()\n        (\n            self.model,\n            self.opt,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.opt,\n        )\n        self.train_q_ds, self.test_q_ds = get_env_datasets(\n            self.config.train.env_name, **self.config.train.task_dataset_kwargs"
        },
        {
            "comment": "Creates distributed samplers for training and testing data, sets up data loaders, initializes on-policy buffer, and clears it.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":70-94",
            "content": "        )\n        sampler = DistributedSampler(self.train_q_ds, shuffle=False)\n        self.task_train_loader = DataLoader(\n            self.train_q_ds, batch_size=1, sampler=sampler, shuffle=False\n        )\n        test_sampler = DistributedSampler(self.test_q_ds, shuffle=True)\n        self.task_test_loader = DataLoader(\n            self.test_q_ds, batch_size=1, sampler=test_sampler, shuffle=False\n        )\n        self.problem_train_iter = loop_iter(self.task_train_loader)\n        self.problem_test_iter = loop_iter(self.task_test_loader)\n        # store on_policy data\n        self.onpolicy_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_buffer.clear_all()\n        self.onpolicy_train_test_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,"
        },
        {
            "comment": "This code initializes a MultiTrajBuffer for on-policy training and testing, clears the buffers, creates q2idx dictionaries for both train and test data, and gets an env_offline_data_component_fn.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":95-121",
            "content": "            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_train_test_buffer.clear_all()\n        self.onpolicy_test_buffer = MultiTrajBuffer(\n            num=len(self.task_test_loader),\n            per_problem_max_size=self.config.train.onpolicy_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"onpolicy_value\",\n        )\n        self.onpolicy_test_buffer.clear_all()\n        # question2idx\n        self.q2idx_dict = {}\n        for idx, problem_inst in enumerate(self.task_train_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict[question] = idx\n        # question2idx\n        self.q2idx_dict_test = {}\n        for idx, problem_inst in enumerate(self.task_test_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict_test[question] = idx\n        build_env_offline_data_component_fn = get_default_critic_data_builder(\n            self.config.train.env_name"
        },
        {
            "comment": "Initializes onpolicy buffer with pre-collected data for faster training. It loads the data from a specified path and adds it to the onpolicy_buffer. This feature is beneficial in scenarios where initial data collection can improve model performance.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":122-143",
            "content": "        )\n        # init onpolicy buffer with pre-collect examples\n        if self.config.train.pre_onpolicy_datapath is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:\n                self.add_traj(buffer_to_add=self.onpolicy_buffer, **traj_dict)\n            print_with_rank(\"finish onpolicy buffer initialization\")\n        if self.config.train.pre_onpolicy_datapath_train_test is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath_train_test,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:"
        },
        {
            "comment": "This code initializes the onpolicy train and test buffers, loads pre-existing data for the onpolicy test buffer if specified in the config, sets up a scheduler, and accelerator.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":144-169",
            "content": "                self.add_traj(\n                    buffer_to_add=self.onpolicy_train_test_buffer, **traj_dict\n                )\n            print_with_rank(\"finish onpolicy train test buffer initialization\")\n        if self.config.train.pre_onpolicy_datapath_test is not None:\n            traj_dict_list = build_env_offline_data_component_fn(\n                jsonl_path=self.config.train.pre_onpolicy_datapath_test,\n                q2idx_dict=self.q2idx_dict_test,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n            )\n            for traj_dict in traj_dict_list:\n                self.add_traj(buffer_to_add=self.onpolicy_test_buffer, **traj_dict)\n            print_with_rank(\"finish onpolicy test buffer initialization\")\n        self.scheduler = self.setup_scheduler()\n        self.scheduler = self.accelerator.prepare(self.scheduler)\n        self.setup_tracker()\n        self.ct2_generator = None\n    def setup_scheduler(self):\n        \"\"\"\n        Returns a learning rate scheduler derived from an instance's TRLConfig"
        },
        {
            "comment": "The code initializes a scheduler for training. It calculates the total number of training steps based on epochs, buffer length, micro-batch size and gradient accumulation steps. The scheduler class is then obtained from the given config. If the scheduler name contains \"warmup\", the num_training_steps is set to the total training steps. Finally, the scheduler object is created with the optimizer and scheduler kwargs.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":170-195",
            "content": "        \"\"\"\n        train_config: TrainConfig = self.config.train\n        assert train_config.train_epoch == 1\n        len_sft = torch.tensor(\n            len(self.onpolicy_buffer), device=self.accelerator.device\n        )\n        avg_sft = self.accelerator.gather(len_sft).float()\n        buffer_max_length = avg_sft.max().item()\n        total_training_steps = train_config.epochs * int(\n            buffer_max_length\n            / train_config.micro_batch_size\n            / train_config.gradient_accumulation_steps\n            + 1\n        )\n        print_rank_0(\"Total Training Step: {}.\".format(total_training_steps))\n        scheduler_class = get_scheduler_class(self.config.scheduler.name)\n        if \"warmup\" in self.config.scheduler.name:\n            self.config.scheduler.kwargs[\"num_training_steps\"] = int(\n                total_training_steps\n            )\n        print_rank_0(self.config.scheduler.kwargs)\n        scheduler = scheduler_class(self.opt, **self.config.scheduler.kwargs)\n        return scheduler\n    def prepare_training(self):"
        },
        {
            "comment": "This code defines a loss function for a model training. It calculates the value loss and reward loss based on the predicted values from the model and the actual returns. The losses are then combined to compute the final loss. This function also collects garbage, empties CUDA cache, and initializes ct2_generator.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":196-224",
            "content": "        del self.ct2_generator\n        gc.collect()\n        torch.cuda.empty_cache()\n        self.ct2_generator = None\n    def loss_fn(self, minibatch: Optional[TrajBatch] = None, setting=\"train\"):\n        stats = {}\n        minibatch.to(self.accelerator.device)\n        onpolicy_output = self.model(\n            input_ids=minibatch.input_ids,\n            #  labels=minibatch.label,\n            attention_mask=minibatch.attn_mask,\n        )\n        value = onpolicy_output.value\n        mask = minibatch.mask\n        returns = minibatch.returns\n        # value loss includes value loss + final step reward loss\n        value_delta = (returns[mask != 0] - value[mask != 0]) ** 2\n        value_loss = value_delta.mean()\n        stats[f\"{setting}/value_loss\"] = value_loss.detach().item()\n        # final step reward statistics\n        reward_loss = (returns[mask == 2] - value[mask == 2]) ** 2\n        reward_loss = reward_loss.mean()\n        stats[f\"{setting}/reward_loss\"] = reward_loss.detach().item()\n        loss = self.config.train.value_loss_coef * value_loss"
        },
        {
            "comment": "This code snippet contains a function that measures the total loss during training and adds a new trajectory instance to a buffer. The policy_forward_seq_value function is used for forward pass in policy network, and the TrajInstance class is instantiated from query_str, answer, value_index, reward_list with tokenizer and policy_forward_seq_value as inputs. The buffer_to_add's add method is called to include this trajectory instance into the buffer.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":226-258",
            "content": "        stats[f\"{setting}/total_loss\"] = loss.detach().item()\n        return loss, stats\n    def add_traj(\n        self,\n        idx: int,\n        query_str: str,\n        answer: str,\n        value_index: np.array,\n        reward_list: np.array,\n        buffer_to_add: MultiTrajBuffer,\n    ):\n        # result 1 for right, -1 for wrong\n        @torch.inference_mode()\n        def policy_forward_seq_value(input_str):\n            input_ids = self.tokenizer(input_str, return_tensors=\"pt\").input_ids.to(\n                self.accelerator.device\n            )\n            value = self.unwrap_critic_model(input_ids=input_ids).value\n            return value.cpu().float().numpy()\n        buffer_to_add.add(\n            idx,\n            TrajInstance.from_string(\n                query_str,\n                answer,\n                value_index,\n                reward_list,\n                self.tokenizer,\n                policy_forward_seq_value,\n                self.config.train.gamma,\n                self.config.train.gae_lambda,\n                use_gae=False,"
        },
        {
            "comment": "The code defines a class with a method called \"learn\" which performs training for a certain number of epochs. It creates data loaders, iterators and initializes variables such as the gradient accumulation steps and the length of the on-policy buffer. The purpose is to train a model using the provided configuration.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":259-287",
            "content": "                cal_value=True,\n            ),\n        )\n    def learn(self, iter_count=0):\n        train_config: TrainConfig = self.config.train\n        self.iter_count = iter_count\n        self.train_step = 0\n        for i_epoch in range(train_config.epochs):\n            stats = {}\n            print_with_rank(\"TRAINING\")\n            self.model.train()\n            # if train_config.pure_sft:\n            #    assert train_config.sft_loss_coef is not None\n            train_dataloader = self.onpolicy_buffer.create_loader(\n                train_config.micro_batch_size, shuffle=True\n            )\n            # train_sft_dataloader = self.sft_buffer.create_loader(\n            #    train_config.sft_micro_batch_size, shuffle=True)\n            train_data_iter = loop_iter(train_dataloader)\n            # train_sft_data_iter = loop_iter(train_sft_dataloader)\n            gas = self.config.train.gradient_accumulation_steps\n            len_onpolicy = torch.tensor(\n                len(train_dataloader), device=self.accelerator.device"
        },
        {
            "comment": "Computes the average on-policy length and calculates the total on-policy number for all soft updates. Then, it asserts that the train epoch is equal to 1. Next, it calculates the number of gradient accumulation steps (nga) based on the maximum on-policy length divided by gas, multiplied by the train epoch. Finally, it loops through nga and gas iterations, computing loss and step statistics for each iteration and updating optimizer accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":288-315",
            "content": "            )\n            avg_onpolicy = self.accelerator.gather(len_onpolicy).float()\n            # all sft buffer size\n            all_onpolicy_num = torch.sum(\n                avg_onpolicy.sum() * train_config.micro_batch_size\n            ).item()\n            assert self.config.train.train_epoch == 1\n            nga = int(\n                int(math.ceil(avg_onpolicy.max().item() / gas))\n                * self.config.train.train_epoch\n            )\n            train_stats_list = []\n            t0 = time.time()\n            for i_nga in tqdm(range(nga), disable=not self.local_rank == 0):\n                for i_gas in range(gas):\n                    loss, cur_step_stats = self.train_iter(train_data_iter)\n                    train_stats_list.append(cur_step_stats)\n                self.opt.step()\n                self.scheduler.step()\n                self.opt.zero_grad()\n                stats_gas = dm_tree.map_structure(\n                    lambda *xs: np.mean(xs).item(), *train_stats_list[-gas:]\n                )"
        },
        {
            "comment": "This code snippet calculates training loss and updates statistics for each step of the training process. It logs these statistics, calculates the training time elapsed, and saves intermediate checkpoints at specified intervals. The loss is printed if the local_rank is 0. Additionally, it checks if the optimizer should be saved when saving a checkpoint.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":317-341",
            "content": "                stats_gas[\"train/learning_rate\"] = self.scheduler.get_last_lr()[0]\n                self.accelerator.log(stats_gas, step=self.train_step)\n                self.train_step += 1\n            t1 = time.time()\n            train_stats = dm_tree.map_structure(\n                lambda *xs: np.mean(xs).item(), *train_stats_list\n            )\n            stats.update(train_stats)\n            stats[\"time/training_time\"] = t1 - t0\n            stats[\"train/sft_buffer_size\"] = all_onpolicy_num\n            if self.local_rank == 0:\n                print_rank_0(\"LOSS: {:.4f}, {}\".format(loss, stats))\n            if self.iter_count % train_config.checkpoint_interval == 0:\n                subfolder = f\"checkpoint_{self.iter_count}_ep{i_epoch}\"\n                directory = os.path.join(train_config.checkpoint_dir, subfolder)\n                print_with_rank(f\"Saving intermediate checkpoint into {directory}\")\n                if train_config.save_optimizer:\n                    self.save(directory)\n                else:"
        },
        {
            "comment": "The code saves the model and config, evaluates the model after specified intervals, updates statistics by replacing \"loss\" with \"average_loss\", logs the updated stats, and performs forward and backward passes for calculating loss.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":342-368",
            "content": "                    self.save_pretrained(directory)\n                self.save_pretrained(\n                    os.path.join(train_config.checkpoint_dir, \"last_model_hf\")\n                )\n                self.save_config()\n            if self.iter_count % train_config.eval_interval == 0:\n                print_with_rank(\"EVALUATING iteration:{}\".format(self.iter_count))\n                eval_stats = self.evaluate()\n                stats.update(eval_stats)\n            # replace all key\n            for key in list(stats.keys()):\n                if \"loss\" in key:\n                    stats[key.replace(\"loss\", \"average_loss\")] = stats.pop(key)\n            self.accelerator.log(stats, step=self.iter_count)\n            self.iter_count += 1\n    def train_iter(self, data_iter):\n        forward_time = -time.time()\n        # onpolicy data loss\n        minibatch = next(data_iter)\n        loss, stats = self.loss_fn(minibatch)\n        forward_time += time.time()\n        backward_time = -time.time()\n        self.accelerator.backward(loss)"
        },
        {
            "comment": "Function `evaluate` is evaluating the model by iterating over train and test dataloaders, and accumulating evaluation statistics. The statistics are stored in a list for both train and test settings. If there were no data in any of the settings, the function will return a default statistic of 0.0 for value_loss and reward_loss. The function is wrapped in `@torch.inference_mode()` which optimizes the code execution for inference mode.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":369-396",
            "content": "        backward_time += time.time()\n        stats[\"time/forward_time\"] = forward_time\n        stats[\"time/backward_time\"] = backward_time\n        return loss.item(), stats\n    @torch.inference_mode()\n    def evaluate(self):\n        self.model.eval()\n        train_test_dataloader = self.onpolicy_train_test_buffer.create_loader(\n            self.config.train.micro_batch_size, shuffle=False\n        )\n        test_dataloader = self.onpolicy_test_buffer.create_loader(\n            self.config.train.micro_batch_size, shuffle=False\n        )\n        def evaluate_on_dataloader(dataloader, setting):\n            print_with_rank(setting)\n            stats_list = []\n            for minibatch in tqdm(dataloader, disable=not self.local_rank == 0):\n                _, stats = self.loss_fn(minibatch, setting=setting)\n                stats_list.append(stats)\n            if len(stats_list) == 0:\n                stats_list.append(\n                    {\n                        f\"{setting}/value_loss\": 0.0,\n                        f\"{setting}/reward_loss\": 0.0,"
        },
        {
            "comment": "This code calculates and evaluates training and testing statistics using dataloaders, maps the results to means, and then gathers them on the device. Finally, it prints the average evaluation statistics for both train and test sets.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":397-425",
            "content": "                        f\"{setting}/total_loss\": 0.0,\n                    }\n                )\n            return stats_list\n        train_test_stats = evaluate_on_dataloader(\n            train_test_dataloader, setting=\"train_test\"\n        )\n        train_test_stats = dm_tree.map_structure(\n            lambda *xs: np.mean(xs), *train_test_stats\n        )\n        test_stats = evaluate_on_dataloader(test_dataloader, setting=\"test\")\n        test_stats = dm_tree.map_structure(lambda *xs: np.mean(xs), *test_stats)\n        device = self.accelerator.device\n        eval_stats = dict()\n        for k, v in train_test_stats.items():\n            tmp_tensor = torch.tensor(v, device=device)\n            gather_tensor = self.accelerator.gather(tmp_tensor)\n            eval_stats[k] = gather_tensor.mean().item()\n        for k, v in test_stats.items():\n            tmp_tensor = torch.tensor(v, device=device)\n            gather_tensor = self.accelerator.gather(tmp_tensor)\n            eval_stats[k] = gather_tensor.mean().item()\n        print_with_rank(eval_stats)"
        },
        {
            "comment": "This code snippet returns the evaluation statistics from a function or method.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_value.py\":426-426",
            "content": "        return eval_stats"
        }
    ]
}