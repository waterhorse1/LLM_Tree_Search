{
    "summary": "The code utilizes the ChatGLM model for text generation, and handles cases where a specific substring is not found in the original text by tokenizing EOS tokens, checking for duplicates, and updating log probabilities.",
    "details": [
        {
            "comment": "This function uses a generator to generate text based on a prompt. It takes in a tokenizer, static_prompt, and prompt as input. The prompt is encoded into tokens, and if a static_prompt exists, it's also encoded. The stop token is checked for being an integer. The generator generates batches of text using the provided parameters like temperature, top_p, top_k, etc. The max_length determines the maximum length of generated text, and return_scores returns scores alongside results if set to True. Finally, include_prompt_in_result specifies whether or not the prompt should be included in the result.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/llm/text_generation.py\":0-29",
            "content": "import requests\nimport torch\ndef llm_gen_ct2(\n    generator, tokenizer, static_prompt, prompt, num_sequence, stop, **generation_config\n):\n    prompt_tokens = tokenizer.convert_ids_to_tokens(\n        tokenizer.encode(prompt, add_special_tokens=False)\n    )\n    if static_prompt is not None:\n        static_prompt_tokens = tokenizer.convert_ids_to_tokens(\n            tokenizer.encode(static_prompt)\n        )\n    else:\n        static_prompt_tokens = None\n        prompt_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))\n    if isinstance(stop, int):\n        stop = [stop]\n    step_results = generator.generate_batch(\n        [prompt_tokens],\n        sampling_temperature=generation_config.get(\"temperature\", 1.0),\n        sampling_topp=generation_config.get(\"top_p\", 1.0),\n        sampling_topk=generation_config.get(\"top_k\", 1),\n        max_length=generation_config.get(\"max_new_tokens\", 16),\n        return_scores=True,\n        include_prompt_in_result=False,\n        end_token=stop,\n        static_prompt=static_prompt_tokens,"
        },
        {
            "comment": "The code defines a function 'llm_gen_ct2' for generating text using an LLM model, considering both static and dynamic prompts. It takes a generator, tokenizer, static prompt, dynamic prompt, number of sequences to generate, and stop conditions as inputs. The 'forward_batch' method of the generator is used to predict logits from the input tokens. The code also includes a function 'llm_forward_ct2' for getting logits from a single input token sequence using the LLM model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/llm/text_generation.py\":30-62",
            "content": "        max_batch_size=generation_config.get(\"max_batch_size\", 0),\n        num_hypotheses=num_sequence,\n    )\n    results = list(step_results)\n    texts = [tokenizer.decode(seq) for seq in results[0].sequences_ids]\n    logps = results[0].scores\n    return texts, logps\ndef llm_forward_ct2(generator, tokenizer, prompt):\n    prompt_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))\n    step_logits = generator.forward_batch(\n        [prompt_tokens],\n    )\n    # Currently this only supports float32\n    logits = torch.as_tensor(step_logits)[:, -1]\n    return logits\n# def llm_gen_ct2(generator, sp, static_prompt, prompt, num_sequence, stop,\n#                 **generation_config):\n#     prompt_tokens = sp.encode(prompt, out_type=str)\n#     if static_prompt is not None:\n#         static_prompt_tokens = [\"<s>\"] + sp.encode(static_prompt, out_type=str)\n#     else:\n#         static_prompt_tokens = None\n#         prompt_tokens = [\"<s>\"] + prompt_tokens\n#     if isinstance(stop, int):\n#         stop = [stop]"
        },
        {
            "comment": "Code generates text responses using LLM with logging of probability scores for each generated response. The function takes a model, tokenizer, static prompt, and input prompt. It also considers generation configuration like temperature, top_p, and max_new_tokens. It generates batch of sequences, decodes them into texts, and logs their associated probabilities before returning the texts and logps.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/llm/text_generation.py\":63-91",
            "content": "#     step_results = generator.generate_batch(\n#         [prompt_tokens] * num_sequence,\n#         sampling_temperature=generation_config.get(\"temperature\", 1.0),\n#         sampling_topp=generation_config.get(\"top_p\", 1.0),\n#         sampling_topk=generation_config.get(\"top_k\", 1),\n#         max_length=generation_config.get(\"max_new_tokens\", 16),\n#         return_scores=True,\n#         include_prompt_in_result=False,\n#         end_token=stop,\n#         static_prompt=static_prompt_tokens,\n#         max_batch_size=generation_config.get(\"max_batch_size\", 0),\n#     )\n#     results = list(step_results)\n#     texts = [sp.decode(r.sequences_ids[0]) for r in results]\n#     logps = [[r.scores[0]] for r in results]\n#     return texts, logps\n@torch.no_grad()\ndef llm_gen_with_logp_v1(\n    model, tokenizer, static_prompt, prompt, num_sequence, stop, **generation_config\n):\n    state_all = [static_prompt + prompt for _ in range(num_sequence)]\n    inputs = tokenizer(\n        state_all,\n        truncation=True,\n        max_length=2048,"
        },
        {
            "comment": "This code is responsible for generating text using the ChatGLM model. It generates output sequences and computes transition scores to determine the most probable continuation of the input text. The split_token_id is used to find the index where the output token sequence should be split, and then the corresponding log probabilities are extracted from the transition scores.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/llm/text_generation.py\":92-122",
            "content": "        return_tensors=\"pt\",\n        return_token_type_ids=False,\n    ).to(model.device)\n    input_length = inputs.input_ids.shape[1]\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **generation_config)\n    transition_scores = model.compute_transition_scores(\n        outputs.sequences, outputs.scores, normalize_logits=True\n    )\n    transition_scores = transition_scores.cpu().float().numpy()\n    output_tokens = outputs.sequences[:, input_length:]\n    split_list = []\n    logprob_list = []\n    # for ChatGLM 13 means <0x0A> for \\n\n    split_token_id = 13\n    def find_index(tensor, element):\n        equals_value = torch.eq(tensor, element)\n        if sum(equals_value) == 0:\n            return -1\n        return torch.nonzero(equals_value)[0].item()\n    for scores, ot in zip(transition_scores, output_tokens):\n        assert len(ot) == len(scores)\n        pos = find_index(ot, split_token_id)\n        if pos != 0 and pos != -1:  # 0 means the first token is split str\n            ot = ot[:pos]\n            scores = scores[:pos]"
        },
        {
            "comment": "This code handles the case when a specific substring is not found in the original text. It uses tokenizer to find the end of sentence (EOS) token's position and creates a new list with only the EOS part. Then, it checks if the generated output string (os) already exists in the split_list, and appends it if not. Finally, it adds scores to logprob_list and continues to next iteration.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/llm/text_generation.py\":123-142",
            "content": "        elif pos == -1:  # -1 means doesn't contain split str,\n            #  just use the whole string as the action, for chatglm\n            eos_token_id = tokenizer.eos_token_id\n            eos_pos = find_index(ot, eos_token_id)\n            ot = ot[:eos_pos]\n            scores = scores[:eos_pos]\n        else:\n            continue\n        os = tokenizer.decode(ot)\n        if os in split_list:\n            continue\n        else:\n            if len(scores) == 0:\n                print(123, state_all[-1], os)\n                exit()\n            split_list.append(os)\n            # todo determine np.sum or np.mean\n            logprob_list.append(scores)\n    return split_list, logprob_list"
        }
    ]
}