{
    "summary": "The code defines a class `OptimizerName` with function `get_optimizer_class()`, supporting Adam, AdamW, and SGD, and a learning rate scheduler function that returns a scheduler based on the provided scheduler name, including cosine annealing, linear, and warmup-cosine utility functions.",
    "details": [
        {
            "comment": "This code defines a class `OptimizerName` which represents the supported optimizer names. The function `get_optimizer_class()` returns an optimizer class based on the provided name. It currently supports Adam, AdamW, and SGD optimizers. A ValueError is raised if an unsupported optimizer name is given. The code also contains a partial implementation of _get_cosine_schedule_with_warmup_lr_lambda().",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/opt_utils.py\":0-39",
            "content": "from enum import Enum\nfrom functools import partial\nimport math\nfrom typing import Optional\nimport torch\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, LambdaLR\nclass OptimizerName(str, Enum):\n    \"\"\"Supported optimizer names\"\"\"\n    ADAM: str = \"adam\"\n    ADAMW: str = \"adamw\"\n    # ADAM_8BIT_BNB: str = \"adam_8bit_bnb\"\n    # ADAMW_8BIT_BNB: str = \"adamw_8bit_bnb\"\n    SGD: str = \"sgd\"\ndef get_optimizer_class(name: OptimizerName):\n    \"\"\"\n    Returns the optimizer class with the given name\n    Args:\n        name (str): Name of the optimizer as found in `OptimizerNames`\n    \"\"\"\n    if name == OptimizerName.ADAM:\n        return torch.optim.Adam\n    if name == OptimizerName.ADAMW:\n        return torch.optim.AdamW\n    if name == OptimizerName.SGD.value:\n        return torch.optim.SGD\n    supported_optimizers = [o.value for o in OptimizerName]\n    raise ValueError(\n        f\"`{name}` is not a supported optimizer. \"\n        f\"Supported optimizers are: {supported_optimizers}\"\n    )\ndef _get_cosine_schedule_with_warmup_lr_lambda("
        },
        {
            "comment": "This code defines a function `get_cosine_schedule_with_warmup` that creates a learning rate schedule for an optimizer, using a cosine annealing with warm-up. The schedule takes into account the number of training steps, optional warm-up steps or ratio, and the number of cycles to smoothly increase and decrease the learning rate. The function asserts the validity of inputs before calculating the learning rate based on current step, warm-up duration, and cycle progress.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/opt_utils.py\":40-73",
            "content": "    current_step: int,\n    *,\n    num_warmup_steps: int,\n    num_training_steps: int,\n    num_cycles: float,\n):\n    if current_step < num_warmup_steps:\n        return float(current_step) / float(max(1, num_warmup_steps))\n    progress = float(current_step - num_warmup_steps) / float(\n        max(1, num_training_steps - num_warmup_steps)\n    )\n    return max(\n        0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n    )\ndef get_cosine_schedule_with_warmup(\n    optimizer,\n    num_training_steps: int,\n    num_warmup_steps: Optional[int] = None,\n    warmup_ratio: Optional[float] = None,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n):\n    assert (num_warmup_steps is not None) ^ (warmup_ratio is not None), (\n        warmup_ratio,\n        num_warmup_steps,\n    )\n    if warmup_ratio is not None:\n        assert (\n            warmup_ratio > 0 and warmup_ratio < 1.0\n        ), \"Invalid warmup ratio: {}\".format(warmup_ratio)\n        num_warmup_steps = int(num_training_steps * warmup_ratio)\n    assert num_warmup_steps >= 0"
        },
        {
            "comment": "This code defines a function that returns a learning rate scheduler based on the provided scheduler name. It also includes utility functions for different types of learning rate schedules like cosine annealing, linear, and warmup-cosine. The `get_scheduler_class` function identifies the requested scheduler and returns its corresponding class.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/opt_utils.py\":74-106",
            "content": "    assert num_training_steps >= 0\n    lr_lambda = partial(\n        _get_cosine_schedule_with_warmup_lr_lambda,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps,\n        num_cycles=num_cycles,\n    )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\nclass SchedulerName(str, Enum):\n    \"\"\"Supported scheduler names\"\"\"\n    COSINE_ANNEALING = \"cosine_annealing\"\n    LINEAR = \"linear\"\n    COSINE_WARMUP = \"cosine_warmup\"\ndef get_scheduler_class(name: SchedulerName):\n    \"\"\"\n    Returns the scheduler class with the given name\n    \"\"\"\n    if name == SchedulerName.COSINE_ANNEALING:\n        return CosineAnnealingLR\n    elif name == SchedulerName.LINEAR:\n        return LinearLR\n    elif name == SchedulerName.COSINE_WARMUP:\n        return get_cosine_schedule_with_warmup\n    supported_schedulers = [s.value for s in SchedulerName]\n    raise ValueError(\n        f\"`{name}` is not a supported scheduler. \"\n        f\"Supported schedulers are: {supported_schedulers}\"\n    )"
        }
    ]
}