{
    "summary": "The code modifies the Llama model's attention mechanism, applying transforms to inputs and updating the `_prepare_decoder_attention_mask` function. It also restricts GPU usage to A100 or H100.",
    "details": [
        {
            "comment": "The code is defining a function `forward` that performs attention operation on input hidden states. It first projects the hidden states into query, key and value states. Then it reshapes them according to the number of heads and head dimension. Finally, it transposes the query states before performing the attention operation.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/llama_flash_attn_monkey_patch.py\":0-36",
            "content": "from typing import List, Optional, Tuple\nimport logging\nimport torch\nfrom torch import nn\nimport transformers\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\nfrom einops import rearrange\nfrom flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\nfrom flash_attn.bert_padding import unpad_input, pad_input\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)"
        },
        {
            "comment": "This code performs a Flash attention operation, which involves transforming the data into the format required by Flash attention. It creates query, key, and value states and applies rotary position embedding to both. The resulting query_states and key_states are then transformed into the required format for Flash attention. The use of output_attentions and cache is not supported in this code.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/llama_flash_attn_monkey_patch.py\":37-67",
            "content": "        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n    assert not output_attentions, (\n        \"output_attentions is not supported\",\n        output_attentions,\n    )\n    assert not use_cache, (\"use_cache is not supported\", use_cache)\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n    # transform the data into the format required by flash attention\n    qkv = torch.stack(\n        [query_states, key_states, value_states], dim=2"
        },
        {
            "comment": "This code is responsible for handling attention masks in the Llama model. It transposes the input and then checks if there is an attention mask. If there isn't, it applies a function to unpadded QKV packed inputs with some parameters. If there is one, it rearranges the input, unpads it, and passes it through a function specifically designed for unpadded QKV packed inputs. The output is then rearranged back into its original shape.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/llama_flash_attn_monkey_patch.py\":68-91",
            "content": "    )  # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, \"b s ... -> (b s) ...\")\n        max_s = q_len\n        cu_q_lens = torch.arange(\n            0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32, device=qkv.device\n        )\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n        )\n        output = rearrange(output, \"(b s) ... -> b s ...\", b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(\n            x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=nheads\n        )\n        output_unpad = flash_attn_unpadded_qkvpacked_func("
        },
        {
            "comment": "This code snippet is modifying the LlamaModel to use Flash attention instead of the original attention mechanism. It introduces a function `replace_llama_attn_with_flash_attn()` which checks the CUDA version and ensures that only A100 or H100 GPUs are used for training due to limitations with Flash attention's head dimension. Additionally, it modifies `_prepare_decoder_attention_mask()` function to disable the transformation of the attention mask in LlamaModel since Flash attention requires the same mask as key\\_padding\\_mask.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/llama_flash_attn_monkey_patch.py\":92-118",
            "content": "            x_unpad, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n        )\n        output = rearrange(\n            pad_input(\n                rearrange(output_unpad, \"nnz h d -> nnz (h d)\"), indices, bsz, q_len\n            ),\n            \"b s (h d) -> b s h d\",\n            h=nheads,\n        )\n    return self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, None\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(\n    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n):\n    # [bsz, seq_len]\n    return attention_mask\ndef replace_llama_attn_with_flash_attn():\n    cuda_major, cuda_minor = torch.cuda.get_device_capability()\n    if cuda_major < 8:\n        logging.warning(\n            \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n            \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\""
        },
        {
            "comment": "In this code snippet, the original `_prepare_decoder_attention_mask` function in `LlamaModel` is being overwritten with a custom function called `_prepare_decoder_attention_mask`. The `forward` function of `LlamaAttention` class is also being replaced.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/llama_flash_attn_monkey_patch.py\":119-123",
            "content": "        )\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (\n        _prepare_decoder_attention_mask\n    )\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward"
        }
    ]
}