{
    "summary": "The code creates a function `get_train_test_dataset` and class `CSVDataset` for dataset generation, and iterates through questions and answers, categorizing them into \"train\" or \"test\", writing to JSONL files, and displaying stats.",
    "details": [
        {
            "comment": "This code defines a function `get_train_test_dataset` that returns train and test datasets. The function takes optional arguments for training and testing data paths, defaulting to the current file's parent directory. It creates instances of `JsonlQuestionDataset` using provided paths, which reads from JSON lines files containing question-answer pairs. It also defines a class `CSVDataset`, but its implementation is incomplete.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/prontoqa/data.py\":0-39",
            "content": "from torch.utils.data import Dataset\nimport pandas as pd\nimport jsonlines\nimport json\nimport numpy as np\nimport re\nfrom pathlib import Path\ndef get_train_test_dataset(*args, **kwargs):\n    env_dir = Path(__file__).parent\n    train_data_path = kwargs.get(\"train_data_path\", env_dir / \"train_data/train.jsonl\")\n    test_data_path = kwargs.get(\"test_data_path\", env_dir / \"train_data/test.jsonl\")\n    train_ds = JsonlQuestionDataset(train_data_path)\n    test_ds = JsonlQuestionDataset(test_data_path)\n    return train_ds, test_ds\nclass JsonlQuestionDataset(Dataset):\n    def __init__(self, data_path):\n        super().__init__()\n        self.data = []\n        with jsonlines.open(data_path, \"r\") as reader:\n            for obj in reader:\n                self.data.append(obj)\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, index):\n        return {\n            \"question\": self.data[index][\"question\"],\n            \"answer\": self.data[index][\"answer\"][0][\"text\"],\n        }\n# This is the dataset that\nclass CSVDataset(Dataset):"
        },
        {
            "comment": "This code defines a class that loads data from a CSV file, processes the questions and answers, and builds text steps. The `__init__` method reads the CSV file and stores the puzzles in a list. The `__len__` method returns the length of the stored puzzles. The `__getitem__` method retrieves an individual puzzle from the list. The `build_question` function creates a new question by appending a True/False statement to the original question. The `build_text` function combines a list of thought chains with the answer and returns it as text. The `data_preprocess` function reads data from a JSON file, processes each batch, builds questions and answers, and stores them in a list called `text_steps`.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/prontoqa/data.py\":40-74",
            "content": "    def __init__(self, data_path) -> None:\n        super().__init__()\n        self.data = list(pd.read_csv(data_path)[\"Puzzles\"])\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, index):\n        return {\"question\": self.data[index]}\ndef build_question(question, query):\n    statement = query[15:-1]\n    new_question = question + ' Is the statement \"{}\" true or false?'.format(statement)\n    return new_question\ndef build_text(cot, answer):\n    text = \"\\n\".join(cot) + \"\\nThe answer is {}.\".format(answer.lower())\n    return text\ndef data_preprocess(read_path, save_dir):\n    text_steps = []\n    with open(read_path, \"r\") as reader:\n        data_set = json.load(reader)\n        i = 0\n        for batch in data_set.values():\n            for key in batch.keys():\n                question = build_question(batch[key][\"question\"], batch[key][\"query\"])\n                text = build_text(batch[key][\"chain_of_thought\"], batch[key][\"answer\"])\n                text_steps.append(len(text.split(\"\\n\")))\n                new_obj = {"
        },
        {
            "comment": "The code iterates through a list of questions and their associated answers. It categorizes each example as either \"train\" or \"test\". The new object containing the question, answer, and category is then written to separate JSONL files (\"train.jsonl\", \"test.jsonl\", and \"all.jsonl\"). Finally, it prints statistics about the number of examples in the list.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/prontoqa/data.py\":75-95",
            "content": "                    \"question\": question,\n                    \"i\": i,\n                    \"answer\": [{\"text\": text, \"correct\": True}],\n                }\n                if \"test_example\" == key:\n                    with jsonlines.open(save_dir + \"test.jsonl\", \"a\") as writer:\n                        writer.write(new_obj)\n                else:\n                    with jsonlines.open(save_dir + \"train.jsonl\", \"a\") as writer:\n                        writer.write(new_obj)\n                with jsonlines.open(save_dir + \"all.jsonl\", \"a\") as writer:\n                    writer.write(new_obj)\n                i += 1\n    print(\"count: \", len(text_steps))\n    print(\"mean step num: \", np.mean(text_steps))\n    print(\"std step num: \", np.std(text_steps))\n    print(\"max step num: \", np.max(text_steps))\n    print(\"min step num: \", np.min(text_steps))"
        }
    ]
}