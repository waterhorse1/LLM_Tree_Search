{
    "summary": "The code defines dataclasses for TimeStep and neural network data, utilizing scipy's lfilter for returns calculation. It includes tokenizing input strings and policy-based reinforcement learning for values and returns. The class also handles tensors and calculates rewards based on value index.",
    "details": [
        {
            "comment": "This code defines a dataclass called \"TimeStep\" that contains various tensors such as query_tensor, response_tensor, reward, value, returns, legal_actions_tensor, action_probs, terminated and truncated. The class also has a classmethod named from_string which takes an AutoTokenizer instance and likely converts a string into a TimeStep object using the given tokenizer.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":0-32",
            "content": "from dataclasses import dataclass\nimport numpy as np\nimport torch\nfrom torchtyping import TensorType\nfrom typing import Optional, Sequence, List\nfrom transformers import AutoTokenizer\nfrom tsllm.distributed.utils import print_with_rank, print_rank_0\ndef _tokenize_fn(s, tokenizer, drop_bos: bool = False):\n    input_ids = tokenizer(s, return_tensors=\"pt\", padding=True).input_ids\n    if drop_bos and torch.all(input_ids[:, 0] == tokenizer.bos_token_id):\n        input_ids = input_ids[:, 1:]\n    return input_ids\n@dataclass\nclass TimeStep:\n    query_tensor: TensorType[\"query_length\"]\n    response_tensor: TensorType[\"response_length\"]\n    reward: TensorType\n    value: TensorType\n    returns: TensorType\n    legal_actions_tensor: TensorType[\"num_action\", \"max_action_length\"]\n    # legal_actions_attn_mask: TensorType[\"num_action\", \"max_action_length\"]\n    action_probs: TensorType[\"num_action\"]\n    termiated: TensorType\n    truncated: TensorType\n    @classmethod\n    def from_string(\n        cls,\n        tokenizer: AutoTokenizer,"
        },
        {
            "comment": "This function tokenizes input strings, applies padding, and creates tensors for query, response, and legal actions. It asserts the tokenizer's padding side and checks if the pad_token is not None. It then uses the _tokenize_fn to create query, total, response, and legal_action_tensors by passing the necessary strings and tokenizer.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":33-60",
            "content": "        query_str: str,\n        response_str: str,\n        reward: float,\n        value: float,\n        legal_actions: List[str],\n        action_probs: TensorType[\"num_action\"],\n        terminated: bool,\n        truncated: bool,\n    ):\n        assert (\n            tokenizer.padding_side == \"right\"\n        ), \"the tokenizer's padding side should be right.\"\n        assert tokenizer.pad_token != None, \"Your tokenizer's pad_token is None\"\n        # here only squeeze the first batch dimension\n        query_tensor = _tokenize_fn(query_str, tokenizer, False).squeeze_(0)\n        total_tensor = _tokenize_fn(\n            query_str + response_str, tokenizer, False\n        ).squeeze_(0)\n        response_tensor = total_tensor[len(query_tensor) :]\n        total_legal_action_qa_tensor = _tokenize_fn(\n            [query_str + action_str for action_str in legal_actions], tokenizer, False\n        )\n        legal_action_tensor = total_legal_action_qa_tensor[:, len(query_tensor) :]\n        # yapf: disable\n        return cls(query_tensor,"
        },
        {
            "comment": "This code defines a function \"discount_cumsum\" that computes the cumulative discounted sum of a list of numbers using scipy.signal's lfilter function. It also defines another function \"_compute_return_fn\" which takes in a list of rewards, values, gamma (discount factor), gae_lambda, and last_value as parameters. This function calculates the Generalized Advantage Estimation (GAE) by iterating over the reversed range of the length of rewards and updating the delta value for each time step.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":61-93",
            "content": "                   response_tensor,\n                   torch.tensor(reward),\n                   torch.tensor(value),\n                   torch.tensor(0.),\n                   legal_action_tensor,\n                   torch.tensor(action_probs),\n                   torch.tensor(terminated),\n                   torch.tensor(truncated))\n        # yapf: enable\nimport scipy.signal\ndef discount_cumsum(x, discount):\n    if isinstance(x, torch.Tensor):\n        x = x.numpy()\n        return torch.tensor(\n            scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[\n                ::-1\n            ].copy()\n        )\n    else:\n        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\ndef _compute_return_fn(rews, vals, gamma, gae_lambda, last_value):\n    last_gae_lam = 0\n    reversed_adv = []\n    for t in reversed(range(len(rews))):\n        next_v = vals[t + 1] if t < len(rews) - 1 else last_value\n        delta = rews[t] + gamma * next_v - vals[t]\n        last_gae_lam = delta + gamma * gae_lambda * last_gae_lam"
        },
        {
            "comment": "This code computes returns for a trajectory by taking rewards and values from the timesteps, applying a function to compute advantages based on gamma and lambda, and then adds these advantages to the values. This is done using a reversed approach where lastgaelam (last generalized advantage estimation lambda) is updated in reverse order. Finally, the advantages are stacked in reverse order for each timestep.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":94-119",
            "content": "        reversed_adv.append(last_gae_lam)\n    adv = torch.tensor(reversed_adv[::-1])\n    ret = adv + vals\n    return ret\n@dataclass\nclass Trajectory:\n    timesteps: Sequence[TimeStep]\n    def compute_returns(\n        self, gamma: float = 0.99, gae_lambda: float = 0.95, last_value: float = 0\n    ):\n        rews = torch.tensor([ts.reward for ts in self.timesteps])\n        vals = torch.tensor([ts.value for ts in self.timesteps])\n        ret = _compute_return_fn(rews, vals, gamma, gae_lambda, last_value)\n        ## ========= trlx PPO's implementation ===========\n        # lastgaelam = 0\n        # advantages_reversed = []\n        # for t in reversed(range(response_length)):\n        #     nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0\n        #     delta = rewards[:, t] + self.gamma * nextvalues - values[:, t]\n        #     lastgaelam = delta + self.gamma * self.lam * lastgaelam\n        #     advantages_reversed.append(lastgaelam)\n        # advantages = torch.stack(advantages_reversed[::-1], dim=1)"
        },
        {
            "comment": "This code snippet is calculating the returns for each timestep in a batch of data. It uses rewards and values from the timesteps to calculate the deltas, then computes the advantages using discounted cumulative sums. The returns are stored in the corresponding timestep object. The MCTSBatch class represents a batch of data with query tensor, response tensor, reward, value, returns, legal_actions_tensor (and potentially legal_actions_attn_mask).",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":120-146",
            "content": "        # returns = advantages + values\n        # if use_whitening:\n        #     advantages = whiten(advantages)\n        ## ========= OpenAI SpinningUp PPO's implementation ===========\n        # rews = [ts.reward for ts in self.timesteps] + [last_value]\n        # vals = [ts.value for ts in self.timesteps] + [last_value]\n        # rews = np.array(rews)\n        # vals = np.array(vals)\n        # deltas = rews[:-1] + gamma * vals[1:] - vals[:-1]\n        # adv = discount_cumsum(deltas, gamma * gae_lambda)\n        # ret = discount_cumsum(rews, gamma)[:-1]\n        for i in range(len(self.timesteps)):\n            self.timesteps[i].returns = ret[i]\n@dataclass\nclass MCTSBatch:\n    query_tensor: TensorType[\"bsz\", \"query_length\"]\n    response_tensor: TensorType[\"bsz\", \"response_length\"]\n    reward: TensorType[\"bsz\"]\n    value: TensorType[\"bsz\"]\n    returns: TensorType[\"bsz\"]\n    legal_actions_tensor: TensorType[\"bsz\", \"num_action\", \"max_action_length\"]\n    # legal_actions_attn_mask: TensorType[\"bsz\", \"num_action\", \"max_action_length\"]"
        },
        {
            "comment": "This code is defining classes for handling data in a neural network model. It includes methods for converting strings to tensors and moving tensors to specific devices (e.g., GPU). The SftInstance class takes input IDs, labels, returns, and mask as input.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":147-176",
            "content": "    action_probs: TensorType[\"bsz\", \"num_action\"]\n    termiated: TensorType[\"bsz\"]\n    truncated: TensorType[\"bsz\"]\n    def to(self, *args, **kwargs):\n        self.query_tensor = self.query_tensor.to(*args, **kwargs)\n        self.response_tensor = self.response_tensor.to(*args, **kwargs)\n        self.reward = self.reward.to(*args, **kwargs)\n        self.value = self.value.to(*args, **kwargs)\n        self.returns = self.returns.to(*args, **kwargs)\n        self.legal_actions_tensor = self.legal_actions_tensor.to(*args, **kwargs)\n        self.action_probs = self.action_probs.to(*args, **kwargs)\n        self.termiated = self.termiated.to(*args, **kwargs)\n        self.truncated = self.truncated.to(*args, **kwargs)\n@dataclass\nclass SftInstance:\n    input_ids: TensorType[\"seq_len\"]\n    label: TensorType[\"seq_len\"]\n    returns: TensorType[\"seq_len\"]\n    mask: TensorType[\"seq_len\"]\n    @classmethod\n    def from_string(\n        cls,\n        q_str: str,\n        r_str: str,\n        tokenizer: AutoTokenizer,\n        policy_forward_seq_value: callable,"
        },
        {
            "comment": "This code tokenizes input strings, creates labels and input_ids for a language model, and initializes necessary variables for a policy-based reinforcement learning algorithm. It handles potential problems in answer steps of the instance being built.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":177-205",
            "content": "        gamma: float,\n        gae_lambda: float,\n        IGNORE_IDX=-100,\n    ):\n        q_ids = _tokenize_fn(q_str, tokenizer, False).squeeze(0)\n        r_ids = _tokenize_fn(r_str, tokenizer, True).squeeze(0)\n        total_ids = _tokenize_fn(q_str + r_str, tokenizer, False).squeeze(0)\n        assert len(total_ids) == len(q_ids) + len(r_ids)\n        label = total_ids.clone()\n        label[: len(q_ids)] = IGNORE_IDX\n        input_ids = total_ids\n        answer_steps = r_str.split(\"\\n\")\n        vs = []\n        mask = torch.zeros_like(input_ids)\n        current_str = q_str\n        value_seq = policy_forward_seq_value(q_str + r_str).squeeze(0)\n        for i, a in enumerate(answer_steps):\n            if len(a) == 0:\n                if i != len(answer_steps) - 1:\n                    print_with_rank(\n                        \"possible problems met in sft instance building. {}\".format(\n                            answer_steps\n                        )\n                    )\n                continue\n            current_str += a + \"\\n\""
        },
        {
            "comment": "The code defines a function that processes Sentence-Fragment Transformer (SFT) instances by tokenizing the input string, calculating values and returns for each instance, and creating a batch of data with input IDs, labels, and returns. The returns are calculated based on computed returns using a given gamma and gae_lambda parameters. The code also includes a class definition for SftBatch to store this processed batch data.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":206-232",
            "content": "            current_ids = _tokenize_fn(current_str, tokenizer, False).squeeze(0)\n            # current_value = policy_forward_value_fn(current_str).item()\n            current_value = value_seq[len(current_ids) - 1]\n            mask[len(current_ids) - 1] = 1\n            vs.append(current_value)\n        vs = torch.tensor(vs)\n        rews = torch.zeros_like(vs)\n        rews[-1] = 1.0  # sft instance is corrent.\n        rets = _compute_return_fn(\n            rews=rews, vals=vs, gamma=gamma, gae_lambda=gae_lambda, last_value=0\n        )  # sft instances always terminate.\n        returns = torch.zeros_like(input_ids)\n        nonzero_indices = mask.nonzero()\n        assert len(nonzero_indices) == len(vs), (\n            len(nonzero_indices),\n            len(vs),\n            nonzero_indices,\n            vs,\n        )\n        for i, idx in enumerate(nonzero_indices):\n            returns[idx.item()] = rets[i]\n        return cls(input_ids, label, returns, mask)\n@dataclass\nclass SftBatch:\n    input_ids: TensorType[\"bsz\", \"seq_len\"]"
        },
        {
            "comment": "The code defines a class that takes input, label, returns, and mask tensors of shape \"bsz\", \"seq_len\". It also has a question and response string attributes. The classmethod from_string creates an instance of the class using a question string, response string, value index array, reward list array, tokenizer, policy forward sequence value function, gamma, gae_lambda, cal_value, use_gae, and IGNORE_IDX parameters.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":233-268",
            "content": "    label: TensorType[\"bsz\", \"seq_len\"]\n    attn_mask: TensorType[\"bsz\", \"seq_len\"]\n    returns: TensorType[\"bsz\", \"seq_len\"]\n    mask: TensorType[\"bsz\", \"seq_len\"]  # value_mask\n    def to(self, *args, **kwargs):\n        self.input_ids = self.input_ids.to(*args, **kwargs)\n        self.label = self.label.to(*args, **kwargs)\n        self.attn_mask = self.attn_mask.to(*args, **kwargs)\n        self.returns = self.returns.to(*args, **kwargs)\n        self.mask = self.mask.to(*args, **kwargs)\n@dataclass\nclass TrajInstance:\n    input_ids: TensorType[\"seq_len\"]\n    label: TensorType[\"seq_len\"]\n    returns: TensorType[\"seq_len\"]\n    mask: TensorType[\"seq_len\"]\n    question: str\n    response: str\n    @classmethod\n    def from_string(\n        cls,\n        q_str: str,\n        r_str: str,\n        value_index: np.array,\n        reward_list: np.array,\n        tokenizer: AutoTokenizer,\n        policy_forward_seq_value: callable,\n        gamma: float,\n        gae_lambda: float,\n        cal_value: bool,\n        use_gae=True,\n        IGNORE_IDX=-100,"
        },
        {
            "comment": "This code segment tokenizes input questions and references, concatenates them, checks if new tokens are created, creates a label tensor with ignore values for the question part, assigns input_ids with total_ids, calculates rewards based on value index, asserts that the value index is correct, and initializes a list vs for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":269-295",
            "content": "    ):\n        q_ids = _tokenize_fn(q_str, tokenizer, False).squeeze(0)\n        r_ids = _tokenize_fn(r_str, tokenizer, True).squeeze(0)\n        total_ids = _tokenize_fn(q_str + r_str, tokenizer, False).squeeze(0)\n        # Check whether q_str + r_str creates some new tokens\n        # assert len(total_ids) == len(q_ids) + len(r_ids)\n        label = total_ids.clone()\n        label[: len(q_ids)] = IGNORE_IDX\n        input_ids = total_ids\n        if cal_value:\n            value_index = torch.tensor(value_index).int()\n            reward_list = torch.tensor(reward_list).float()\n            mask = torch.zeros_like(input_ids)\n            # Check value index dimension is correct\n            assert value_index[0] == len(q_ids) - 1\n            assert value_index[-1] == len(mask) - 1\n            # assert r_str.split(\"\\n\")[-1] == \"\"\n            # answer_steps = r_str.split(\"\\n\")[:-1]  # The last one is null or eos\n            vs = []\n            # current_str = q_str\n            # TODO: Organize code and check logic\n            if use_gae:"
        },
        {
            "comment": "This code calculates returns for a sequence of values and rewards. It applies a mask to select the value index and final reward, then computes returns using MC method. The function handles both terminating and non-terminating instances separately.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":296-322",
            "content": "                value_seq = policy_forward_seq_value(q_str + r_str).squeeze(0)\n                # mask and reward for the final answer\n                # Value index\n                mask[value_index[:-1]] = 1\n                # Final reward index\n                mask[value_index[-1]] = 2\n                vs = value_seq[value_index[:-1]]\n                rews = reward_list\n                rets = _compute_return_fn(\n                    rews=rews, vals=vs, gamma=gamma, gae_lambda=gae_lambda, last_value=0\n                )  # sft instances always terminate.\n                # add the final reward\n                rets = torch.cat([rets, reward_list[-1:]])\n                returns = torch.zeros(len(input_ids))\n                nonzero_indices = mask.nonzero()\n                assert len(nonzero_indices) == len(rets) == len(vs) + 1\n                for i, idx in enumerate(nonzero_indices):\n                    returns[idx.item()] = rets[i]\n                # result = torch.tensor([result])\n            else:\n                # conduct MC return calculation"
        },
        {
            "comment": "Creates a mask for value and final reward indices, calculates cumulative rewards, appends final reward, initializes returns tensor with zeros, assigns returns to nonzero indices in mask, and creates TrajBatch object with input_ids, label, returns, mask, q_str, r_str.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":323-351",
            "content": "                # Value index\n                mask[value_index[:-1]] = 1\n                # Final reward index\n                mask[value_index[-1]] = 2\n                assert value_index[-1] == len(mask) - 1\n                rets = discount_cumsum(reward_list, gamma)\n                # append the final reward\n                rets = torch.cat([rets, reward_list[-1:]])\n                returns = torch.zeros(len(input_ids))\n                nonzero_indices = mask.nonzero()\n                assert len(nonzero_indices) == len(rets)\n                for i, idx in enumerate(nonzero_indices):\n                    returns[idx.item()] = rets[i]\n        else:\n            # add padding tensor if not calculate value\n            # result = torch.tensor([result])\n            returns = torch.zeros(len(input_ids))\n            mask = torch.zeros_like(input_ids)\n        return cls(input_ids, label, returns, mask, q_str, r_str)\n@dataclass\nclass TrajBatch:\n    input_ids: TensorType[\"bsz\", \"seq_len\"]\n    label: TensorType[\"bsz\", \"seq_len\"]"
        },
        {
            "comment": "This code defines a class method \"to\" that transfers the input_ids, label, attn_mask, and returns tensors to specified device. The attn_mask and returns tensors are of type TensorType[\"bsz\", \"seq_len\"], and the mask tensor is an alias for value_mask.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/data/node_types_new.py\":352-361",
            "content": "    attn_mask: TensorType[\"bsz\", \"seq_len\"]\n    returns: TensorType[\"bsz\", \"seq_len\"]\n    mask: TensorType[\"bsz\", \"seq_len\"]  # value_mask\n    def to(self, *args, **kwargs):\n        self.input_ids = self.input_ids.to(*args, **kwargs)\n        self.label = self.label.to(*args, **kwargs)\n        self.attn_mask = self.attn_mask.to(*args, **kwargs)\n        self.returns = self.returns.to(*args, **kwargs)\n        self.mask = self.mask.to(*args, **kwargs)"
        }
    ]
}