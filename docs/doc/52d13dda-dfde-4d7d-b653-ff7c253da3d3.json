{
    "summary": "This code converts Llama models, generates responses, and compares them with ground truth using ThreadPoolExecutor for efficiency. It also sets command line arguments for environment name, GPU usage, batch size, output directory, tokenizer path, and test mode.",
    "details": [
        {
            "comment": "This code is importing necessary libraries and defining a function called `_cot_gen`. The function takes in a ct2 generator, tokenizer, query string builder, problem, number of outputs, stop sequence, max new tokens, and optional keyword arguments. It generates responses using the given parameters and a provided prompt based on the problem's question.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/generate_data.py\":0-33",
            "content": "from pathlib import Path\nfrom typing import Callable\nimport ctranslate2\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nimport sentencepiece as spm\nimport os\nfrom tqdm import tqdm\nimport argparse\nimport jsonlines\nfrom tsllm.distributed.utils import print_rank_0\nfrom tsllm.llm.text_generation import llm_gen_ct2\nfrom tsllm.llm.ct2_utils import OnlineHfConverter\nfrom tsllm.envs import get_env_datasets, get_default_query_str_builder, get_env_answer_checker\nfrom tsllm.argparse_utils import list_of_ints, str2bool\nfrom importlib import import_module\ndef _cot_gen(\n    ct2_generator,\n    tokenizer,\n    query_str_build_fn: Callable,\n    problem,\n    n=100,\n    stop=2,\n    max_new_tokens=256,\n    **kwargs,\n):\n    # prompt = \"Question: \" + problem[\"question\"] + \"\\nAnswer: Let's think step by step\\n\"\n    # if use_prefix:\n    #     prompt = prefix + \"\\n\" + prompt\n    prompt = query_str_build_fn(problem[\"question\"])"
        },
        {
            "comment": "The code converts a Hugging Face LLama model to CT2 format, generates texts using the generated CT2 model, and checks answers against ground truth. It initializes an AutoModelForCausalLM from pre-trained model, tokenizer, uses OnlineHfConverter to convert the model to CT2 format, and defines a check_answers function to compare generated responses with ground truth for each problem instance.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/generate_data.py\":34-69",
            "content": "    texts, logps = llm_gen_ct2(\n        ct2_generator,\n        tokenizer,\n        static_prompt=None,\n        prompt=prompt,\n        num_sequence=n,\n        stop=stop,\n        max_new_tokens=max_new_tokens,\n        **kwargs,\n    )\n    return texts\n\"\"\"convert hf model to ct2 model\"\"\"\n# base_llm_dir = \"huggyllama/llama-7b\"\n# cache_dir =\n# ct2_dir =\n# base_llm = AutoModelForCausalLM.from_pretrained(base_llm_dir,\n#                                                 cache_dir=cache_dir)\n# tokenizer = AutoTokenizer.from_pretrained(base_llm_dir, cache_dir=cache_dir)\n# cvt = OnlineHfConverter(model=base_llm,\n#                         model_name_or_path=base_llm_dir,\n#                         copy_files=[\"tokenizer.model\"])\n# cvt.convert(ct2_dir, force=True, quantization=\"bfloat16\")\ndef check_answers(check_fn, problem_inst, texts):\n    # groundtruth = extract_groundtruth(problem_inst[\"answer\"])\n    write_obj = {\n        \"question\": problem_inst[\"question\"],\n        \"groundtruth\": problem_inst[\"answer\"],\n    }\n    ans_list = []"
        },
        {
            "comment": "The code defines a function that takes a list of texts and checks each text against a given problem instance, determining whether it is correct or not. The function appends the correct texts to an answer list and returns this along with counts for correct answers and total number of texts. The main function loads data and generates an output directory if necessary. It then initializes a ctranslate2 generator and tokenizer, and prints out information about their location and device usage. A query string builder is also defined based on the environment name.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/generate_data.py\":71-106",
            "content": "    cnt = 0\n    for txt in texts:\n        correct = check_fn(problem_inst[\"question\"], problem_inst[\"answer\"], txt)\n        if correct:\n            cnt += 1\n        ans_list.append({\"text\": txt, \"correct\": correct})\n    write_obj[\"answer\"] = ans_list\n    return write_obj, cnt, len(texts)\ndef main(args):\n    train_ds, test_ds = get_env_datasets(args.env_name)\n    if args.test:\n        ds = test_ds\n    else:\n        ds = train_ds\n    args.output_path = Path(args.output_path)\n    if not args.output_path.parent.exists():\n        args.output_path.parent.mkdir(parents=True)\n    ct2_generator = ctranslate2.Generator(\n        args.ct2_dir, device=\"cuda\", device_index=args.gpu_ids, compute_type=\"bfloat16\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n    print(\n        \"LOADING CT2 MODEL AT: {}. DEVICES={}.\\nOUTPUT_DIR: {}\".format(\n            args.ct2_dir, args.gpu_ids, args.output_path\n        )\n    )\n    query_str_build_fn = partial(\n        get_default_query_str_builder(args.env_name), is_few_shot=args.is_few_shot"
        },
        {
            "comment": "This code generates data for an environment using LLAMa, a large language model. It utilizes a ThreadPoolExecutor to generate results from a dataset (ds) and checks answers using a given checker function. The results are then written to a file specified by args.output_path. The progress is tracked in a tqdm progress bar.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/generate_data.py\":107-134",
            "content": "    )\n    cot_gen = partial(\n        _cot_gen,\n        ct2_generator,\n        tokenizer,\n        query_str_build_fn,\n        n=args.k,\n        stop=tokenizer.eos_token_id,  # llama tokenizer: 2 is eos_token_id\n        max_new_tokens=256,\n        temperature=args.t,\n        top_p=1,\n        top_k=100,\n        max_batch_size=args.max_batch_size,\n    )\n    checker_fn = get_env_answer_checker(args.env_name)\n    correct_num, total_num = 0, 0\n    with ThreadPoolExecutor(args.num_workers) as pool:\n        results = pool.map(cot_gen, ds)\n        with jsonlines.open(args.output_path, \"w\") as writer:\n            for i, txts in enumerate(pbar := tqdm(results, total=len(ds))):\n                write_obj, cnt, len_list = check_answers(checker_fn, ds[i], txts)\n                writer.write(write_obj)\n                correct_num += cnt\n                total_num += len_list\n                pbar.set_description(\n                    \"{}-corrent: {:.3%}[{}/{}]\".format(\n                        i + 1, correct_num / total_num, correct_num, total_num"
        },
        {
            "comment": "This code is setting up command line arguments for the program. It requires an environment name, specifies a few default values (e.g., few-shot learning is disabled by default), defines several optional parameters such as number of GPUs to use and batch size, and then calls the main function with these arguments. The code also includes options for an output directory, tokenizer path, and whether or not the program should run in test mode.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/generate_data.py\":135-159",
            "content": "                    )\n                )\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    # env/task config\n    parser.add_argument(\"--is_few_shot\", type=str2bool, default=False)\n    parser.add_argument(\"--test\", type=str2bool, default=False)\n    parser.add_argument(\"--env_name\", type=str, required=True)\n    parser.add_argument(\"-k\", type=int, default=1)\n    parser.add_argument(\"-t\", type=float, default=0.7)\n    parser.add_argument(\"--ct2_dir\", type=str, required=True)\n    parser.add_argument(\"--tokenizer_path\", type=str, required=True)\n    parser.add_argument(\"--output_path\", type=str, required=True)\n    parser.add_argument(\n        \"--gpu_ids\", type=list_of_ints, default=[0, 1, 2, 3, 4, 5, 6, 7]\n    )\n    parser.add_argument(\"--max_batch_size\", type=int, default=50)\n    parser.add_argument(\"--num_workers\", type=int, default=8)\n    args = parser.parse_args()\n    main(args)"
        }
    ]
}