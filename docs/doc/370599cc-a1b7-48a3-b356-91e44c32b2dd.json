{
    "summary": "The code initializes a class for training Monte Carlo Tree Search models, sets up devices and data loaders, prepares MCTS trainer model with epochs, iterators, saves optimizers, and logs progress. It also utilizes accelerator, removes unnecessary files, and loads checkpoints using load state hook or setting `strict=False`.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a class named `AccelerateMCTSTrainer` that inherits from `BaseMCTSTrainer`. The class takes in a configuration object (`config`) and additional keyword arguments (`**kwargs`). The class is likely for training a model using Monte Carlo Tree Search algorithm.\n\nThis code defines a function named `loop_iter` which loops over a data loader, yielding each batch of data. It also includes a helper function called `load_jsonl` that loads data from a JSON lines file into a list of dictionaries. The `AccelerateMCTSTrainer` class likely utilizes these functions for training and processing data during the training process.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":0-37",
            "content": "import gc\nimport math\nimport os\nfrom typing import List, Optional\nimport numpy as np\nimport torch\nimport transformers\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport time\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nfrom tsllm.envs import get_env_datasets, get_default_sft_data_builder\nfrom tsllm.rl.config import TrainConfig\nfrom tsllm.rl.data.node_types_new import TrajBatch, TrajInstance\nfrom tsllm.rl.data.traj_buffer import MultiTrajBuffer\nfrom tsllm.rl.trainer.base_trainer import BaseMCTSTrainer\nfrom tsllm.rl.trainer.opt_utils import get_scheduler_class\nimport tree as dm_tree\nfrom tqdm import tqdm\nimport json\ndef loop_iter(loader):\n    while True:\n        for x in loader:\n            yield x\ndef load_jsonl(file_path):\n    data_list = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            data = json.loads(line.strip())\n            data_list.append(data)\n    return data_list\nclass AccelerateMCTSTrainer(BaseMCTSTrainer):\n    def __init__(self, config, **kwargs):"
        },
        {
            "comment": "This code initializes an object, sets up the model and optimizer on the accelerator's device, prepares them for training, gets train and test datasets, creates samplers for both datasets, and sets up data loaders for the tasks.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":38-67",
            "content": "        super().__init__(config, **kwargs)\n        self.model = self.setup_model()\n        # run in pure_bf16\n        self.model = self.model.to(self.accelerator.device, torch.bfloat16)\n        self.opt = self.setup_optimizer()\n        (\n            self.model,\n            self.opt,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.opt,\n        )\n        self.train_q_ds, self.test_q_ds = get_env_datasets(\n            self.config.train.env_name, **self.config.train.task_dataset_kwargs\n        )\n        print_with_rank(\n            \"#train tasks: {}, test_tasks: {}\".format(\n                len(self.train_q_ds), len(self.test_q_ds)\n            )\n        )\n        sampler = DistributedSampler(self.train_q_ds, shuffle=False)\n        self.task_train_loader = DataLoader(\n            self.train_q_ds, batch_size=1, sampler=sampler, shuffle=False\n        )\n        test_sampler = DistributedSampler(self.test_q_ds, shuffle=True)\n        self.task_test_loader = DataLoader(\n            self.test_q_ds, batch_size=1, sampler=test_sampler, shuffle=False"
        },
        {
            "comment": "This code initializes the trainer for a machine comprehension task, setting up data buffers and dictionaries for training and testing. It also prepares the Soft-Finetuning (SFt) buffer with pre-collected examples if available. The code loads question-to-index mappings from the train loader and uses a default SFt data builder based on the environment name.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":68-90",
            "content": "        )\n        self.problem_train_iter = loop_iter(self.task_train_loader)\n        self.problem_test_iter = loop_iter(self.task_test_loader)\n        # store imitation data\n        self.sft_buffer = MultiTrajBuffer(\n            num=len(self.task_train_loader),\n            per_problem_max_size=self.config.train.sft_per_problem_max_size,\n            pad_token_id=self.tokenizer.pad_token_id,\n            buffer_name=\"sft\",\n        )\n        self.sft_buffer.clear_all()\n        # question2idx\n        self.q2idx_dict = {}\n        for idx, problem_inst in enumerate(self.task_train_loader):\n            question = problem_inst[\"question\"][0]\n            self.q2idx_dict[question] = idx\n        # init sft buffer with pre-collect examples\n        sft_data_builder_fn = get_default_sft_data_builder(self.config.train.env_name)\n        if self.config.train.pre_sft_datapath is not None:\n            # predata = load_jsonl(self.config.train.pre_sft_datapath)\n            # for d in predata:\n            #     question = d[\"question\"]"
        },
        {
            "comment": "This code checks if a given question is present in the q2idx_dict dictionary. If it is, the task index (task_idx) is retrieved from the dictionary and then iterates over each answer in the \"answer\" list. The result variable is set to 1.0 if the answer is correct and 0.0 otherwise. If the answer is correct, the add_traj function is called with appropriate parameters to store the trajectory data. The sft_data_builder_fn function is then called to build the soft prompt data using the provided arguments.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":91-110",
            "content": "            #     if question in self.q2idx_dict.keys():\n            #         # because question here is split by distributed training,\n            #         # so ignore those questions not stored in the dictionary\n            #         task_idx = self.q2idx_dict[question]\n            #         for a in d[\"answer\"]:\n            #             result = 1.0 if a[\"correct\"] else 0.0\n            #             if a[\"correct\"]:\n            #                 self.add_traj(\n            #                     task_idx,\n            #                     question,\n            #                     a[\"text\"],\n            #                     result,\n            #                     add_sft_buffer=True,\n            #                 )\n            sft_data_list = sft_data_builder_fn(\n                jsonl_path=self.config.train.pre_sft_datapath,\n                q2idx_dict=self.q2idx_dict,\n                tokenizer=self.tokenizer,\n                is_few_shot=self.config.env.is_few_shot,\n                add_eos_token=True,"
        },
        {
            "comment": "Function initializes the scheduler, unwraps the model for training, and sets up a tracker. It also initializes an sft buffer with provided data and finishes with printing its size. The function asserts that the train epoch is set to 1 in the config.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":111-142",
            "content": "            )\n            for sft_data in sft_data_list:\n                self.add_traj(\n                    sft_data[\"idx\"],\n                    query_str=sft_data[\"query_str\"],\n                    response_str=sft_data[\"response_str\"],\n                    result=1,\n                )\n            print_with_rank(\n                \"finish sft buffer initialization. size: {}\".format(\n                    len(self.sft_buffer)\n                )\n            )\n        self.scheduler = self.setup_scheduler()\n        self.scheduler = self.accelerator.prepare(self.scheduler)\n        self.setup_tracker()\n        self.ct2_generator = None\n    @property\n    def unwrap_model(self):\n        # unwrap_model = self.model\n        unwrap_model = self.accelerator.unwrap_model(self.model)  # .module\n        return unwrap_model\n    def setup_scheduler(self):\n        \"\"\"\n        Returns a learning rate scheduler derived from an instance's TRLConfig\n        \"\"\"\n        train_config: TrainConfig = self.config.train\n        assert train_config.train_epoch == 1"
        },
        {
            "comment": "This code calculates the total training steps based on the maximum length of the soft (sft) buffer and other parameters. It creates a scheduler class for learning rate adjustment during training. The total number of training steps, configuration details, and scheduler are returned. Additionally, the previous ct2_generator is deleted and GPU memory is cleared to prepare for new training.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":143-167",
            "content": "        len_sft = torch.tensor(len(self.sft_buffer), device=self.accelerator.device)\n        avg_sft = self.accelerator.gather(len_sft).float()\n        sft_buffer_max_length = avg_sft.max().item()\n        total_training_steps = train_config.epochs * int(\n            sft_buffer_max_length\n            / train_config.sft_micro_batch_size\n            / train_config.gradient_accumulation_steps\n            + 1\n        )\n        print_rank_0(\"Total Training Step: {}.\".format(total_training_steps))\n        scheduler_class = get_scheduler_class(self.config.scheduler.name)\n        if \"warmup\" in self.config.scheduler.name:\n            self.config.scheduler.kwargs[\"num_training_steps\"] = int(\n                total_training_steps\n            )\n        print_rank_0(self.config.scheduler.kwargs)\n        scheduler = scheduler_class(self.opt, **self.config.scheduler.kwargs)\n        return scheduler\n    def prepare_training(self):\n        del self.ct2_generator\n        gc.collect()\n        torch.cuda.empty_cache()\n        self.ct2_generator = None"
        },
        {
            "comment": "The code defines a loss function for the SFT policy and adds a traj (trajectory) to the trainer. The loss is computed from the output of the model, and the loss function returns the total loss and some statistics. If the critic is used to compute V in the SFT trainer, an error will be raised. Dummy values are also set for value index and reward list.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":169-201",
            "content": "    def loss_fn(self, sftbatch: Optional[TrajBatch] = None):\n        stats = {}\n        sftbatch.to(self.accelerator.device)\n        # ============\n        # sft policy loss\n        # ============\n        sft_output = self.model(\n            input_ids=sftbatch.input_ids,\n            labels=sftbatch.label,\n            attention_mask=sftbatch.attn_mask,\n            return_dict=True,\n            use_cache=False,\n        )\n        policy_loss = sft_output.loss\n        loss = self.config.train.sft_loss_coef * policy_loss\n        stats[\"train/sft_policy_loss\"] = policy_loss.detach().item()\n        stats[\"train/total_loss\"] = loss.detach().item()\n        return loss, stats\n    def add_traj(\n        self,\n        idx: int,\n        query_str: str,\n        response_str: str,\n        result: int,\n    ):\n        # result 1 for right, 0 for wrong\n        def policy_forward_seq_value(x):\n            raise RuntimeError(\"IN SFT TRAINER YOU SHOULD NOT USE CRITIC TO COMPUTE V.\")\n        dummy_value_index = np.zeros(1)\n        dummy_reward_list = np.ones(1)"
        },
        {
            "comment": "This code initializes and configures the training process for an MCTS trainer model. It sets up epochs, data loaders, and iterators for training, and prepares the model to start learning from the provided data.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":202-234",
            "content": "        self.sft_buffer.add(\n            idx,\n            TrajInstance.from_string(\n                query_str,\n                response_str,\n                dummy_value_index,\n                dummy_reward_list,\n                self.tokenizer,\n                policy_forward_seq_value,\n                self.config.train.gamma,\n                self.config.train.gae_lambda,\n                cal_value=False,\n            ),\n        )\n    def learn(self, iter_count=0):\n        train_config: TrainConfig = self.config.train\n        self.iter_count = iter_count\n        self.train_step = 0\n        for i_epoch in range(train_config.epochs):\n            stats = {}\n            print_with_rank(\"TRAINING\")\n            self.prepare_training()\n            self.model.train()\n            train_sft_dataloader = self.sft_buffer.create_loader(\n                train_config.sft_micro_batch_size, shuffle=True\n            )\n            train_sft_data_iter = loop_iter(train_sft_dataloader)\n            gas = self.config.train.gradient_accumulation_steps"
        },
        {
            "comment": "This code calculates the number of training steps based on the length of the SFT (Soft-target) buffer and then performs the training iterations. It uses an optimizer and scheduler to update the model's weights and keeps track of the training statistics in the 'train_stats_list'.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":236-262",
            "content": "            # currently use the length of sft buffer as the number of training step\n            len_sft = torch.tensor(\n                len(train_sft_dataloader), device=self.accelerator.device\n            )\n            avg_sft = self.accelerator.gather(len_sft).float()\n            # all sft buffer size\n            all_sft_num = torch.sum(\n                avg_sft.sum() * train_config.sft_micro_batch_size\n            ).item()\n            assert self.config.train.train_epoch == 1\n            nga = int(\n                int(math.ceil(avg_sft.max().item() / gas))\n                * self.config.train.train_epoch\n            )\n            train_stats_list = []\n            t0 = time.time()\n            for i_nga in tqdm(range(nga), disable=not self.local_rank == 0):\n                for i_gas in range(gas):\n                    loss, stats = self.train_iter(train_sft_data_iter)\n                    train_stats_list.append(stats)\n                self.opt.step()\n                self.scheduler.step()\n                self.opt.zero_grad()"
        },
        {
            "comment": "Code snippet calculates statistics for training progress, updates logs and prints loss at the end of each epoch. It also saves intermediate checkpoints periodically based on the provided config settings. The code uses mean averaging to gather stats from previous epochs, handles local rank logging and saving, and tracks total training time.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":263-288",
            "content": "                stats_gas = dm_tree.map_structure(\n                    lambda *xs: np.mean(xs).item(), *train_stats_list[-gas:]\n                )\n                stats_gas[\"train/learning_rate\"] = self.scheduler.get_last_lr()[0]\n                self.accelerator.log(stats_gas, step=self.train_step)\n                self.train_step += 1\n            t1 = time.time()\n            train_stats = dm_tree.map_structure(\n                lambda *xs: np.mean(xs).item(), *train_stats_list\n            )\n            stats.update(train_stats)\n            stats[\"time/training_time\"] = t1 - t0\n            stats[\"train/sft_buffer_size\"] = all_sft_num\n            if self.local_rank == 0:\n                print_rank_0(\"LOSS: {:.4f}, {}\".format(loss, stats))\n            if self.iter_count % train_config.checkpoint_interval == 0:\n                subfolder = f\"checkpoint_{self.iter_count}_ep{i_epoch}\"\n                directory = os.path.join(train_config.checkpoint_dir, subfolder)\n                print_with_rank(f\"Saving intermediate checkpoint into {directory}\")"
        },
        {
            "comment": "This code snippet is for a machine learning model trainer that performs training iterations. It saves the optimizer if specified, saves the pretrained model in a specific directory, updates statistics, logs statistics to track progress, and measures forward and backward propagation time.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":289-316",
            "content": "                if train_config.save_optimizer:\n                    self.save(directory)\n                else:\n                    self.save_pretrained(directory)\n                self.save_pretrained(\n                    os.path.join(train_config.checkpoint_dir, \"last_model_hf\")\n                )\n            # replace all key\n            for key in list(stats.keys()):\n                if \"loss\" in key:\n                    stats[key.replace(\"loss\", \"average_loss\")] = stats.pop(key)\n            self.accelerator.log(stats, step=self.iter_count)\n            self.iter_count += 1\n    def train_iter(self, train_sft_data_iter):\n        forward_time = -time.time()\n        # onpolicy data loss\n        sftbatch = next(train_sft_data_iter)\n        loss, stats = self.loss_fn(sftbatch)\n        forward_time += time.time()\n        backward_time = -time.time()\n        self.accelerator.backward(loss)\n        backward_time += time.time()\n        stats[\"time/forward_time\"] = forward_time\n        stats[\"time/backward_time\"] = backward_time"
        },
        {
            "comment": "This code defines a method `save_pretrained` for the `Trainer` class, which saves the underlying Hugging Face model, tokenizer, and configuration files to a directory. It takes an optional `directory` argument and additional keyword arguments. If no `directory` is specified, it saves the model to a directory named `hf_model` in the checkpoint directory as per the Trainer's config. The method ensures that all processes have finished before saving the model and uses the accelerator's save function for saving.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":318-338",
            "content": "        return loss.item(), stats\n    def save_pretrained(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Save the underlying Hugging Face model, tokenizer, and configuration files to a directory for\n        later use.\n        Args:\n            directory (str, *optional*): The directory to save the trainer files to.\n                NOTE: If not specified, the model will be saved to a directory named `hf_model` in the\n                checkpoint directory as specified by the Trainer's config.\n            **kwargs: Additional keyword arguments passed to the underlying Hugging Face model's\n                `save_pretrained` method.\n        \"\"\"\n        if directory is None:\n            directory = os.path.join(self.config.train.checkpoint_dir, \"hf_model\")\n        self.accelerator.wait_for_everyone()\n        self.accelerator.unwrap_model(self.model).save_pretrained(\n            directory,\n            save_function=self.accelerator.save,\n            is_main_process=self.accelerator.is_main_process,"
        },
        {
            "comment": "The code defines a class with methods for saving and loading model checkpoints, optimizer, and scheduler. It uses accelerator to handle state and file operations. If PEFT configuration is provided, it removes unnecessary files during saving and loading processes.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":339-362",
            "content": "            state_dict=self.accelerator.get_state_dict(self.model),\n            **kwargs,\n        )\n        if self.accelerator.is_main_process:\n            self.tokenizer.save_pretrained(directory)\n    def save(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Creates a checkpoint of the optimizer, scheduler and model\"\"\"\n        dst_dir = directory or self.config.train.checkpoint_dir\n        self.accelerator.save_state(dst_dir, **kwargs)\n        if (\n            self.config.model.peft_config is not None\n            and self.accelerator.is_main_process\n        ):\n            # Remove \"pytorch_model.bin\" because it contains more than necessary,\n            # let save_pretrained recreate it with just the value heads.\n            model_file = os.path.join(dst_dir, \"pytorch_model.bin\")\n            if os.path.exists(model_file):\n                os.remove(model_file)\n            self.accelerator.unwrap_model(self.model).save_pretrained(dst_dir)\n    def load(self, directory: Optional[str] = None, **kwargs):"
        },
        {
            "comment": "This code is loading a checkpoint of an optimizer, scheduler, and a model. If the `config.model.peft_config` is not None, it registers a load state hook to load all models in the list. Otherwise, it sets `strict=False`. Finally, it loads the state from the directory or config's checkpoint_dir with the given parameters.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/mcts_trainer_traj_ct2_sft.py\":363-381",
            "content": "        \"\"\"Load checkpoint of optimizer, scheduler and a model\"\"\"\n        if self.config.model.peft_config is not None:\n            def load_state_hook(models: List[torch.nn.Module], input_dir: str):\n                with self.accelerator.main_process_first():\n                    for model in models:\n                        model.from_pretrained(input_dir)\n            self.accelerator.register_load_state_pre_hook(load_state_hook)\n            strict = False\n        else:\n            strict = True\n        self.accelerator.load_state(\n            directory or self.config.train.checkpoint_dir,\n            load_module_strict=strict,\n            **kwargs,\n        )"
        }
    ]
}