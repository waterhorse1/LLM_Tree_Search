{
    "summary": "The code defines the `value_fn` function, which takes a critic model, tokenizer, and input text. It prepares the input text, passes it through the critic model to obtain value predictions for each word, gathers the results, and returns them as numpy float arrays in torch inference mode.",
    "details": [
        {
            "comment": "This code defines the function `value_fn` which takes a critic model, tokenizer, and input text as inputs. It prepares the input text, passes it through the critic model to obtain value predictions for each word in the input text. The returned values are then gathered and converted into numpy float arrays.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/inference/value.py\":0-32",
            "content": "import torch\nfrom typing import Union, List\nfrom tsllm.model import ValueHeadedLLM\nfrom tsllm.model.modeling_actor_critic import AutoModelForCausalLMWithValueHead\nfrom tsllm.llm.text_generation import llm_gen_ct2\nfrom transformers import AutoTokenizer\nimport re\nimport numpy as np\n@torch.inference_mode()\ndef value_fn(\n    critic: ValueHeadedLLM, tokenizer: AutoTokenizer, input_str: Union[List[str], str]\n):\n    if isinstance(input_str, list):\n        indices2pick = torch.LongTensor(\n            [len(tokenizer.encode(txt)) - 1 for txt in input_str]\n        )\n    else:\n        indices2pick = torch.LongTensor([len(tokenizer.encode(input_str)) - 1])\n    # print(input_str)\n    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True).to(critic.device)\n    if \"token_type_ids\" in inputs:\n        inputs.pop(\"token_type_ids\")\n    value = critic(**inputs).value.cpu()\n    value = value.gather(1, indices2pick.unsqueeze_(1)).squeeze_(1).float().numpy()\n    return value\n@torch.inference_mode()\ndef value_fn_rlhf(\n    critic: AutoModelForCausalLMWithValueHead,"
        },
        {
            "comment": "This function takes a critic model, tokenizer, and input string. It converts the input string into tokenized tensors using the tokenizer and moves them to the device of the critic model. Then, it calculates the value using the critic model and returns it as a float numpy array. The `@torch.inference_mode()` decorator enables torch inference mode for this function.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/inference/value.py\":33-54",
            "content": "    tokenizer: AutoTokenizer,\n    input_str: Union[List[str], str],\n):\n    if isinstance(input_str, list):\n        indices2pick = torch.LongTensor(\n            [len(tokenizer.encode(txt)) - 1 for txt in input_str]\n        )\n    else:\n        indices2pick = torch.LongTensor([len(tokenizer.encode(input_str)) - 1])\n    inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True).to(critic.device)\n    value = critic(**inputs, return_dict=True).value.cpu()\n    value = value.gather(1, indices2pick.unsqueeze_(1)).squeeze_(1).float().numpy()\n    return value\n@torch.inference_mode()\ndef seq_value_fn(critic_model, tokenizer, input_str):\n    input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids.to(\n        critic_model.device\n    )\n    value = critic_model(input_ids, return_dict=True).value\n    return value.cpu().float().numpy()"
        }
    ]
}