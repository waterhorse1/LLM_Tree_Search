{
    "summary": "This code imports modules, sets up Lora and Flash Attention for Llama fine-tuning, initializes AccelerateMCTSTrainer with training details, and trains the model using the \"learn\" method.",
    "details": [
        {
            "comment": "This code imports necessary modules, sets up a Lora config and PeftType for model fine-tuning. It replaces the attention mechanism in Llama with Flash Attention. The configuration includes the model path, tokenizer path, optimizer settings, scheduler settings, and training data paths.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/train_mcts_scripts/prontoqa/train_prontoqa_critic.py\":0-22",
            "content": "from tsllm.rl.trainer.mcts_trainer_traj_ct2_value import AccelerateMCTSTrainer\nfrom tsllm.rl.config import RLConfig\nfrom peft import LoraConfig, PeftType\nfrom tsllm.model.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\nreplace_llama_attn_with_flash_attn()\nconfig = {\n    \"model\": {\n        \"model_path\": \"meta-llama/Llama-2-7b-hf\",\n    },\n    \"tokenizer\": {\n        \"tokenizer_path\": \"meta-llama/Llama-2-7b-hf\",\n        \"padding_side\": \"right\",\n    },\n    \"optimizer\": {\n        \"name\": \"adamw\",\n        \"kwargs\": dict(lr=2.0e-5, betas=(0.9, 0.999), eps=1.0e-8, weight_decay=0.0),\n    },\n    \"scheduler\": {\"name\": \"cosine_warmup\", \"kwargs\": dict(warmup_ratio=0.03)},\n    \"train\": {\n        \"pre_onpolicy_datapath\": \"../../tsllm/offline_rl/prontoqa/processed/prontoqa_train_cot_sample_offline_sft_k100_ep1_dedup_sample50.jsonl\",\n        \"pre_onpolicy_datapath_train_test\": \"../../tsllm/offline_rl/prontoqa/processed/prontoqa_train_cot_sample_offline_sft_k100_ep1_dedup_sample50_train_test_sample_3.jsonl\","
        },
        {
            "comment": "This code initializes an AccelerateMCTSTrainer with the given configuration and then calls the \"learn\" method to train the model. The configuration includes details such as epochs, training epoch, gamma, GAE lambda, sequence length, micro batch size, and more.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/train_mcts_scripts/prontoqa/train_prontoqa_critic.py\":23-48",
            "content": "        \"env_name\": \"prontoqa\",\n        \"epochs\": 3,  # this is the epoch for the whole sampling/training process\n        \"train_epoch\": 1,  # this is the epoch for training process after each sampling\n        \"gamma\": 1.0,\n        \"gae_lambda\": 0.95,\n        \"seq_length\": 1024,\n        \"micro_batch_size\": 4,\n        \"gradient_accumulation_steps\": 4,\n        \"value_loss_coef\": 1.0,\n        \"eval_interval\": 1,\n        \"checkpoint_interval\": 1,\n        \"checkpoint_dir\": tmp_for_check,\n        \"save_optimizer\": False,\n        \"project_name\": \"tmp_for_check\",\n        \"tracker\": \"tensorboard\",\n        \"logging_dir\": \"logs/\",\n        \"onpolicy_per_problem_max_size\": 1000,\n    },\n    \"mcts\": {},\n    \"env\": {},\n}\nconfig = RLConfig.from_dict(config)\ntrainer = AccelerateMCTSTrainer(config)\ntrainer.learn()"
        }
    ]
}