{
    "summary": "The code sets up an environment for PrOntoQA question answering task, using Llama-2-7b-hf model for tokenization. It creates and prints the length of different datasets, with 'sft_data' and 'critic_data' generated by get_default_sft_data_builder and get_default_critic_data_builder functions respectively. The code also performs additional print statements on selected elements from critic_data.",
    "details": [
        {
            "comment": "This code imports necessary modules and defines a PrOntoQAEnv class, which is an environment for a question answering task. The code then creates an instance of this class to solve a specific problem input and check if the statement \"Polly is not small\" is true or false.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_prontoqa.py\":0-18",
            "content": "from tsllm.envs.prontoqa.env import (\n    PrOntoQAEnv,\n    COT_EXAMPLES,\n    COT_TASK_DESC,\n    PROBLEM_FORMAT_STR,\n    SEP,\n)\nif __name__ == \"__main__\":\n    problem_input = 'Butterflies are lepidopterans. Every arthropod is small. Whales are not small. Invertebrates are animals. Every insect is an arthropod. Lepidopterans are insects. Every insect is six-legged. Every arthropod is an invertebrate. Animals are multicellular. Polly is a lepidopteran. Is the statement \"Polly is not small\" true or false?'\n    env = PrOntoQAEnv(\n        config={},\n        math_problems=[\n            {\n                \"question\": 'Butterflies are lepidopterans. Every arthropod is small. Whales are not small. Invertebrates are animals. Every insect is an arthropod. Lepidopterans are insects. Every insect is six-legged. Every arthropod is an invertebrate. Animals are multicellular. Polly is a lepidopteran. Is the statement \"Polly is not small\" true or false?',\n                \"answer\": False,\n            }\n        ],\n        tokenizer=None,"
        },
        {
            "comment": "This code initializes an environment for PrOntoQA, prints its state, and demonstrates zero-shot and few-shot capabilities. It then tokenizes using the Llama-2-7b-hf model and retrieves default SFG data builder for \"prontoqa\".",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_prontoqa.py\":19-51",
            "content": "        llm_gen_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    print(\"correct: \", env._is_correct(\"The answer is false.\"))\n    build_query_str = PrOntoQAEnv.build_query_str\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    print(\"\\n\\n====== FEW SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, True\n        )\n    )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _ = get_env_datasets(\"prontoqa\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx"
        },
        {
            "comment": "This code snippet is responsible for creating and printing the length of different datasets. It uses the get_default_sft_data_builder and get_default_critic_data_builder functions to generate 'sft_data' and 'critic_data' respectively, which contain training data for a model. The code then prints the lengths of these datasets (train_ds, sft_data, critic_data) and performs some additional print statements on selected elements from critic_data.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_prontoqa.py\":52-78",
            "content": "    sft_data = get_default_sft_data_builder(\"prontoqa\")(\n        \"tsllm/envs/prontoqa/train_data/train.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")\n    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"prontoqa\")(\n        \"tsllm/envs/prontoqa/train_data/train.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False,\n    )\n    print(\n        \"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(\n            len(critic_data), critic_data[0]\n        )\n    )\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"] + critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))"
        }
    ]
}