{
    "summary": "This code introduces a `CategoricalHeadLMOutputWithPast` class for causal language models, improving sequential decoding speed. It also defines a language model output class with methods for loading, initializing, and cleanup, along with return types for a language modeling function, a ValueHeadedLLM class as a wrapper for AutoModel base model, enabling pre-trained model loading while maintaining additional layers like cat_head module.",
    "details": [
        {
            "comment": "This code defines a `CategoricalHeadLMOutputWithPast` class that represents the outputs of a causal language model. It includes properties like loss, logits, and past_key_values. The `@dataclass` decorator is used for data class functionality, while `transformers` library imports are utilized for necessary functionalities.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":0-23",
            "content": "from collections import OrderedDict\nfrom dataclasses import dataclass\nimport gc\nfrom typing import Optional, Tuple\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\nimport transformers\nfrom transformers.modeling_outputs import ModelOutput\nfrom tsllm.model.modeling_base import PreTrainedModelWrapper\n@dataclass\nclass CategoricalHeadLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for causal language model (or autoregressive) outputs.\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape"
        },
        {
            "comment": "This code defines a model with output features including past key-values, hidden states, and attentions. The past key-values are precomputed hidden states used for speeding up sequential decoding. The `hidden_states` contains the model's hidden states at the output of each layer, and optional initial embedding outputs if present. The `attentions` is a tuple containing the attention scores for each layer, returned when either `output_attentions=True` or `config.output_attentions=True`.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":24-34",
            "content": "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,"
        },
        {
            "comment": "Class for a language model with a categorical head, initializes the base model, adds a linear layer for classification tasks, and defines forward pass to include the base model's output and the new layer. Loss function can be specified in constructor.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":35-72",
            "content": "            sequence_length)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\nclass CategoricalHeadedLLM(PreTrainedModelWrapper):\n    _auto_model_parent_class = AutoModel\n    _supported_modules = [\"cat_head\"]\n    _supported_args = [\"n_out\", \"loss_fn\"]\n    def __init__(\n        self,\n        base_model: AutoModel,\n        n_out: int = 3,\n        loss_fn: callable = nn.CrossEntropyLoss(),\n        **kwargs,\n    ):\n        super().__init__(base_model, **kwargs)\n        self.base_model = base_model\n        self.n_out = n_out\n        hidden_state = self.base_model.config.hidden_size\n        self.cat_head = nn.Linear(hidden_state, n_out)\n        self.loss_fn = loss_fn\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        labels: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ):\n        outputs = self.base_model("
        },
        {
            "comment": "This code is for a model that outputs the logits and loss of a categorical language modeling task. It uses gradient checkpointing to save memory, and allows for retrieving only the state dictionary of the head layer if needed. The function `gradient_checkpointing_enable` enables gradient checkpointing in the base model, while `state_dict` returns the state dictionary of both the model and the head layer.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":73-98",
            "content": "            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n        hidden_states = outputs[0]\n        logits = self.cat_head(hidden_states)\n        loss = None\n        if labels is not None:\n            labels = labels.view(-1).to(logits.device)\n            # you should keep logits with shape (-1, n_class) and labels with shape [-1]\n            loss = self.loss_fn(logits.view(-1, logits.size(-1)), labels)\n        return CategoricalHeadLMOutputWithPast(loss=loss, logits=logits)\n    def gradient_checkpointing_enable(self):\n        self.base_model.gradient_checkpointing_enable()\n    def state_dict(self, *args, heads_only=False, **kwargs):\n        \"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `cat_head.`.\n        \"\"\"\n        cat_head_state_dict = self.cat_head.state_dict(*args, **kwargs)\n        if heads_only:\n            model_state_dict = OrderedDict()"
        },
        {
            "comment": "This code defines a class with methods to load the state dictionary of a model with a categorical head, and adds it to the base model's state dictionary. It also includes a method for initializing the model from a configuration file and a cleanup function to remove unnecessary keys.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":99-127",
            "content": "        else:\n            model_state_dict = self.base_model.state_dict(*args, **kwargs)\n        for k, v in cat_head_state_dict.items():\n            model_state_dict[f\"cat_head.{k}\"] = v\n        return model_state_dict\n    def post_init(self, state_dict):\n        \"\"\"\n        Adds the state dictionary of the value head to the state dictionary of the wrapped model\n        by prepending the key with `cat_head.`. This function removes the `cat_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        super().post_init()\n        for k in list(state_dict.keys()):\n            if \"cat_head.\" in k:\n                state_dict[k.replace(\"cat_head.\", \"\")] = state_dict.pop(k)\n        self.cat_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n        gc.collect()\n    @classmethod\n    def from_config(cls, config: transformers.PretrainedConfig, **kwargs):\n        \"\"\"Instantiate the pretrained pytorch model from a configuration.\n        Args:\n            config (transformers.PretrainedConfig): The configuration to use to"
        },
        {
            "comment": "This code defines a method to instantiate the base model and create a language model output class. The base model is instantiated by loading its configuration file, but not the weights. The `AutoModel.from_pretrained` function should be used separately to load the model weights. The `ValueHeadLMOutputWithPast` class represents the output of a causal language model (autoregressive) and includes properties like loss and logits.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":128-155",
            "content": "                instantiate the base model.\n        NOTE: Loading a model from its configuration file does **not** load the\n        model weights. It only affects the model's configuration. Use\n        `~transformers.AutoModel.from_pretrained` to load the model weights.\n        \"\"\"\n        if kwargs is not None:\n            wrapped_model_kwargs, from_config_kwargs = cls._split_kwargs(kwargs)\n        else:\n            from_config_kwargs = {}\n            wrapped_model_kwargs = {}\n        base_model = cls._auto_model_parent_class.from_config(\n            config, **from_config_kwargs\n        )\n        model = cls(base_model, **wrapped_model_kwargs)\n        return model\n@dataclass\nclass ValueHeadLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for causal language model (or autoregressive) outputs.\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):"
        },
        {
            "comment": "The code defines the return types for a language modeling function. It returns prediction scores, optional past key values for faster sequential decoding, and optionally hidden states if output_hidden_states is set to True or config.output_hidden_states. The past_key_values are tuples of tensors of shape (batch_size, num_heads, sequence_length, embed_size_per_head). Hidden_states are optional and provided as a tuple of torch.FloatTensor of shape (batch_size, sequence_length, hidden_size) if output_hidden_states is set to True or config.output_hidden_states.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":156-165",
            "content": "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`."
        },
        {
            "comment": "The code defines a class called ValueHeadedLLM, which is a wrapper for an AutoModel base model. It has an optional loss function (defaulting to nn.CrossEntropyLoss()) and adds a \"cat_head\" module. The class also stores the hidden states of the model at each layer and attentions weights for self-attention heads as optional outputs.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":167-190",
            "content": "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n    loss: Optional[torch.FloatTensor] = None\n    value: torch.FloatTensor = None\nclass ValueHeadedLLM(PreTrainedModelWrapper):\n    _auto_model_parent_class = AutoModel\n    _supported_modules = [\"cat_head\"]\n    _supported_args = [\"loss_fn\"]\n    def __init__(\n        self, base_model: AutoModel, loss_fn: callable = nn.CrossEntropyLoss(), **kwargs\n    ):\n        super().__init__(base_model, **kwargs)\n        self.base_model = base_model\n        hidden_state = self.base_model.config.hidden_size"
        },
        {
            "comment": "This code defines a model with a value head and loss function. The forward method processes input_ids, attention_mask, and returns the value and loss. The gradient_checkpointing_enable method enables gradient checkpointing for the base model. The state_dict method returns the model's state dictionary along with the value head's state dictionary if heads_only is True.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":192-222",
            "content": "        self.cat_head = nn.Linear(hidden_state, 1)\n        self.loss_fn = loss_fn\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        labels: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ):\n        outputs = self.base_model(\n            input_ids=input_ids, attention_mask=attention_mask, use_cache=False\n        )\n        hidden_states = outputs[0]\n        value = self.cat_head(hidden_states).squeeze(dim=-1)\n        loss = None\n        return ValueHeadLMOutputWithPast(loss=loss, value=value)\n    def gradient_checkpointing_enable(self):\n        self.base_model.gradient_checkpointing_enable()\n    def state_dict(self, *args, heads_only=False, **kwargs):\n        \"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `cat_head.`.\n        \"\"\"\n        cat_head_state_dict = self.cat_head.state_dict(*args, **kwargs)\n        if heads_only:"
        },
        {
            "comment": "This code defines two functions for handling the state dictionary of a model. The first function, \"get_model_state_dict\", checks if the input is an OrderedDict or not, and returns the base model's state dictionary with the cat_head appended to each key. The second function, \"post_init\", takes in a state dictionary, removes the \"cat_head.\" prefix from its keys, and loads the updated dictionary into the cat_head module of the model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":223-249",
            "content": "            model_state_dict = OrderedDict()\n        else:\n            model_state_dict = self.base_model.state_dict(*args, **kwargs)\n        for k, v in cat_head_state_dict.items():\n            model_state_dict[f\"cat_head.{k}\"] = v\n        return model_state_dict\n    def post_init(self, state_dict):\n        \"\"\"\n        Adds the state dictionary of the value head to the state dictionary of the wrapped model\n        by prepending the key with `cat_head.`. This function removes the `cat_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        super().post_init()\n        for k in list(state_dict.keys()):\n            if \"cat_head.\" in k:\n                state_dict[k.replace(\"cat_head.\", \"\")] = state_dict.pop(k)\n        self.cat_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n        gc.collect()\n    @classmethod\n    def from_config(cls, config: transformers.PretrainedConfig, **kwargs):\n        \"\"\"Instantiate the pretrained pytorch model from a configuration."
        },
        {
            "comment": "This code defines a function that loads a model's configuration from a file and instantiates the base model using the config. It does not load the model weights. Use `~transformers.AutoModel.from_pretrained` to load the model weights.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_prm.py\":251-269",
            "content": "        Args:\n            config (transformers.PretrainedConfig): The configuration to use to\n                instantiate the base model.\n        NOTE: Loading a model from its configuration file does **not** load the\n        model weights. It only affects the model's configuration. Use\n        `~transformers.AutoModel.from_pretrained` to load the model weights.\n        \"\"\"\n        if kwargs is not None:\n            wrapped_model_kwargs, from_config_kwargs = cls._split_kwargs(kwargs)\n        else:\n            from_config_kwargs = {}\n            wrapped_model_kwargs = {}\n        base_model = cls._auto_model_parent_class.from_config(\n            config, **from_config_kwargs\n        )\n        model = cls(base_model, **wrapped_model_kwargs)\n        return model"
        }
    ]
}