{
    "summary": "The code introduces a \"PreTrainedModelWrapper\" class that extends `transformers.PreTrainedModel` and integrates HuggingFace's `trl` library for Peft model support, efficiently downloading large models using Hub API, merges PyTorch downloads, and handles model states with post-initialization methods.",
    "details": [
        {
            "comment": "The code defines a class called \"PreTrainedModelWrapper\" that serves as a wrapper for the `transformers.PreTrainedModel` and extends it with additional functionality from HuggingFace's `trl` library. It allows for easy integration of Peft models and provides push-to-hub capabilities. The code references the original source code for this class, which can be found on GitHub.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":0-31",
            "content": "# NOTE: This file contains a modified version of the `PreTrainedModelWrapper` class from\n# HuggingFace's `trl` library. The original source code can be found here:\n# https://github.com/lvwerra/trl/blob/78c13226bf8ea1ccd9b1c091f03a938098521f6c/trl/models/modeling_base.py\nimport inspect\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom huggingface_hub import hf_hub_download\nfrom peft import (\n    PeftConfig,\n    get_peft_config,\n    get_peft_model,\n    get_peft_model_state_dict,\n    PeftModel,\n)\nfrom tsllm.distributed.utils import print_rank_0, print_with_rank\nclass PreTrainedModelWrapper(nn.Module, transformers.utils.PushToHubMixin):\n    \"\"\"A wrapper around `transformers.PreTrainedModel`\n    Reference: @younesbelkada's `PreTrainedModelWrapper`\n    https://github.com/lvwerra/trl/blob/4f5c16fafde42d9aca971952bcdcc1f5a0a68cf0/trl/models/modeling_base.py#L2\n    Attributes:\n        _auto_model_parent_class (transformers.AutoModel): The `transformers.AutoModel`"
        },
        {
            "comment": "This code defines a class with three attributes: _auto_model_parent_class, _supported_modules, and _supported_args. These attributes are used to specify the type of underlying model, the modules of the underlying architecture to save and load, and specific arguments for the underlying architecture, respectively. The __init__ method is called when creating an instance of this class.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":32-50",
            "content": "            type to base the wrapping behavior off of, e.g. `transformers.AutoModelForCausalLM`.\n        _supported_modules (List[str]): A list of attribute names for modules of\n            the underlying architecture model. This is used, for example, to save\n            and load any additional modules by manipulating the state dict.\n        _supported_args (List[str]): A list of arguments specific to the underlying\n            architecture to separate from arguments that are supported by the\n            parent `AutoModel` class. Any arguments that are not supported by the\n            underlying model will be passed to the parent `AutoModel` class.\n    \"\"\"\n    _auto_model_parent_class: transformers.AutoModel = None\n    _supported_modules: List[str] = None\n    # Supported args should come from a `PretrainedConfig` of the\n    # specific underlying type similar to how config instances can be used to instantiate\n    # `transformers.PreTrainedModel`s.\n    _supported_args: List[str] = []\n    def __init__(\n        self,"
        },
        {
            "comment": "This code defines a class that serves as a base for PEFT models. It initializes the base model, caches forward function arguments, and handles 8-bit quantization. It also provides device property access and delegates transition scores computation to the underlying base model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":51-76",
            "content": "        base_model: Optional[transformers.PreTrainedModel] = None,\n        peft_config: Optional[PeftConfig] = None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.base_model = base_model\n        # cache `forward` args for general use (avoids incompatible args across architectures)\n        self.forward_kwargs = inspect.getfullargspec(self.base_model.forward).args\n        self.is_loaded_in_8bit = getattr(base_model, \"is_loaded_in_8bit\", False)\n        if self.is_loaded_in_8bit:\n            raise NotImplementedError(\n                \"`is_loaded_in_8bit` is an experimental feature not yet fully supported. Please do not use it.\"\n            )\n        self.peft_config = peft_config\n    @property\n    def device(self):\n        return self.base_model.device\n    def compute_transition_scores(self, *args, **kwargs):\n        return self.base_model.compute_transition_scores(*args, **kwargs)\n    @classmethod\n    def _split_kwargs(cls, kwargs: Dict[str, Any]):\n        \"\"\"Separates the kwargs from the supported arguments within `supported_args`"
        },
        {
            "comment": "The code defines a class method `from_config` that instantiates a pretrained PyTorch model from a configuration. It also includes a helper function `_split_kwargs` that splits the input kwargs into supported and unsupported ones. This helps to ensure only the supported arguments are used for instantiation, while preserving the unsupported ones.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":77-106",
            "content": "        and those that are not\n        \"\"\"\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        for key, value in kwargs.items():\n            if key in cls._supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n        return supported_kwargs, unsupported_kwargs\n    @classmethod\n    def from_config(\n        cls,\n        config: transformers.PretrainedConfig,\n        peft_config: Optional[PeftConfig] = None,\n        **kwargs,\n    ):\n        \"\"\"Instantiate the pretrained pytorch model from a configuration.\n        Args:\n            config (transformers.PretrainedConfig): The configuration to use to\n                instantiate the base model.\n        NOTE: Loading a model from its configuration file does **not** load the\n        model weights. It only affects the model's configuration. Use\n        `~transformers.AutoModel.from_pretrained` to load the model weights.\n        \"\"\"\n        if kwargs is not None:\n            wrapped_model_kwargs, from_config_kwargs = cls._split_kwargs(kwargs)"
        },
        {
            "comment": "This code defines a class method for instantiating pretrained PyTorch models from a pretrained model configuration. It first checks the given arguments, and if no specific model is provided, it creates an instance of the base model using the config provided. If a PEFT (Parameter-Efficient Fine-Tuning) configuration is present, it replaces the base model with a PEFT-wrapped model. The method ultimately returns the instantiated pretrained PyTorch model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":107-135",
            "content": "        else:\n            from_config_kwargs = {}\n            wrapped_model_kwargs = {}\n        base_model = cls._auto_model_parent_class.from_config(\n            config, **from_config_kwargs\n        )\n        if peft_config:\n            base_model = get_peft_model(peft_config)\n        model = cls(base_model, **wrapped_model_kwargs)\n        return model\n    @classmethod\n    def from_pretrained(  # noqa: max-complexity\n        cls,\n        pretrained_model_name_or_path: Union[str, transformers.PreTrainedModel],\n        revision=None,\n        peft_config: Optional[PeftConfig] = None,\n        *model_args,\n        **kwargs,\n    ):\n        \"\"\"Instantiate a pretrained pytorch model from a pretrained model configuration.\n        This method is a wrapper around `transformers.PreTrainedModel.from_pretrained`.\n        Please refer to the documentation of `transformers.PreTrainedModel.from_pretrained`\n        for more information.\n        Args:\n            pretrained_model_name_or_path (str or `transformers.PreTrainedModel`):"
        },
        {
            "comment": "This code defines a function that loads a pretrained model or the model itself by specifying its identifier. The function takes three parameters: `model_id`, which is either the identifier of the pretrained model or the pretrained model itself; `revision`, an optional Git branch, tag, or commit hash; and `model_args`, a sequence of positional arguments that will be passed to the underlying `_auto_model_parent_class`. The function also takes keyword arguments `**kwargs`, which are passed both to the `_auto_model_parent_class` call and the specific instance of the wrapped model. Note that any keyword arguments specific to the wrapped model must be passed as `peft_from_pretrained_kwargs` or `peft_int8_kwargs`.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":136-151",
            "content": "                The identifier of the pretrained model to load or the pretrained model itself.\n            revision (str, *optional*): Optional specific Git branch, tag or commit hash.\n            *model_args (sequence of positional arguments, *optional*):\n                All remaining positional arguments will be passed to the `_auto_model_parent_class`.\n            **kwargs (dict, *optional*):\n                Dictionary of keyword arguments to pass to both the underlying `_auto_model_parent_class`\n                call (e.g. `transformers.AutoModelForCausalLM.from_pretrained`) and the specific\n                instance of the wrapped model.\n        NOTE: You must pass in arguments specific to the wrapped model as keyword arguments.\n        \"\"\"\n        if kwargs is not None:\n            peft_from_pretrained_kwargs = kwargs.pop(\"peft_from_pretrained_kwargs\", {})\n            peft_int8_kwargs = kwargs.pop(\"peft_int8_kwargs\", {})\n            wrapped_model_kwargs, from_pretrained_kwargs = cls._split_kwargs(kwargs)"
        },
        {
            "comment": "This code checks if a pretrained model is loaded in 8-bit mode. If so, it raises a NotImplementedError as this feature is not yet supported. The code also determines if there's a local peft adapter by checking for the existence of 'adapter_config.json'. If found, the base model is set to None.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":152-181",
            "content": "        else:\n            peft_from_pretrained_kwargs = {}\n            peft_int8_kwargs = {}\n            from_pretrained_kwargs = {}\n            wrapped_model_kwargs = {}\n        if isinstance(pretrained_model_name_or_path, str):\n            is_loaded_in_8bit = (\n                from_pretrained_kwargs[\"load_in_8bit\"]\n                if \"load_in_8bit\" in from_pretrained_kwargs\n                else False\n            )\n        else:\n            is_loaded_in_8bit = getattr(\n                pretrained_model_name_or_path, \"is_loaded_in_8bit\", False\n            )\n        if is_loaded_in_8bit:\n            raise NotImplementedError(\n                \"`is_loaded_in_8bit` is not yet fully supported. Please do not use it.\"\n            )\n        if isinstance(pretrained_model_name_or_path, str):\n            # Check if there is a local peft adapter\n            local_peft_adapter = os.path.exists(\n                os.path.join(pretrained_model_name_or_path, \"adapter_config.json\")\n            )\n            base_model = None\n            try:"
        },
        {
            "comment": "This code attempts to create a new peft adapter with a given config. If a peft_config is provided, it will print a warning and ignore the pretrained model's config file. It then initializes a base model from the pre-trained model path and applies the peft adapter on it. Finally, it prints \"peft adapter initialized\".",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":182-202",
            "content": "                trained_adapter_config = PeftConfig.from_pretrained(\n                    pretrained_model_name_or_path\n                )\n            except ValueError:\n                trained_adapter_config = None\n            if peft_config is not None:\n                if trained_adapter_config is not None:\n                    print_with_rank(\n                        f\"WARNING: peft config file detected in {pretrained_model_name_or_path}\"\n                        \" but ignored since the argument `peft_config` is provided. Remove the\"\n                        \" argument `peft_config` to use the trained peft adapter.\"\n                    )\n                # Create a new peft adapter with the given config\n                base_model = cls._auto_model_parent_class.from_pretrained(\n                    pretrained_model_name_or_path, *model_args, **from_pretrained_kwargs\n                )\n                base_model = get_peft_model(base_model, peft_config)\n                print_with_rank(\"peft adapter initialised\")"
        },
        {
            "comment": "This code checks if a trained adapter configuration is present. If it is, it loads the pretrained adapter and base model using PeftModel, then prints a message. If no PEFT is available, it simply loads the base model from the specified directory.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":204-225",
            "content": "            elif trained_adapter_config is not None:\n                peft_config = trained_adapter_config\n                # Use the pretrained (local or remote) peft adapter file \"adapter_config.json\"\n                base_model = cls._auto_model_parent_class.from_pretrained(\n                    trained_adapter_config.base_model_name_or_path,\n                    *model_args,\n                    **from_pretrained_kwargs,\n                )\n                # Load the peft weights in \"adapter_model.bin\" and wrap the base model with a PeftModel\n                base_model = PeftModel.from_pretrained(\n                    base_model,\n                    pretrained_model_name_or_path,\n                    **peft_from_pretrained_kwargs,\n                )\n                print_with_rank(\"Trained peft adapter loaded\")\n            if base_model is None:\n                # No peft\n                print_with_rank(\"Load Base Model, Won't matter if Warning\")\n                base_model = cls._auto_model_parent_class.from_pretrained("
        },
        {
            "comment": "This code initializes a model based on the provided `pretrained_model_name_or_path`. If it is a string, it loads the PyTorch model binary file. If it's an instance of transformers.PreTrainedModel, it uses it directly or applies PEFT if a peft_config is provided. The wrapped model kwargs are then passed to the class constructor, and the model is initialized.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":226-249",
            "content": "                    pretrained_model_name_or_path, *model_args, **from_pretrained_kwargs\n                )\n        elif isinstance(pretrained_model_name_or_path, transformers.PreTrainedModel):\n            base_model = pretrained_model_name_or_path\n            if peft_config is not None:\n                base_model = get_peft_model(base_model, peft_config)\n                print_with_rank(\"peft adapter initialized\")\n        else:\n            raise ValueError(\n                f\"Invalid type for `base_model_name_or_path`: {type(pretrained_model_name_or_path)}\"\n                \"Expected `str` or `transformers.PreTrainedModel`.\"\n            )\n        if peft_config is not None:\n            wrapped_model_kwargs[\"peft_config\"] = peft_config\n        model = cls(base_model, **wrapped_model_kwargs)\n        if isinstance(pretrained_model_name_or_path, str):\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n            sharded_index_filename = os.path.join(\n                pretrained_model_name_or_path, \"pytorch_model.bin.index.json\""
        },
        {
            "comment": "This code checks if the required model file exists. If not, it attempts to download the sharded PyTorch model using the Hugging Face Hub API. If the download encounters an exception, it looks for the sharded index file instead and extracts the necessary information to download only the required parts of the model. This ensures efficient downloading and handling of large models.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":250-274",
            "content": "            )\n            is_sharded = False\n            if not os.path.exists(filename):\n                try:\n                    filename = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        \"pytorch_model.bin\",\n                        revision=revision,\n                    )\n                # Sharded\n                except Exception:\n                    if os.path.exists(sharded_index_filename):\n                        index_file_name = sharded_index_filename\n                    else:\n                        index_file_name = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            \"pytorch_model.bin.index.json\",\n                            revision=revision,\n                        )\n                    with open(index_file_name, \"r\") as f:\n                        index = json.load(f)\n                    # Collect files containing weights from supported modules\n                    files_to_download = set()\n                    for k, v in index[\"weight_map\"].items():"
        },
        {
            "comment": "This code checks if the model is sharded and downloads or merges shard files into a state dict. If not sharded, it directly loads the state dict from the given path. It also handles the case when pretrained_model_name_or_path is already a state dict itself.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":275-294",
            "content": "                        if any([module in k for module in cls._supported_modules]):\n                            files_to_download.add(v)\n                    is_sharded = True\n            if is_sharded:\n                # Merge each shard into a state dict\n                # TODO: Optimize this to avoid wasting RAM\n                state_dict = {}\n                for shard_file in files_to_download:\n                    filename = os.path.join(pretrained_model_name_or_path, shard_file)\n                    # Download if shard file doesn't exist locally\n                    if not os.path.exists(filename):\n                        filename = hf_hub_download(\n                            pretrained_model_name_or_path, shard_file, revision=revision\n                        )\n                    state_dict.update(torch.load(filename, map_location=\"cpu\"))\n            else:\n                state_dict = torch.load(filename, map_location=\"cpu\")\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()"
        },
        {
            "comment": "This code defines a class with two methods: \"load_linear_layer\" and \"save_pretrained\". The \"load_linear_layer\" method is responsible for loading a linear layer into the model. It also initializes the model with the provided state dictionary. The \"save_pretrained\" method saves the pre-trained model to a directory, using underlying methods from the transformers library. If no state dictionary is provided, it creates one using the model's own state dictionary method and updates the kwargs accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":296-320",
            "content": "        print_with_rank(\"Load Linear Layer\")\n        model.post_init(state_dict=state_dict)\n        return model\n    def save_pretrained(self, *args, **kwargs):\n        \"\"\"Save the pretrained model to a directory. This method is a wrapper\n        around `transformers.PreTrainedModel.save_pretrained`. Please refer to\n        the documentation of `transformers.PreTrainedModel.save_pretrained` for\n        more information.\n        Args:\n            *args (`list`, *optional*):\n                Positional arguments passed along to the underlying model's\n                `save_pretrained` method.\n            **kwargs (`dict`, *optional*):\n                Keyword arguments passed along to the underlying model's\n                `save_pretrained` method.\n        \"\"\"\n        state_dict = kwargs.get(\"state_dict\", None)\n        if state_dict is None:\n            state_dict = self.state_dict()\n            kwargs[\"state_dict\"] = state_dict\n        if self.peft_config is not None:\n            # Save the heads, which are not part of the peft adapter"
        },
        {
            "comment": "Function is part of a class (LoraModelForCausalLM) that inherits from another class (LoraModel). It has methods to save and load model states, as well as post-initialization. The code is for the base model's saving method which calls the save_pretrained function. If a peft_config exists, it uses the underlying transformer model's interface instead of the PEFT one.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":321-342",
            "content": "            os.makedirs(args[0], exist_ok=True)\n            save_path = os.path.join(args[0], \"pytorch_model.bin\")\n            head_state_dict = self.state_dict(heads_only=True)\n            torch.save(head_state_dict, save_path)\n        return self.base_model.save_pretrained(*args, **kwargs)\n    def state_dict(self, *args, **kwargs):\n        \"\"\"Return the state_dict of the pretrained model.\"\"\"\n        raise NotImplementedError\n    def post_init(self, *args, **kwargs):\n        \"\"\"Post initialization method. This method is called after the model is\n        instantiated and loaded from a checkpoint. It can be used to perform\n        additional operations such as loading the state_dict.\n        \"\"\"\n        if self.peft_config is not None:\n            # Don't use the interface of the peft model,\n            # use the interface of the underlying transformer model instead.\n            # (peft adds 2 \"base_model\" layers)\n            # self.base_model LoraModelForCausalLM\n            # self.base_model.base_model LoraModel"
        },
        {
            "comment": "This code retrieves the valid arguments for the `base_model.transformer.forward` function and filters out any unsupported arguments. It's a hack to work around inconsistent APIs in transformers architectures used by the model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_base.py\":343-354",
            "content": "            # self.base_model.base_model.model LlamaModelForCausalLM\n            self.forward_kwargs = inspect.getfullargspec(\n                self.base_model.base_model.model.forward\n            ).args\n    def get_compatible_forward_kwargs(self, **kwargs) -> Dict[str, Any]:\n        \"\"\"Filter out arguments not supported by the specific instance of\n        `base_model.transformer.forward`\n        \"\"\"\n        # FIXME: This is a hack to get around the fact that the `transformers`\n        # architectures we use don't have a consistent API for `forward` parameters.\n        return {k: v for k, v in kwargs.items() if k in self.forward_kwargs}"
        }
    ]
}