{
    "summary": "This code defines MLP heads, extends AutoModel for causal language models, and includes a Transformer-based model with Actor-Critic approach for sequence generation tasks, computing attention and hidden states while optionally caching intermediate results.",
    "details": [
        {
            "comment": "This code defines functions for creating MLP heads and retrieving the hidden size from a HuggingFace transformers config. It also imports necessary modules, defines dataclasses and types, and creates a class that inherits from PreTrainedModelWrapper. The function hf_get_hidden_size handles different model configurations to retrieve their respective hidden layer dimensionality.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":0-28",
            "content": "from collections import OrderedDict\nfrom dataclasses import dataclass\nimport gc\nfrom typing import List, Optional, Tuple, Union\nimport torch\nfrom torch import nn\nimport transformers\nfrom transformers.modeling_outputs import ModelOutput\nfrom tsllm.model.modeling_base import PreTrainedModelWrapper\nfrom tsllm.model.utils import findattr\nfrom peft import PeftConfig\ndef make_head(n_embd: int, out: int, dtype: type = torch.float32) -> nn.Sequential:\n    \"\"\"Returns a generic sequential MLP head.\"\"\"\n    return nn.Sequential(\n        nn.Linear(n_embd, n_embd * 2, dtype=dtype),\n        nn.ReLU(),\n        nn.Linear(n_embd * 2, out, dtype=dtype),\n    )\ndef hf_get_hidden_size(config: transformers.PretrainedConfig) -> int:\n    \"\"\"Returns the hidden layer dimensionality of the model architecture specified\n    by the HuggingFace transformers config.\n    NOTE: Different model configurations have different hidden size attribute names.\n        - hidden_size: (OPTConfig, BloomConfig)\n        - n_embd: (GPT2Config, GPTJConfig)\n        - d_model: (PegasusConfig, XLNetConfig)"
        },
        {
            "comment": "This code defines a class `CausalLMOutputWithValue` and a model wrapper `AutoModelForCausalLMWithValueHead`. The class represents the output of a causal language model, including loss, logits, past key values, hidden states, attentions, cross-attentions, and value. The model wrapper is used to extend transformers' AutoModel for causal language models with an additional value head and supports PEFT configuration.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":29-58",
            "content": "    \"\"\"\n    hidden_size_attrs = (\"hidden_size\", \"n_embd\", \"d_model\")\n    return findattr(config, hidden_size_attrs)\n@dataclass\nclass CausalLMOutputWithValue(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n    value: Optional[torch.FloatTensor] = None\nclass AutoModelForCausalLMWithValueHead(PreTrainedModelWrapper):\n    \"\"\"An `AutoModel` class wrapper for `transformers` causal models that have a\n    language modeling head and a value head\n    \"\"\"\n    _auto_model_parent_class = transformers.AutoModelForCausalLM\n    _supported_modules = [\"v_head\", \"peft_config\"]\n    def __init__(\n        self,\n        base_model: transformers.PreTrainedModel,\n        peft_config: Optional[PeftConfig] = None,\n    ):\n        super().__init__(base_model, peft_config=peft_config)"
        },
        {
            "comment": "The code defines a model class with a `v_head` attribute and a forward method. The forward method takes input arguments such as input_ids, attention_mask, etc., and returns CausalLMOutputWithValue. Use cache is set to False for flash-attn, and the method retrieves compatible kwargs from self.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":59-82",
            "content": "        self.v_head = make_head(hf_get_hidden_size(self.base_model.config), 1)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        position_ids: Optional[List[torch.FloatTensor]] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithValue]:\n        # # set use cache = False to use flash-attn\n        if use_cache is None:\n            use_cache = False\n        forward_kwargs = self.get_compatible_forward_kwargs(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,"
        },
        {
            "comment": "This code snippet is for a Transformer-based model with an Actor-Critic approach, used for sequence generation tasks. It utilizes past key values and input embeddings to compute attention and hidden states. The use_cache parameter determines whether to cache intermediate results for efficiency. Flash-attn is disabled by default. The outputs include logits, attentions, and hidden states (optionally including the value).",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":83-108",
            "content": "            past_key_values=past_key_values,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            labels=labels,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        forward_kwargs[\"output_hidden_states\"] = True\n        forward_kwargs[\"return_dict\"] = True\n        # # set use cache = False to use flash-attn\n        # assert not use_cache, (\"To use flast-attn, you should now use cache\", use_cache)\n        # forward_kwargs[\"use_cache\"] = False\n        # outputs = self.base_model(**forward_kwargs)\n        # value = self.v_head(outputs.hidden_states[-1]).squeeze(-1)\n        # if not return_dict:\n        #     outputs = (outputs.logits,) + outputs[1:] + (value,)\n        #     return outputs\n        # return CausalLMOutputWithValue(**outputs, value=value)\n        forward_kwargs.pop(\"labels\")\n        transformer_outputs = self.base_model.transformer(**forward_kwargs)"
        },
        {
            "comment": "This code defines a model for text generation that uses a transformer architecture. It includes a value head, which outputs the predicted value of each token in the generated sequence. The `generate` method is used to generate sequences from the input prompt. The `state_dict` method returns the state dictionary of the model, including the state dictionary of the value head. The `post_init` method adds the state dictionary of the value head to the state dictionary of the wrapped model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":109-134",
            "content": "        # hidden_states = transformer_outputs[0]\n        value = self.v_head(transformer_outputs.hidden_states[-1]).squeeze(-1)\n        return CausalLMOutputWithValue(value=value)\n    def generate(self, *args, **kwargs) -> Union[ModelOutput, torch.LongTensor]:\n        return self.base_model.generate(*args, **kwargs)\n    def state_dict(self, *args, heads_only=False, **kwargs):\n        \"\"\"\n        Returns the state dictionary of the model. We add the state dictionary of the value head\n        to the state dictionary of the wrapped model by prepending the key with `v_head.`.\n        \"\"\"\n        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)\n        if heads_only:\n            model_state_dict = OrderedDict()\n        else:\n            model_state_dict = self.base_model.state_dict(*args, **kwargs)\n        for k, v in v_head_state_dict.items():\n            model_state_dict[f\"v_head.{k}\"] = v\n        return model_state_dict\n    def post_init(self, state_dict):\n        \"\"\"\n        Adds the state dictionary of the value head to the state dictionary of the wrapped model"
        },
        {
            "comment": "This code snippet defines a class with methods for loading state dictionaries and enabling/disabling gradient checkpointing. It removes the `v_head.` prefix from keys of value head state dictionary, loads the updated state dictionary into v_head, and provides functions to enable or disable gradient checkpointing in the base model.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/model/modeling_actor_critic.py\":135-151",
            "content": "        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the\n        keys of the value head state dictionary.\n        \"\"\"\n        super().post_init()\n        for k in list(state_dict.keys()):\n            if \"v_head.\" in k:\n                state_dict[k.replace(\"v_head.\", \"\")] = state_dict.pop(k)\n        self.v_head.load_state_dict(state_dict, strict=False)\n        del state_dict\n        gc.collect()  # noqa: E702\n    def gradient_checkpointing_enable(self):\n        self.base_model.gradient_checkpointing_enable()\n    def gradient_checkpointing_disable(self):\n        self.base_model.gradient_checkpointing_disable()"
        }
    ]
}