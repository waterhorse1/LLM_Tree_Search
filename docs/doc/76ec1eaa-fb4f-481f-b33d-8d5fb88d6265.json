{
    "summary": "The code introduces `BaseEnv` and \"CoTEnv\" classes for Monte Carlo Tree Search, supporting few-shot learning, environment resetting, managing game states, actions, rewards, termination, and language model-generated text in text-based games. It handles tasks, termination, copying instances, natural language problems at the token level, and includes features for LLM tree search implementation such as action history, instance building checks, and cumulative indices.",
    "details": [
        {
            "comment": "This code defines a base environment class, `BaseEnv`, for Monte Carlo Tree Search (MCTS) in a transformers-based context. It includes abstract methods like `reset`, `step`, and `legal_actions` that must be implemented by derived classes. The class also contains a static method `build_query_str` for wrapping the problem text with certain formats. The code uses various imported modules such as `abc`, `numpy`, `torch`, and `transformers`.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":0-48",
            "content": "import abc\nfrom typing import Dict, List, Optional\nimport numpy as np\nimport copy\nimport pdb\nimport torch\nfrom tsllm.distributed.utils import print_with_rank\nfrom transformers import PreTrainedTokenizer\nINVALID_ANS = \"[invalid]\"\nclass NoLegalActionException(Exception):\n    pass\nclass ResetException(Exception):\n    pass\nclass BaseEnv(abc.ABC):\n    \"\"\"Basic environment to use for MCTS\"\"\"\n    @abc.abstractmethod\n    def reset(self, update_legal_action: bool):\n        raise NotImplementedError\n    @abc.abstractmethod\n    def step(self):\n        raise NotImplementedError\n    @abc.abstractproperty\n    def legal_actions(self):\n        raise NotImplementedError\n    @abc.abstractmethod\n    def copy(self):\n        raise NotImplementedError\n    @staticmethod\n    def build_query_str(\n        cot_task_desc: Optional[str],\n        cot_examples: Optional[str],\n        problem_format_str: str,\n        problem_input: str,\n        sep: str,\n        is_few_shot: bool = False,\n    ):\n        \"\"\"a wrap function that wrap the problem text with certrain format"
        },
        {
            "comment": "This code defines a class \"CoTEnv\" for solving natural language problems using Chain of Thought (CoT) approach. The method \"build_query_str\" is used to generate a query string containing the problem input and steps. The \"build_response_str\" is a static method that builds a response string based on answer and tokenizer, but it raises NotImplementedError as it hasn't been implemented yet.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":49-83",
            "content": "        e.g. prompt_str = \"Input: \" + join_numbers(\" \", xs) + \"\\nSteps:\\n\"\n        >>> query_str = Game24Env.build_query_str(\"1 1 1 1\")\n        >>> print(query_str)\n        >>> Use numbers and basic arithmetic operations (+ - * /) to obtain 24. Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.\n        Input: 1 1 1 1\n        Steps:\n        >>>\n        \"\"\"\n        ret = \"\"\n        if cot_task_desc:\n            ret += cot_task_desc + \"\\n\"\n        if is_few_shot:\n            ret += cot_examples + \"\\n\"\n        ret += problem_format_str.format(question=problem_input)\n        # THIS is because CoTEnv.answer/get_state() append \"\\n\"\n        ret += sep\n        return ret\n    @staticmethod\n    def build_response_str(\n        answer_str: str, tokenizer: PreTrainedTokenizer, add_eos_token: bool\n    ):\n        raise NotImplementedError\nclass CoTEnv(BaseEnv):\n    \"\"\"The basic environment for solving natural language problems using CoT\"\"\"\n    sep: str\n    @staticmethod\n    def build_response_str("
        },
        {
            "comment": "This code defines a base environment class for a task involving text completion using a language model. The `answer_str` method adds an end-of-sentence token if necessary, and the `stop_str` method returns NotImplementedError. The `_is_correct` method also returns NotImplementedError. The `get_reward` method needs to be implemented based on a learned reward model. The class is initialized with config, math problems, llm generation function, tokenizer, task description string, context example string, and problem format string. The `reset` parameter is optional and defaults to True.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":84-117",
            "content": "        answer_str: str, tokenizer: PreTrainedTokenizer, add_eos_token: bool\n    ):\n        if add_eos_token:\n            # if text ends with </s>, remove it so the follwing strip can remove \\n\n            #  and in latter add_traj, \\n and </s> will be added again\n            if answer_str.endswith(tokenizer.eos_token):\n                answer_str = answer_str.replace(tokenizer.eos_token, \"\")\n            answer_str = answer_str.strip()\n            answer_str += tokenizer.eos_token\n        return answer_str\n    @property\n    def stop_str(self):\n        return NotImplementedError\n    def _is_correct(self, completion) -> bool:\n        raise NotImplementedError\n    def get_reward(self):\n        \"\"\"To implement based on learned reward model\"\"\"\n        raise NotImplementedError\n    def __init__(\n        self,\n        config,\n        math_problems,\n        llm_gen_fn,\n        tokenizer,\n        task_desc_str: str,\n        cot_example_str: str,\n        problem_format_str: str,\n        reset=True,\n    ):\n        self.config = config"
        },
        {
            "comment": "This code initializes a base environment for the LLM tree search algorithm. It sets attributes like mcts_mode, math_problems, and llm_gen_fn. It also checks if it's a few-shot learning scenario and generates task prefixes to be used. The reset() function resets the environment to problem index 0 and initializes the action history.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":118-147",
            "content": "        self.mcts_mode = \"play_with_bot_mode\"\n        self.math_problems = math_problems\n        self.llm_gen_fn = llm_gen_fn\n        self.tokenizer = tokenizer\n        self.action_history = None\n        self.math_problem = None\n        self._legal_actions = None\n        self.is_few_shot = config.get(\"is_few_shot\", False)\n        self._task_desc_str = task_desc_str\n        self._cot_example_str = cot_example_str\n        self._problem_format_str = problem_format_str\n        prefixes = []\n        if self._task_desc_str is not None:\n            prefixes.append(self._task_desc_str)\n        if self.is_few_shot:\n            prefixes.append(self._cot_example_str)\n        if len(prefixes) > 0:\n            self.task_prefix = \"\\n\".join(prefixes)\n        else:\n            self.task_prefix = None\n        if reset:\n            self.reset(update_legal_action=True)\n    def reset(self, update_legal_action=True):\n        # reset environment to problem idx\n        self.set_problem(idx=0)\n        self.action_history = self.init_action_history()"
        },
        {
            "comment": "The code defines a base environment for the game and provides methods to update legal actions, get state, reward, and done status. If there are no legal actions left after a step, the game is terminated and reward set to 0. If not terminated or truncated, it updates legal actions. The \"info\" dictionary includes the winner if the game ended due to no legal actions.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":148-175",
            "content": "        if update_legal_action:\n            cnt = 0\n            while cnt < 3:\n                cnt += 1\n                try:\n                    self._legal_actions = self.update_legal_actions()\n                    break\n                except NoLegalActionException as e:\n                    if cnt == 3:\n                        raise ResetException\n        return self.get_state()\n    def step(self, action, update_legal_action=True):\n        self.action_history.append(action)\n        state = self.get_state()\n        reward = self.get_reward()\n        terminated, truncated, info = self.get_done_and_info()\n        # update legal actions\n        if not (terminated or truncated) and update_legal_action:\n            try:\n                self._legal_actions = self.update_legal_actions()\n            except NoLegalActionException as e:\n                terminated = True\n                reward = 0\n                self._legal_actions = None\n                info[\"winner\"] = 2\n        else:\n            self._legal_actions = None"
        },
        {
            "comment": "In this code snippet, we have a base_env class with methods for handling game states and actions. It checks if the player won the game, returns the current state, initializes the action history, and updates legal actions based on the action history. The reward is assigned if the player wins.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":176-201",
            "content": "            if info[\"winner\"] == 1:\n                reward = 1.0\n        return state, reward, terminated, truncated, info\n    def get_state(self):\n        return \"\\n\".join(self.action_history) + \"\\n\"\n    def init_action_history(self):\n        # add the first prompted questions\n        return ([self.task_prefix] if self.task_prefix is not None else []) + [\n            # f\"Question: {self.math_problem['question']}\\nAnswer: Let's think step by step\"\n            self._problem_format_str.format(question=self.math_problem[\"question\"])\n        ]\n    def update_legal_actions(self):\n        def reduce_prob_list(prob_list: List[List]) -> List:\n            ans_list = []\n            for scores in prob_list:\n                ans_list.append(np.exp(np.mean(scores)))\n            return ans_list\n        prefix = (\n            (self.action_history[0] + \"\\n\") if self.task_prefix is not None else None\n        )\n        act_hist_start_i = 0 if self.task_prefix is None else 1\n        unprefixed_state = \"\\n\".join(self.action_history[act_hist_start_i:]) + \"\\n\""
        },
        {
            "comment": "This code is using a language model to generate text based on a prefix and an unprefixed state. It checks for valid actions and raises an exception if none are generated. The code then normalizes the generated probabilities, removes special tokens like BOS/EOS, and stores the results in lists for later use.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":202-229",
            "content": "        texts, logps = self.llm_gen_fn(\n            static_prompt=prefix,\n            prompt=unprefixed_state,\n            num_sequence=self.config[\"max_actions\"],\n            stop=[13, self.tokenizer.eos_token_id],\n            **self.config[\"generation_config\"],\n        )\n        text_list, prob_list = [], []\n        for i in range(len(texts)):\n            if len(texts[i]) > 0 and texts[i] not in text_list:\n                text_list.append(texts[i])\n                prob_list.append(logps[i])\n        if len(prob_list) == 0:\n            print_with_rank(\n                \"{} {} {}\".format(prefix, act_hist_start_i, unprefixed_state)\n            )\n            raise NoLegalActionException(\"No possible action have been generated.\")\n        prob_list = reduce_prob_list(prob_list)\n        prob_list = np.array(prob_list)\n        # normalize probability\n        prob_list = prob_list / np.sum(prob_list)\n        # set add special tokens as False to remove bos/eos tokens\n        num_token_list = [\n            len(self.tokenizer.encode(txt, add_special_tokens=False))"
        },
        {
            "comment": "This code defines a base environment class for a text-based game. It sets the legal actions based on input texts, allows setting and getting problems, and provides properties to get the question and answer in different formats depending on whether there is a task prefix or not. The done status depends on reaching maximum length or encountering stop words, and the environment can also be truncated if it exceeds the maximum length set in the configuration.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":230-263",
            "content": "            for txt in text_list\n        ]\n        _legal_actions = [\n            {\"action\": action, \"prob\": prob, \"num_token\": n_token}\n            for action, prob, n_token in zip(text_list, prob_list, num_token_list)\n        ]\n        return _legal_actions\n    def set_problem(self, idx):\n        self.math_problem = self.math_problems[idx]\n    @property\n    def question(self):\n        return (\n            \"\\n\".join(self.action_history[:1]) + \"\\n\"\n            if self.task_prefix is None\n            else \"\\n\".join(self.action_history[:2]) + \"\\n\"\n        )\n    @property\n    def answer(self):\n        return (\n            \"\\n\".join(self.action_history[1:]) + \"\\n\"\n            if self.task_prefix is None\n            else \"\\n\".join(self.action_history[2:]) + \"\\n\"\n        )\n    def get_done_and_info(self):\n        info = {\"winner\": 0}\n        # done when reaches maximum length or LLM generates stop words\n        terminated = self.stop_str in self.action_history[-1]\n        truncated = len(self.action_history) >= self.config[\"max_length\"] + ("
        },
        {
            "comment": "The code is part of a class-based environment, responsible for setting up and running tasks. It checks the length of action history and terminates/truncates the task if necessary. It also includes a copy method that creates an identical instance of the environment without resetting it.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":264-293",
            "content": "            2 if self.task_prefix is not None else 1\n        )\n        assert len(self.action_history) <= self.config[\"max_length\"] + (\n            2 if self.task_prefix is not None else 1\n        ), \"action history length: {}, max length: {}\".format(\n            len(self.action_history),\n            self.config[\"max_length\"] + (2 if self.task_prefix is not None else 1),\n        )\n        if terminated or truncated:\n            if self._is_correct(self.action_history[-1]):\n                info[\"winner\"] = 1\n            else:\n                info[\"winner\"] = 2\n            return terminated, truncated, info\n        return terminated, truncated, info\n    def copy(self):\n        env = self.__class__(\n            self.config,\n            self.math_problems,\n            self.llm_gen_fn,\n            self.tokenizer,\n            self._task_desc_str,\n            self._cot_example_str,\n            self._problem_format_str,\n            reset=False,\n        )\n        env.math_problem = copy.deepcopy(self.math_problem)\n        env._legal_actions = copy.deepcopy(self._legal_actions)"
        },
        {
            "comment": "This code defines a TokenEnv class, a subclass of BaseEnv, for solving natural language problems at the token level. It overrides methods for determining legal actions, checking correctness, and getting rewards. The class takes in configuration, problems, LLM forward function, tokenizer, task description, example, and problem format string.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":294-334",
            "content": "        env.action_history = copy.deepcopy(self.action_history)\n        return env\n    @property\n    def legal_actions(self):\n        return self._legal_actions\nclass TokenEnv(BaseEnv):\n    \"\"\"The Token-level environment for solving natural language problems\"\"\"\n    sep: str\n    @property\n    def stop_str(self):\n        raise NotImplementedError\n    def _is_correct(self, completion) -> bool:\n        raise NotImplementedError\n    def get_reward(self, state):\n        \"\"\"To implement based on learned reward model\"\"\"\n        raise NotImplementedError\n    def __init__(\n        self,\n        config,\n        problems,\n        llm_forward_fn,\n        tokenizer,\n        task_desc_str: str,\n        cot_example_str: str,\n        problem_format_str: str,\n        reset=True,\n    ):\n        self.config = config\n        # do not use sep in config, but use sep defined in each env.prompt.SEP\n        # self.sep = config[\"sep\"]\n        self.mcts_mode = \"play_with_bot_mode\"\n        self.problems = problems\n        self.llm_forward_fn = llm_forward_fn"
        },
        {
            "comment": "The code initializes the environment for a task and resetting it. It sets up the tokenizer, action history, problem, legal actions, few-shot flag, and task prefix based on provided parameters. The `reset` function resets the environment to its initial state by setting the problem index, updating the action history, and updating the legal actions if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":335-364",
            "content": "        self.tokenizer = tokenizer\n        self.action_history = None\n        self.problem = None\n        self._legal_actions = None\n        self.is_few_shot = config.get(\"is_few_shot\", False)\n        self._task_desc_str = task_desc_str\n        self._cot_example_str = cot_example_str\n        self._problem_format_str = problem_format_str\n        prefixes = []\n        if self._task_desc_str is not None:\n            prefixes.append(self._task_desc_str)\n        if self.is_few_shot:\n            prefixes.append(self._cot_example_str)\n        if len(prefixes) > 0:\n            self.task_prefix = \"\\n\".join(prefixes)\n        else:\n            self.task_prefix = None\n        if reset:\n            self.reset(update_legal_action=True)\n    def reset(self, update_legal_action=True):\n        # reset environment to problem idx\n        self.set_problem(idx=0)\n        self.action_history = self.init_action_history()\n        if update_legal_action:\n            self._legal_actions = self.update_legal_actions()\n        return self.get_state()"
        },
        {
            "comment": "This code defines a step method in an environment class, which takes an action and updates legal actions if the episode is not terminated or truncated. It retrieves the state, reward, termination, truncation, and info from the environment and returns them as part of the state transition. The sep_index property calculates the separation index between task prefix (if provided) and subsequent actions.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":366-393",
            "content": "    def step(self, action, update_legal_action=True):\n        if not action == self.stop_str:\n            self.action_history.append(action)\n        state = self.get_state()\n        reward = self.get_reward(state)\n        terminated, truncated, info = self.get_done_and_info()\n        # update legal actions\n        if not (terminated or truncated) and update_legal_action:\n            self._legal_actions = self.update_legal_actions()\n        else:\n            self._legal_actions = None\n        return state, reward, terminated, truncated, info\n    @property\n    def sep_index(self):\n        pre_state = (\n            self.action_history[:1]\n            if self.task_prefix is None\n            else self.action_history[:2]\n        )\n        pre_state_token_length = len(self.tokenizer.encode([pre_state + self.sep]))\n        index = [pre_state_token_length]\n        post_state = (\n            self.action_history[1:]\n            if self.task_prefix is None\n            else self.action_history[2:]\n        )\n        for action in post_state:"
        },
        {
            "comment": "This code initializes an action history, gets the current state, and calculates action lengths to build online value instances. It also checks for possible problems in the instance building process and asserts that the sum of action lengths matches the length of the encoded state. Finally, it returns cumulative indices for the action lengths minus one.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":394-419",
            "content": "            action_length = len(\n                self.tokenizer.encode(action + self.sep, add_special_tokens=False)\n            )\n            index.append(action_length)\n            if action_length == 0:\n                print_with_rank(\n                    \"possbile problems met in online value instance building. {}\".format(\n                        action\n                    )\n                )\n        assert sum(index) == len(self.tokenizer.encode(self.get_state()))\n        index = np.cumsum(index) - 1\n        return index\n    def get_state(self):\n        # if self.action_history[-1] == self.stop_str:# remove the final stop token\n        #    return \"\".join(self.action_history[:-1])\n        return self.sep.join(self.action_history)\n    def init_action_history(self):\n        # add the first prompted questions\n        return ([self.task_prefix] if self.task_prefix is not None else []) + [\n            self._problem_format_str.format(question=self.problem[\"question\"])\n        ]\n    def update_legal_actions(self):"
        },
        {
            "comment": "This code is from the base_env.py file in a LLM tree search implementation. It includes methods for getting state, calculating probabilities, and setting/retrieving problem information. The class also provides a question property based on action history.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":420-451",
            "content": "        state = self.get_state()\n        logits = self.llm_forward_fn(prompt=state)[0]\n        probs = torch.nn.functional.softmax(logits / self.config[\"temperature\"], dim=-1)\n        topk_values, topk_indices = torch.topk(probs, self.config[\"max_actions\"])\n        text_list = self.tokenizer.batch_decode(\n            topk_indices.reshape(topk_indices.shape[-1], 1)\n        )\n        prob_list = topk_values.tolist()\n        prob_list = prob_list / np.sum(prob_list)\n        _legal_actions = [\n            {\n                \"action\": action,\n                \"prob\": prob,\n                \"num_token\": 1 / self.config[\"max_actions\"],\n            }\n            for action, prob in zip(text_list, prob_list)\n        ]\n        return _legal_actions\n    def set_problem(self, idx):\n        self.problem = self.problems[idx]\n    @property\n    def question(self):\n        return (\n            \"\\n\".join(self.action_history[:1])\n            if self.task_prefix is None\n            else \"\\n\".join(self.action_history[:2])\n        )\n    @property"
        },
        {
            "comment": "The code defines a class with methods for generating responses, determining done and truncated statuses, and cloning the environment. The response is generated based on action history, separator, and task prefix. Done status is determined when stop string matches the last action or if it reaches maximum length. Truncated status is determined when it exceeds the max length with additional characters for task prefix. Maximum length is asserted to prevent exceeding limits. The environment can be cloned using the copy method.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":452-480",
            "content": "    def answer(self):\n        return (\n            self.sep.join(self.action_history[1:])\n            if self.task_prefix is None\n            else self.sep.join(self.action_history[2:])\n        )\n    def get_done_and_info(self):\n        info = {\"winner\": 0}\n        # done when reaches maximum length or LLM generates stop words\n        terminated = self.stop_str == self.action_history[-1]\n        truncated = len(self.action_history) >= self.config[\"max_length\"] + (\n            2 if self.task_prefix is not None else 1\n        )\n        assert len(self.action_history) <= self.config[\"max_length\"] + (\n            2 if self.task_prefix is not None else 1\n        ), \"action history length: {}, max length: {}\".format(\n            len(self.action_history),\n            self.config[\"max_length\"] + (2 if self.task_prefix is not None else 1),\n        )\n        return terminated, truncated, info\n    def copy(self):\n        env = self.__class__(\n            self.config,\n            self.problems,\n            self.llm_forward_fn,"
        },
        {
            "comment": "This code is creating a new environment object by setting various attributes such as tokenizer, task description, and problem format. It then deep copies the problem, legal actions, and action history to the new environment and returns it.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/base_env.py\":481-494",
            "content": "            self.tokenizer,\n            self._task_desc_str,\n            self._cot_example_str,\n            self._problem_format_str,\n            reset=False,\n        )\n        env.problem = copy.deepcopy(self.problem)\n        env._legal_actions = copy.deepcopy(self._legal_actions)\n        env.action_history = copy.deepcopy(self.action_history)\n        return env\n    @property\n    def legal_actions(self):\n        return self._legal_actions"
        }
    ]
}