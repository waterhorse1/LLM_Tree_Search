{
    "summary": "The code initializes modules, retrieves the GSM8k dataset, builds a few-shot learning task dataset, and tests the model by creating an instance of GSMLLMTreeSearch. It prints dataset information and lengths of encoded text data for analysis.",
    "details": [
        {
            "comment": "The code imports necessary modules and defines the environment for a math problem game. It creates an instance of Gsm8kEnv with a given math problem, resets the game state, checks correct answers, and prints the current game state. It also demonstrates zero-shot contextualized ordinal regression task description using provided functions.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_gsm8k.py\":0-38",
            "content": "from tsllm.envs.gsm8k.env import (\n    Gsm8kEnv,\n    COT_EXAMPLES,\n    COT_TASK_DESC,\n    PROBLEM_FORMAT_STR,\n    SEP,\n)\nimport pytest\nif __name__ == \"__main__\":\n    problem_input = \"1 3 3 4\"\n    env = Gsm8kEnv(\n        config={},\n        math_problems=[{\"question\": \"1 3 3 4\", \"answer\": \"3\"}],\n        tokenizer=None,\n        llm_gen_fn=None,\n        reset=False,\n    )\n    env.reset(False)\n    print(env.get_state())\n    print(env._is_correct(\"The answer is 3\"))\n    print(env._is_correct(\"\\n\\nThe answer is 3.\"))\n    print(env._is_correct(\"The answer is 4\"))\n    print(env._is_correct(\"The answer is x\"))\n    build_query_str = Gsm8kEnv.build_query_str\n    print(\"\\n\\n====== ZERO SHOT COT ============\")\n    print(\n        build_query_str(\n            COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, False\n        )\n    )\n    # print(\"\\n\\n====== FEW SHOT COT ============\")\n    # print(\n    #     build_query_str(\n    #         COT_TASK_DESC, COT_EXAMPLES, PROBLEM_FORMAT_STR, problem_input, SEP, True\n    #     )"
        },
        {
            "comment": "This code initializes a tokenizer, retrieves the training dataset for an 8k General Science Multitask (GSMT) dataset, creates a dictionary mapping questions to indices, builds a few-shot learning task dataset using the GSM8K dataset, and prints information about the datasets.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_gsm8k.py\":39-67",
            "content": "    # )\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    print(\"\\n\\n====== default sft dataset ============\")\n    from tsllm.envs import get_default_sft_data_builder, get_env_datasets\n    train_ds, _ = get_env_datasets(\"gsm8k\")\n    q2idx_dict = {}\n    for idx, problem_inst in enumerate(train_ds):\n        question = problem_inst[\"question\"]\n        q2idx_dict[question] = idx\n    sft_data = get_default_sft_data_builder(\"gsm8k\")(\n        \"tsllm/envs/gsm8k/train_data/sft_init.jsonl\",\n        q2idx_dict,\n        tokenizer=tokenizer,\n        add_eos_token=True,\n        is_few_shot=False,\n    )\n    print(\"Len train_ds: {}\\ntrian_ds[0]:\\n{}\".format(len(train_ds), train_ds[0]))\n    print(\"Len sft_data: {}\\nsft_data[0]:\\n{}\".format(len(sft_data), sft_data[0]))\n    print(\"\\n\\n====== default critic dataset ============\")\n    from tsllm.envs import get_default_critic_data_builder\n    critic_data = get_default_critic_data_builder(\"gsm8k\")(\n        \"tsllm/envs/gsm8k/train_data/sft_init.jsonl\","
        },
        {
            "comment": "This code tests the GSM8k dataset by creating an instance of the GSMLLMTreeSearch model, setting tokenizer and few-shot parameters to False, then prints various lengths related to encoded text data for further analysis.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/tests/test_gsm8k.py\":68-78",
            "content": "        q2idx_dict,\n        tokenizer=tokenizer,\n        is_few_shot=False,\n    )\n    print(\n        \"Len critic_data: {}\\ncritic_data[0]:\\n{}\".format(\n            len(critic_data), critic_data[0]\n        )\n    )\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"] + critic_data[0][\"answer\"])))\n    print(len(tokenizer.encode(critic_data[0][\"query_str\"])))"
        }
    ]
}