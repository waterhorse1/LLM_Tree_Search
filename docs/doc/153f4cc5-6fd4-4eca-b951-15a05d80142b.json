{
    "summary": "The TS_LLM framework accelerates language model inference through AlphaZero-like techniques, offers training instructions for various tasks, and utilizes SFT or pretrained models. It also provides guidance on testing, iterative updates, and emphasizes proper usage of SearchArguments.",
    "details": [
        {
            "comment": "TS_LLM is an AlphaZero-like tree search learning framework for large language models (LLMs). It's based on a paper titled \"Alphazero-like Tree-Search can guide large language model decoding and training.\" The code requires specific versions of transformers and Ctranslate2, with installation instructions provided. The framework accelerates LLM inference by converting HuggingFace models to Ctranslate2 using bfloat16 quantization for LLaMA or float32 for GPT2.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/README.md\":0-25",
            "content": "# TS_LLM: AlphaZero-like tree-search learning framework for LLMs \nThe official implementation of paper: [Alphazero-like Tree-Search can guide large language model decoding and training](https://arxiv.org/pdf/2309.17179.pdf)\n# Enviroment Installation\nplease use correct version of `transformers` and `ctranlate2`\n```\nconda create -n tsllm python==3.10\nconda activate tsllm\npip install -r requirement.txt\npip install -e .\n```\n# Runnable Scripts\nWe show examples of one task, other tasks are similar and we provide the corresponding scripts.\n## Start\nWe use [Ctranslate2(3.17.1)](https://github.com/OpenNMT/CTranslate2) to speedup LLM inference, which is implemented in C++ and much faster than python huggingface. To use Ctranslate2, you need first transform your LLM model, here is an example:\n```bash\nct2-transformers-converter --model {your huggingface model path} --quantization bfloat16 --output_dir {your ct2_cache target path}\n# We use bfloat 16 for LLaMA model and float32 for GPT2 model\n```\nNote that we use Ctr"
        },
        {
            "comment": "The code outlines the training process for value and policy inference using a specific model, ct2. It provides instructions to train the model on tasks such as GSM8K, Game24, and ProntoQA. The code also mentions that SFT (text-conditioned finetuning) is used for some tasks, while others utilize VicGalle's pretrained model directly. Furthermore, it describes the data collection process for value training and specifies using three checkpoints to collect rollout data for GSM8K and Game24.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/README.md\":25-46",
            "content": "anslate2 for all policy inference, so for any policy in our codebase, do not forget to convert to ct2 model first.\n## Training of Value and Policy\nExamples are shown in `tran_mcts_scripts`, use GSM8k as example\n```bash\ncd train_mcts_scripts/gsm8k\n# SFT for GSM8K, Game24 and ProntoQA\n# Note that For RLHF we do not conduct SFT training, we directly utilize vicgalle/gpt2-open-instruct-v1.\naccelerate launch --config_file mcts_gsm8k_llama_deepspeed.yaml train_gsm8k_sft.py \n# Critic training for all four tasks, data is collected by data collection section.\naccelerate launch --config_file mcts_gsm8k_llama_deepspeed.yaml train_gsm8k_critic.py\n```\nYou can customize `config` in each py files, e.g. `config[\"train\"][\"checkpoint_dir\"]` and `config[\"train\"][\"project_name\"]`. (we use accelerate so we also provide the accelerate config in `accelerate_config.yaml`)\n## Data Collection for Value Training\nExamples are shown in `tsllm/offline_rl`, use GSM8k as example \n```bash\ncd tsllm/offline_rl\n# please check the scripts, for gsm8k and game24, we use 3 checkpoints to rollout data"
        },
        {
            "comment": "The code provides instructions for generating and processing datasets for GSM8K, Game24, and ProtoQA using specific scripts. The testing of policy models and value functions is done using `tsllm/offline_rl/test_sft_and_v.py`. Four test settings (`TEST_NO_TERMINAL`, `TEST_WITH_TERMINAL`, `TEST_COT_GREEDY`, `TEST_COT_SC`) are controlled by environment variables, and search arguments are set in the Python script as elements in `arg_list`.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/README.md\":47-61",
            "content": "# which is named as ${CT2_CACHE}/llama2_sft_ep${i}_ct2\nsh gsm8k_data/gen_3.sh {your ct2 transformed path} {your model tokenizer path} # This is for dataset generation\nsh gsm8k_data/process.sh # This is for dataset processing\n```\n## Testing over CoT, CoT-SC and TS-LLM\nFor GSM8K, Game24, ProtoQA, you should use `tsllm/offline_rl/test_sft_and_v.py` to test the policy model and value function.\nTo run the tests, you should know 2 key concepts used in the code, the first one is 4 test settings, which is controlled by setting environment variables; the other is search arguments, which is set in `tsllm/offline_rl/test_sft_and_v.py` as elements in `arg_list`\nThere are four types of test setting:\n- `TEST_NO_TERMINAL` is MCTS/other tree search methods in GSM8K/ProntoQA/Game24 (we assume we do not know the final reward in these 3 tasks)\n- `TEST_WITH_TERMINAL` is MCTS/other tree search methods in RLHF (we assume we know the final reward in RLHF)\n- `TEST_COT_GREEDY` is CoT greedy decoding\n- `TEST_COT_SC` is CoT-SC"
        },
        {
            "comment": "This code explains how to test and run a language model's policy and value functions using tree search and CoT-SC methods. It provides examples of scripts and instructions for both general and RLHF environments, with emphasis on proper usage of SearchArguments and setting TEST_WITH_TERMINAL for the reward function in RLHF setting. Additionally, it offers guidance on how to perform iterative updates using provided instructions in specific folders.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/README.md\":63-81",
            "content": "There are several args to control CoT-SC and Tree Search methods, see `tsllm/offline_rl/test_sft_and_v.py::SearchArgs` for more information.\nTo run the test, using the following scripts (examples in `train_mcts_scripts/gsm8k/test_policy_and_value.sh`)\n```\ncd train_mcts_scripts/gsm8k/\nsh test_policy_and_value_sh {your save dir}\n```\n**Please make sure the SearchArguments you are using are correct, e.g. check `\"max_action\"`, `\"max_length\"`, etc.**\nFor RLHF environment, it is basically similar except that you should use `tsllm/offline_rl/test_sft_and_v_rlhf.py`. We assume we have a reward function in RLHF setting, so you shoud set `TEST_WITH_TERMINAL=1` for rlhf experiment. \nThere are several args to control CoT-SC and Tree Search methods, see `tsllm/offline_rl/test_sft_and_v_rlhf.py::SearchArgs` for more information.\nTo run the test, we provide an example in `train_mcts_scripts/rlhf/test_policy_and_value.sh`.\n## Iterative Update\nFor iterative update, please refer to `train_mcts_scripts/gsm8k` and `train_mcts_scripts/rlhf` for more instructions."
        },
        {
            "comment": "This code provides a citation for the repo and acknowledges the use of code from lightzero repository.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/README.md\":83-96",
            "content": "## Citation\nIf you find our repo useful, please cite it in your publications.\n```bibtex\n@article{feng2023alphazero,\n  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},\n  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},\n  journal={arXiv preprint arXiv:2309.17179},\n  year={2023}\n}\n```\n## Acknowledgement\nOur code implementation refers to code from [lightzero](https://github.com/opendilab/LightZero)."
        }
    ]
}