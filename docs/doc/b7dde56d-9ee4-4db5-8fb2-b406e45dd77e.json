{
    "summary": "The code defines a reinforcement learning environment class, extending TokenEnv, for language model tasks. It initializes with parameters and includes properties for actions, termination conditions, updates, reward calculation, and environment copying.",
    "details": [
        {
            "comment": "This code defines a class called RLHF_TokenEnv that extends the TokenEnv class. It initializes an object with various parameters such as config, problems, llm_forward_fn, tokenizer, reward_fn, task_desc_str, cot_example_str, problem_format_str, and reset. The class also defines a static method build_response_str that takes in answer_str, tokenizer, and add_eos_token as parameters. This method adds the eos_token to the answer if the answer length is less than 64 and add_eos_token is True.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/rlhf/env.py\":0-42",
            "content": "import copy\nimport re\nfrom typing import List\nimport numpy as np\nfrom transformers import PreTrainedTokenizer\nfrom tsllm.envs.base_env import TokenEnv\nfrom .prompt import PROBLEM_FORMAT_STR, SEP\nclass RLHF_TokenEnv(TokenEnv):\n    sep=SEP\n    def __init__(\n        self,\n        config,\n        problems,\n        llm_forward_fn,\n        tokenizer,\n        reward_fn,\n        task_desc_str= None,\n        cot_example_str = None,\n        problem_format_str = PROBLEM_FORMAT_STR,\n        reset=True,\n    ):\n        super().__init__(\n            config,\n            problems,\n            llm_forward_fn,\n            tokenizer,\n            task_desc_str,\n            cot_example_str,\n            problem_format_str,\n            reset,\n        )\n        self.reward_fn = reward_fn\n    @staticmethod\n    def build_response_str(\n        answer_str: str, tokenizer: PreTrainedTokenizer, add_eos_token: bool\n    ):\n        if (\n            add_eos_token\n            and len(tokenizer.encode(answer_str, add_special_tokens=False)) < 64\n        ):\n            answer_str += tokenizer.eos_token"
        },
        {
            "comment": "The code defines a class with properties stop_str and step, which is used in a reinforcement learning environment. The stop_str returns the end of sequence token, and the step function handles actions taken by the environment, termination conditions, and updates legal actions. It also retrieves the state and reward based on the action taken.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/rlhf/env.py\":43-71",
            "content": "        return answer_str\n    @property\n    def stop_str(self):\n        return self.tokenizer.eos_token\n    # def init_action_history(self):\n    #     # add the first prompted questions\n    #     return ([self.task_prefix] if self.task_prefix is not None else []) + [\n    #         self._problem_format_str.format(self.problem['prompt'])\n    #     ]\n    def step(self, action, update_legal_action=True):\n        terminated = False\n        if not self.stop_str == action:\n            # remove the final stop string like eos token\n            self.action_history.append(action)\n        else:\n            terminated = True\n        state = self.get_state()\n        truncated = len(self.action_history) >= self.config[\"max_length\"] + (\n            2 if self.task_prefix is not None else 1\n        )\n        reward = self.get_reward(terminated, truncated)\n        # update legal actions\n        if not (terminated or truncated) and update_legal_action:\n            self._legal_actions = self.update_legal_actions()\n        else:\n            self._legal_actions = None"
        },
        {
            "comment": "The code defines an environment (env.py) for a reinforcement learning task involving language models. It returns state, reward, termination, and truncation status, along with additional information. The get_reward function calculates the reward based on a learned reward model, while the copy function creates a copy of the environment with the same configuration and current state.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/envs/rlhf/env.py\":72-101",
            "content": "        return state, reward, terminated, truncated, {'winner': None, 'reward': reward}\n    def get_reward(self, terminated, truncated):\n        \"\"\"To implement based on learned reward model\"\"\"\n        if terminated or truncated:\n            reward = self.reward_fn(self.question, self.answer)\n        else:\n            reward = 0\n        return reward\n    def copy(self):\n        env = self.__class__(\n            self.config,\n            self.problems,\n            self.llm_forward_fn,\n            self.tokenizer,\n            self.reward_fn,\n            self._task_desc_str,\n            self._cot_example_str,\n            self._problem_format_str,\n            reset=False,\n        )\n        env.problem = copy.deepcopy(self.problem)\n        env._legal_actions = copy.deepcopy(self._legal_actions)\n        env.action_history = copy.deepcopy(self.action_history)\n        return env\nif \"__name__\" == '__main__':\n    pass"
        }
    ]
}