{
    "summary": "This code defines two functions: one flattens a dictionary recursively and another retrieves the latest git commit details. The first function also returns distributed configuration when DeepSpeed plugin is used, including gradient accumulation steps and clipping.",
    "details": [
        {
            "comment": "This code defines two functions. The first function `flatten_dict` takes a dictionary and recursively flattens it, returning a new dictionary with all keys separated by \"/\". The second function `get_distributed_config` returns a dictionary containing distributed configuration based on the accelerator settings. If DeepSpeed plugin is used, additional configuration such as gradient accumulation steps and gradient clipping are included in the returned dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/utils.py\":0-37",
            "content": "import subprocess\nfrom typing import Dict, MutableMapping, Tuple, Union\nimport accelerate\ndef flatten_dict(\n    d: Union[dict, MutableMapping],\n    parent_key: str = \"\",\n    sep: str = \"/\",\n) -> dict:\n    # From: https://stackoverflow.com/a/6027615\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\ndef get_distributed_config(accelerator: accelerate.Accelerator):\n    \"\"\"\n    Return accelerator distributed config\n    \"\"\"\n    dist_config = {\n        \"mixed_precision\": accelerator.mixed_precision,\n        \"num_gpus\": accelerator.num_processes,\n    }\n    if accelerator.state.deepspeed_plugin is not None:\n        ds_plugin = accelerator.state.deepspeed_plugin\n        dist_config.update(\n            {\n                \"gradient_accumulation_steps\": ds_plugin.gradient_accumulation_steps,\n                \"gradient_clipping\": ds_plugin.gradient_clipping,"
        },
        {
            "comment": "This code defines two functions: `get_dist_config()` and `get_git_tag()`. The first function sets various parameters for distributed learning and returns them in a dictionary. The second function uses subprocess to retrieve the short hash and date of the most recent commit in git.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/utils.py\":38-56",
            "content": "                \"zero_stage\": ds_plugin.zero_stage,\n                \"offload_optimizer_device\": ds_plugin.offload_optimizer_device,\n                \"offload_param_device\": ds_plugin.offload_param_device,\n            }\n        )\n    return dist_config\ndef get_git_tag() -> Tuple[str, str]:\n    \"\"\"\n    Returns commit's short hash and date\n    \"\"\"\n    try:\n        output = subprocess.check_output(\"git log --format='%h/%as' -n1\".split())\n        branch = subprocess.check_output(\"git rev-parse --abbrev-ref HEAD\".split())\n        return branch.decode()[:-1], output.decode()[1:-2]\n    except subprocess.CalledProcessError:\n        return \"unknown\", \"unknown\""
        }
    ]
}