{
    "summary": "The code snippet includes an abstract class for MCTS-based model training and three methods: get_arch, setup_optimizer, and another unnamed method. The get_arch method returns a model architecture based on the configuration and handles backward compatibility. The setup_optimizer method returns an optimizer based on the trainer's TRLConfig. If a model path is specified, it loads a pre-existing model instead of initializing a new one.",
    "details": [
        {
            "comment": "BaseMCTSTrainer is an abstract class that serves as the base for training a model using Monte Carlo Tree Search (MCTS) algorithm. It initializes the Accelerator object with gradient accumulation steps, DeepSpeed plugin configuration, and micro-batch size per GPU. If the world size is greater than 1, it handles distributed training settings.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":0-31",
            "content": "from abc import abstractmethod, ABC\nfrom ast import List\nfrom functools import partial\nimport json\nimport os\nimport sys\nfrom typing import Callable, Iterable, Optional\nfrom accelerate import Accelerator\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nfrom tsllm.distributed.utils import print_with_rank\nfrom tsllm.rl.trainer.opt_utils import get_optimizer_class\nfrom tsllm.rl.trainer.utils import flatten_dict, get_distributed_config, get_git_tag\nclass BaseMCTSTrainer(ABC):\n    def __init__(self, config):\n        self.config = config\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=config.train.gradient_accumulation_steps,\n            log_with=config.train.tracker,\n            project_dir=config.train.logging_dir,\n            split_batches=True,\n        )\n        self.accelerator.state.deepspeed_plugin.deepspeed_config[\n            \"train_micro_batch_size_per_gpu\"\n        ] = config.train.micro_batch_size\n        if int(os.environ.get(\"WORLD_SIZE\", 1)) > 1:"
        },
        {
            "comment": "The code sets up the environment for distributed training, initializes a tokenizer, and defines an abstract method `learn` to be implemented by derived classes. It also introduces a property `unwrap_model` that returns the unwrapped model using the accelerator's `unwrap_model` function. The code appears to be part of a machine learning model trainer.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":32-56",
            "content": "            torch.distributed.barrier(device_ids=[int(os.environ.get(\"LOCAL_RANK\", 0))])\n        self.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            config.tokenizer.tokenizer_path, cache_dir=self.config.model.cache_dir\n        )\n        self.tokenizer.padding_side = config.tokenizer.padding_side\n        self.tokenizer.truncation_side = config.tokenizer.truncation_side\n        # FIXME: this may not be right and not general for all tokenizer.\n        if not self.tokenizer.pad_token:\n            self.tokenizer.pad_token = \"<|padding|>\"\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n        self.tokenizer.sep_token = \"<sep>\"\n    @abstractmethod\n    def learn(self):\n        raise NotImplementedError\n    @property\n    def unwrap_model(self):\n        # unwrap_model = self.model\n        unwrap_model = self.accelerator.unwrap_model(self.actor_critic_model)  # .module"
        },
        {
            "comment": "This code snippet is a part of the Trainer class in a deep learning model. It defines two methods - `unwrap_model` and `save_pretrained`. The `unwrap_model` method returns the unwrapped model, useful for saving or exporting purposes. The `save_pretrained` method saves the underlying Hugging Face model, tokenizer, and configuration files to a specified directory. If no directory is provided, it uses the checkpoint directory with a \"hf_model\" subdirectory. It also handles distributed training by waiting for all processes to complete before saving and providing necessary arguments to save the model correctly.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":57-78",
            "content": "        return unwrap_model\n    def save_pretrained(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Save the underlying Hugging Face model, tokenizer, and configuration files to a directory for\n        later use.\n        Args:\n            directory (str, *optional*): The directory to save the trainer files to.\n                NOTE: If not specified, the model will be saved to a directory named `hf_model` in the\n                checkpoint directory as specified by the Trainer's config.\n            **kwargs: Additional keyword arguments passed to the underlying Hugging Face model's\n                `save_pretrained` method.\n        \"\"\"\n        if directory is None:\n            directory = os.path.join(self.config.train.checkpoint_dir, \"hf_model\")\n        self.accelerator.wait_for_everyone()\n        self.accelerator.unwrap_model(self.model).save_pretrained(\n            directory,\n            save_function=self.accelerator.save,\n            is_main_process=self.accelerator.is_main_process,\n            state_dict=self.accelerator.get_state_dict(self.model),"
        },
        {
            "comment": "The code defines three methods for saving the trainer's state, optimizer, scheduler, and model. The `save` method creates a checkpoint in the specified directory or the default one defined in the configuration file. It uses accelerator's save_state function to save the state of the trainer. If PEFT (Parameter-Efficient Fine-Tuning) is enabled, it removes the \"pytorch_model.bin\" file before saving the model again to include only value heads. The `save_config` method saves the trainer's configuration file in the specified directory or default checkpoint directory.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":79-103",
            "content": "            **kwargs,\n        )\n        if self.accelerator.is_main_process:\n            self.tokenizer.save_pretrained(directory)\n    def save(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Creates a checkpoint of the optimizer, scheduler and model\"\"\"\n        dst_dir = directory or self.config.train.checkpoint_dir\n        self.accelerator.save_state(dst_dir, **kwargs)\n        if (\n            self.config.model.peft_config is not None\n            and self.accelerator.is_main_process\n        ):\n            # Remove \"pytorch_model.bin\" because it contains more than necessary,\n            # let save_pretrained recreate it with just the value heads.\n            model_file = os.path.join(dst_dir, \"pytorch_model.bin\")\n            if os.path.exists(model_file):\n                os.remove(model_file)\n            self.accelerator.unwrap_model(self.model).save_pretrained(dst_dir)\n    def save_config(self, directory: Optional[str] = None):\n        dst_dir = directory or self.config.train.checkpoint_dir\n        config_path = os.path.join(dst_dir, \"trainer_config.json\")"
        },
        {
            "comment": "The code saves the config to a specified path, loads checkpoints for optimizer, scheduler, and model from a given directory, and sets up a tracker. The code also includes a condition to only save the config if it's the main process, and it registers a load state pre-hook when using PEFT.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":104-130",
            "content": "        if self.accelerator.is_main_process:\n            json.dump(self.config.to_dict(), open(config_path, \"w\"), indent=2)\n            print_with_rank(\"Saving config to {}\".format(config_path))\n    def load(self, directory: Optional[str] = None, **kwargs):\n        \"\"\"Load checkpoint of optimizer, scheduler and a model\"\"\"\n        if self.config.model.peft_config is not None:\n            def load_state_hook(models: List[torch.nn.Module], input_dir: str):\n                with self.accelerator.main_process_first():\n                    for model in models:\n                        model.from_pretrained(input_dir)\n            self.accelerator.register_load_state_pre_hook(load_state_hook)\n            strict = False\n        else:\n            strict = True\n        self.accelerator.load_state(\n            directory or self.config.train.checkpoint_dir,\n            load_module_strict=strict,\n            **kwargs,\n        )\n    def setup_tracker(self):\n        script_name = os.path.basename(sys.argv[0]).rsplit(\".\", 1)[0]"
        },
        {
            "comment": "This code snippet is responsible for generating a run name and configuring distributed training. It checks the model path, number of GPUs, and retrieves the branch from git tag to construct the run name. If in the main process, it converts the configuration to dictionary form, gets distributed configuration, and adds it to the main configuration. Then it initializes trackers based on the specified tracker type (WandB in this case) with the generated run name, entity name, group name, and tags including git branch information.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":131-155",
            "content": "        if not isinstance(self.config.model.model_path, str):\n            model_name = str(self.config.model.model_path).split()[0]\n        else:\n            model_name = self.config.model.model_path.split(\"/\")[-1]\n        if self.accelerator.num_processes == 1:\n            num_gpus = \"1gpu\"\n        else:\n            num_gpus = f\"{self.accelerator.num_processes}gpus\"\n        branch = get_git_tag()[0]\n        run_name = \"/\".join([script_name, model_name, num_gpus]) + f\":{branch}\"\n        if self.accelerator.is_main_process:\n            config_dict = self.config.to_dict()\n            dist_config = get_distributed_config(self.accelerator)\n            config_dict[\"distributed\"] = dist_config\n            init_trackers_kwargs = {}\n            if self.config.train.tracker == \"wandb\":\n                init_trackers_kwargs[\"wandb\"] = {\n                    \"name\": run_name,\n                    \"entity\": self.config.train.entity_name,\n                    \"group\": self.config.train.group_name,\n                    \"tags\": self.config.train.tags + [\"/\".join(get_git_tag())],"
        },
        {
            "comment": "This code checks if the debug environment variable is set, and sets the mode accordingly. It then initializes trackers for training, either using the \"disabled\" or \"online\" mode based on the debug setting. If the tracker is set to \"tensorboard\", it flattens the config dictionary and handles the \"peft_config\" special case before initializing the trackers.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":156-176",
            "content": "                    \"mode\": \"disabled\" if os.environ.get(\"debug\", False) else \"online\",\n                }\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name,\n                    config=config_dict,\n                    init_kwargs=init_trackers_kwargs,\n                )\n            elif self.config.train.tracker == \"tensorboard\":\n                # flatten config for tensorboard, split list in hparams into flatten config\n                if config_dict[\"model\"].get(\n                    \"peft_config\", None\n                ):  # tensorboard does not support peft config type\n                    config_dict[\"model\"][\"peft_config\"] = str(\n                        config_dict[\"model\"][\"peft_config\"]\n                    )\n                config_dict_flat = flatten_dict(config_dict)\n                config_dict_flat[\"optimizer/kwargs/beta_1\"] = config_dict_flat[\n                    \"optimizer/kwargs/betas\"\n                ][0]\n                config_dict_flat[\"optimizer/kwargs/beta_2\"] = config_dict_flat["
        },
        {
            "comment": "This code initializes trackers for the trainer. If the config has a specified tracker (either \"wandb\" or \"tensorboard\"), it initializes those trackers. If no tracker is specified, it initializes default trackers. If an unsupported tracker is provided, it raises a ValueError with an appropriate error message. The model setup function returns an instance of the AutoModelForCausalLM class derived from the TRLConfig.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":177-200",
            "content": "                    \"optimizer/kwargs/betas\"\n                ][1]\n                config_dict_flat.pop(\"optimizer/kwargs/betas\", None)\n                for ix, tag in enumerate(config_dict_flat.pop(\"train/tags\")):\n                    config_dict_flat[f\"train/tag_{ix}\"] = tag\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name,\n                    config=config_dict_flat,\n                )\n            elif self.config.train.tracker is None:\n                self.accelerator.init_trackers(\n                    project_name=self.config.train.project_name\n                )\n            else:\n                raise ValueError(\n                    f\"Only supported trackers are `wandb` and `tensorboard`. Got: `{self.config.train.tracker}`. \"\n                    \"Set `tracker` to `None` to disable tracking.\"\n                )\n    def setup_model(self, model_class=AutoModelForCausalLM):\n        \"\"\"\n        Returns a model derived from an instance's TRLConfig\n        \"\"\""
        },
        {
            "comment": "This code snippet contains three methods in the base_trainer.py file: get_arch, setup_optimizer, and an unnamed method. The get_arch method is responsible for returning a model architecture based on the configuration provided. It handles backward compatibility by attempting to create a randomly initialized architecture if the model path is a pre-trained config. The setup_optimizer method returns an optimizer based on the trainer's TRLConfig. If a model path is specified, it will load a pre-existing model instead of initializing a new one.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/rl/trainer/base_trainer.py\":201-227",
            "content": "        model = self.get_arch(self.config, model_class)\n        if self.config.model.model_arch_type == \"seq2seq\":\n            raise NotImplementedError\n        return model\n    def get_arch(self, config, model_class=AutoModelForCausalLM):\n        from_fn = partial(\n            model_class.from_pretrained, cache_dir=self.config.model.cache_dir\n        )\n        # backward-compat: Try to create a randomly initialized architecture from a config\n        if issubclass(type(config.model.model_path), transformers.PretrainedConfig):\n            from_fn = model_class.from_config\n        return from_fn(config.model.model_path)\n    def setup_optimizer(self):\n        \"\"\"\n        Returns an optimizer derived from an instance's TRLConfig\n        \"\"\"\n        optimizer_class = get_optimizer_class(self.config.optimizer.name)\n        optimizer = optimizer_class(\n            self.model.parameters(),\n            **self.config.optimizer.kwargs,\n        )\n        return optimizer"
        }
    ]
}