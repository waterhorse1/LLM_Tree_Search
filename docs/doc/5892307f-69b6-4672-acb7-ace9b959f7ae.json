{
    "summary": "The code handles Reinforcement Learning for LLMs by setting up search parameters, models for text generation, rollout methods, token limits, reward calculations, and CotSC scenario comparisons. It also manages reward data structures, gathers results from multiple processes, and prints total results with time taken if the local rank is 0.",
    "details": [
        {
            "comment": "The code imports necessary libraries and modules for distributed computing, text generation, and model training. It also includes functions for MCTS rollout, value function estimation, and loading models. The code initializes the environment, sets up the model, and prepares for text generation and inference using MCTS algorithm. It handles data processing and loading of CT2 model, and includes utility functions for distributed learning.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":0-28",
            "content": "import os\nfrom pathlib import Path\nfrom typing import List, Optional\nimport torch.distributed as dist\nfrom tsllm.distributed.utils import print_with_rank, init_distributed, gather_scalar\nfrom transformers import AutoTokenizer, pipeline\nimport torch\nfrom functools import partial\nfrom tsllm.envs import get_env_datasets, get_default_query_str_builder\nfrom tsllm.envs.rlhf.prompt import PROBLEM_FORMAT_STR\nfrom tsllm.inference.trajectory_collector import _mcts_rollout_v1, _mcts_rollout_v2\nfrom tsllm.inference.value import value_fn\nimport json\nfrom tsllm.llm.ct2_utils import load_ct2_model\nfrom tsllm.mcts.utils import get_root\nfrom tsllm.model import load_critic_model\nfrom tsllm.model.modeling_actor_critic import AutoModelForCausalLMWithValueHead\nimport torch\nimport jsonlines\nfrom tsllm.llm.text_generation import llm_gen_ct2\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nfrom tsllm.mcts.tree import MCTS\nfrom dataclasses import dataclass\nfrom argparse import ArgumentParser\nimport ctranslate2\nfrom tsllm.inference.value import value_fn_rlhf"
        },
        {
            "comment": "This code defines a `SearchArgs` class with various parameters for tree search methods in Reinforcement Learning. Parameters include temperature, aggregation, max length, exploration rate, rollout method, and more. It also includes options for pruning nodes under a certain value, resetting or clearing the tree between simulations, and setting hyperparameters for MCTS-alpha and MCT-rollout.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":30-56",
            "content": "import time\nimport importlib\n@dataclass\nclass SearchArgs:\n    temperature: float = 1.0 # sampling temperature\n    num_mcts_aggregation: int = 5 # how many trajectories to sample\n    max_length: int = 8 # max depth of tree\n    pb_c_init: float = 10 # for mcts exploration\n    # init_critic_value: bool = True # whether we use value function to initialize the tree node value\n    rollout_method: str = None # which tree-search method we use\n    # mask_no_terminal_reward_value: bool = False # whether we mask the non-terminal value\n    prune_node_under_v: Optional[float] = None\n    num_simulations: int = 10 # parameter for mcts-alpha\n    # aggregation parameters\n    reset_total_tree: bool = False # intra-tree\n    clear_total_tree: bool = False # inter-tree\n    mcts_sample: bool = False # whether to use sample in mcts-alpha\n    max_simulation: Optional[int] = None # hyperparameter for mcts-alpha\n    max_token: Optional[int] = None # hyperparameter for mct-rollout\n    # DFS hyper parameter\n    prune_ratio: Optional[float] = 0.7"
        },
        {
            "comment": "This code initializes various optional parameters for the task at hand. It includes parameters like prune_value, select_by_prior, k_maj, and max_new_tokens. The code also reads in environment variables to determine which test to run and what arguments are required. Finally, it parses these arguments using an ArgumentParser.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":57-82",
            "content": "    prune_value: Optional[float] = None\n    # COT-SC-Tree equals to RAP with select_by_prior=True\n    select_by_prior: bool = False\n    # COT-SC number\n    k_maj: int = 100\n    max_new_tokens: int = 64\ninput_prompt_format = PROBLEM_FORMAT_STR\nif __name__ == \"__main__\":\n    TEST_NO_TERMINAL = int(os.getenv(\"TEST_NO_TERMINAL\", 0))\n    TEST_WITH_TERMINAL = int(os.getenv(\"TEST_WITH_TERMINAL\", 0))\n    TEST_COT_GREEDY = int(os.getenv(\"TEST_COT_GREEDY\", 0))\n    TEST_COT_SC = int(os.getenv(\"TEST_COT_SC\", 0))\n    assert TEST_NO_TERMINAL + TEST_WITH_TERMINAL + TEST_COT_SC + TEST_COT_GREEDY > 0\n    parser = ArgumentParser()\n    parser.add_argument(\"--ct2_dir\", type=str, required=True)\n    parser.add_argument(\"--tokenizer_path\", type=str, required=True)\n    parser.add_argument(\"--critic_model_path\",type=str, required=True)\n    parser.add_argument(\"--save_dir\", type=str, required=True)\n    parser.add_argument(\"--env_name\", type=str, required=True)\n    parser.add_argument(\"--dataset_name\", type=str, default=\"Dahoas/synthetic-instruct-gptj-pairwise\")"
        },
        {
            "comment": "The code snippet is setting up arguments and configurations for an Reinforcement Learning model. It initializes a parser to handle command line arguments, creates a list of configuration settings for the model, imports a task module, defines a save directory, initializes distributed processes, and specifies a dataset path and number of training data for a specific environment (\"rlhf\").",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":83-116",
            "content": "    parser.add_argument(\"--train\", action='store_true', default=False)\n    config = parser.parse_args()\n    args_list = [\n        {\n            \"temperature\": 1.0,\n            \"max_length\": 64,\n            \"pb_c_init\": 3,\n            \"num_simulations\": 10,\n            \"k_maj\": 5,\n            \"num_mcts_aggregation\": 1,\n            \"max_simulation\": None,\n            \"max_token\": 5000,\n            \"reset_total_tree\": False,\n            \"clear_total_tree\": True,\n            \"rollout_method\": \"mcts.get_next_action\",\n            \"select_by_prior\": False,\n            \"max_new_tokens\": 64,\n            \"mcts_sample\": False,\n            \"prune_ratio\": 0.9,\n            \"prune_value\": None\n        }\n    ]\n    task_module = importlib.import_module(f\"tsllm.envs.{config.env_name}\")\n    save_dir = Path(config.save_dir) / config.env_name\n    local_rank, world_size = init_distributed()\n    if config.env_name == \"rlhf\":\n        task_dataset_kwargs = {\"path\": config.dataset_name, 'num_train_data': 30000}\n        #assert config.train"
        },
        {
            "comment": "This code is creating a dataset for the task, determining if it should use the training or testing set based on config.train, and initializing the tokenizer, reward model, and reward function. The reward model takes in a question and answer pair as input and outputs a score. This score can be used to evaluate the quality of the generated answers.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":117-137",
            "content": "        #task_dataset_kwargs = {\"path\": config.dataset_name, 'train_data_pre': 22500, 'train_data_post':30000}\n    else:\n        task_dataset_kwargs = {}\n    train_ds, test_ds = get_env_datasets(config.env_name, **task_dataset_kwargs)\n    if config.train:\n        print_with_rank('Use training set')\n        test_ds = train_ds\n    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)\n    tokenizer.eos_token = \"### End\"\n    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n    reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n    rank_model, reward_tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name), AutoTokenizer.from_pretrained(reward_name)\n    rank_model = rank_model.bfloat16().to(f\"cuda:{local_rank}\")\n    @torch.inference_mode()\n    def reward_fn(question, answer):    \n        inputs = reward_tokenizer(question, answer, return_tensors='pt').to(f\"cuda:{local_rank}\")\n        score = rank_model(**inputs).logits[0].cpu().item()"
        },
        {
            "comment": "This code initializes a critic model if the config specifies a model path, otherwise it sets it to None. It also initializes a policy forward value based on whether there is a critic model or not. The code then proceeds to convert the model to CT2 files using ctranslate2 generator and defines prompt and cot_direct_output functions for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":138-165",
            "content": "        return score\n    if not config.critic_model_path == 'None':\n        critic = AutoModelForCausalLMWithValueHead.from_pretrained(config.critic_model_path).to(f\"cuda:{local_rank}\").bfloat16()\n        policy_forward_value = partial(value_fn_rlhf, critic, tokenizer)\n    else:\n        critic = None\n        policy_forward_value = None\n    ############ CONVERT MODEL to CT2 files ###################\n    dist.barrier()\n    # PS\uff1a\u5982\u679cconvert\u597d\u4e86\uff0c\u4e0a\u9762\u4e24\u6b65\u90fd\u53ef\u4ee5\u8df3\u8fc7\n    ################ LOAD CT2 model ####################\n    # ct2_generator = ctranslate2.Generator(ct2_dir,\n    #                                       )\n    # ct2_sp = spm.SentencePieceProcessor(os.path.join(ct2_dir, \"tokenizer.model\"))\n    ct2_generator = ctranslate2.Generator(\n        config.ct2_dir, device=\"cuda\", device_index=local_rank, compute_type=\"float32\"\n    )\n    def prompt_fn(problem_input: str):\n        return input_prompt_format.format(question=problem_input)\n    def cot_direct_output(args, problem_inst, stop, **kwargs):\n        prompt = prompt_fn(problem_inst[\"question\"])"
        },
        {
            "comment": "The code snippet is from a machine learning model and appears to be responsible for generating text using LLM (Language Model) and evaluating the quality of generated outputs by applying a reward function. The code includes functions llm_gen_ct2, cot_sc_output, and llm_forward_fn which are used for text generation, output evaluation, and LLM forward pass respectively. The tokenizer is used to convert text into numerical sequences for model input. The reward_fn is applied to the generated texts and their corresponding prompts to evaluate the quality of each output. The max_batch_size parameter in cot_sc_output function limits the number of texts generated at a time.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":166-196",
            "content": "        texts, logps = llm_gen_ct2(\n            ct2_generator,\n            tokenizer,\n            static_prompt=None,\n            prompt=prompt,\n            num_sequence=1,\n            stop=tokenizer.eos_token_id,\n            **kwargs,\n        )\n        reward_list = [reward_fn(prompt, txt) for txt in texts]\n        return reward_list[0], list(zip(texts, reward_list))\n    def cot_sc_output(args, problem_inst, stop, **kwargs):\n        prompt = prompt_fn(problem_inst[\"question\"])\n        texts, logps = llm_gen_ct2(\n            ct2_generator,\n            tokenizer,\n            static_prompt=None,\n            prompt=prompt,\n            num_sequence=args.k_maj,\n            stop=tokenizer.eos_token_id,\n            max_batch_size=50,\n            **kwargs,\n        )\n        reward_list = [reward_fn(prompt, txt) for txt in texts]\n        judge_results = max(reward_list)\n        return judge_results, list(zip(texts, reward_list))\n    def llm_forward_fn():\n        from llm.text_generation import llm_forward_ct2\n        llm_gen_v2 = partial(llm_forward_ct2, ct2_generator, tokenizer)"
        },
        {
            "comment": "This code defines a function that performs MCTS search, initializes an environment for the given problem instance, and sets up an MCTS object with specified parameters. The environment is initialized with the provided question from the problem instance, using the given LLM forward function, tokenizer, and reward function. If the rollout method is set to \"mcts.rollout\", it uses the MCTS object for rollouts.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":197-228",
            "content": "        return llm_gen_v2\n    def mcts_multi_search(args: \"SearchArgs\", problem_inst, no_terminal_reward=True):\n        env = task_module.Env(\n            config={\n                \"max_actions\": 50,\n                \"sep\": \"\",\n                \"max_length\": args.max_length,\n                \"temperature\": args.temperature,\n            },\n            problems=[\n                {\n                    \"question\": problem_inst[\"question\"],\n                }\n            ],\n            llm_forward_fn=llm_forward_fn(),\n            tokenizer=tokenizer,\n            reward_fn=reward_fn,\n        )\n        # llm_gen_fn=partial(llm_gen_with_logp_v1, model, tokenizer),\n        mcts = MCTS(\n            cfg={\n                \"num_simulations\": args.num_simulations,\n                \"pb_c_base\": 19652,\n                \"pb_c_init\": args.pb_c_init,\n                \"root_dirichlet_alpha\": 0.3,\n                \"root_noise_weight\": 0.25,\n                \"no_terminal_reward\": no_terminal_reward,\n            }\n        )\n        if args.rollout_method == \"mcts.rollout\":"
        },
        {
            "comment": "The code checks the rollout method and performs a MCTS rollout or uses mcts.get_next_action to get output. It also handles maximum token and simulation settings, and can clear the total tree if needed. The prompt function is used to generate text for evaluation.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":229-254",
            "content": "            assert args.max_token is not None and args.max_simulation is None\n            output_list, _, _ = mcts.rollout(\n                env,\n                args.num_mcts_aggregation,\n                policy_forward_value,\n                max_num_simulation=args.max_simulation,\n                max_token=args.max_token,\n                return_tree=True,\n            )\n        elif args.rollout_method == \"mcts.get_next_action\":\n            output_list = _mcts_rollout_v1(\n                mcts,\n                env,\n                policy_forward_value,\n                args.num_mcts_aggregation,\n                args.reset_total_tree,\n                sample=args.mcts_sample,\n                clear_total_tree=args.clear_total_tree\n            )\n            prompt = prompt_fn(problem_inst[\"question\"])\n            # if len(texts) > 0:\n            #     value_list = policy_forward_value(\n            #         [prompt + txt + env.sep for txt in texts]\n            #     ).tolist()\n            # else:\n            #     value_list = []"
        },
        {
            "comment": "This code snippet handles different rollout methods for a reinforcement learning algorithm. If \"mcts.rap\" is specified, it runs the rapid action planning method. If \"mcts.beam_search\" is given, it executes the beam search algorithm. If \"mcts.dfs\" is set, it performs depth-first search. For any other unrecognized rollout method, it raises a ValueError. Finally, it extracts the text outputs and prompts for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":255-279",
            "content": "        elif args.rollout_method == \"mcts.rap\":\n            output_list = mcts.rap(\n                env,\n                args.num_mcts_aggregation,\n                policy_forward_value,\n                args.select_by_prior,\n            )\n        elif args.rollout_method == \"mcts.beam_search\":\n            output_list = mcts.beam_search(\n                env, args.num_mcts_aggregation, args.max_length, policy_forward_value\n            )\n        elif args.rollout_method == \"mcts.dfs\":\n            # here the num_mcts_aggregation is the step_limit which indicate how many nodes \n            # will be visited in the tree.\n            output_list = mcts.dfs(\n                env, \n                args.num_mcts_aggregation, \n                policy_forward_value, \n                prune_value=args.prune_value,\n                prune_ratio=args.prune_ratio\n            )\n        else:\n            raise ValueError(\"Unknow rollout method: {}\".format(args.rollout_method))\n        texts = [o[\"text\"] for o in output_list]\n        prompt = prompt_fn(problem_inst[\"question\"])"
        },
        {
            "comment": "The code initializes a reward list, handles cases with no rewards, calculates maximum reward, counts tokens, and returns an MCTS object, judge results, and output list. It also defines a save function for storing problem results.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":280-311",
            "content": "        reward_list = [reward_fn(prompt, txt) for txt in texts]\n        judge_results = {}\n        if len(reward_list) == 0:\n            # default reward as -1\n            assert args.rollout_method == \"mcts.rollout\"\n            reward_list = [-1] \n        judge_results[f\"{args.rollout_method}@agg_{args.num_mcts_aggregation}\"] = max(reward_list)\n        if output_list and args.rollout_method != \"mcts.rollout\":\n            num_token = output_list[-1][\"num_generated_token\"]\n        else:\n            num_token = mcts.num_generated_token\n        judge_results[\"#token\"] = num_token\n        return mcts, judge_results, output_list\n    def test_problem(\n        args,\n        idx,\n        problem_inst,\n        cot_writer,\n        cot_sc_writer,\n        mcts_no_term_writer,\n        mcts_w_term_writer,\n    ):\n        results = {}\n        def save_fn(writer, output, result: dict):\n            if writer is not None:\n                writer.write(\n                    {\n                        \"i\": idx,\n                        \"prompt\": problem_inst[\"question\"],"
        },
        {
            "comment": "This code snippet is part of a larger program for reinforcement learning and testing. It tests the model using different approaches such as COT-greedy and with terminal states, saving the results accordingly. The results are then added to the \"results\" dictionary for further analysis or processing.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":312-338",
            "content": "                        \"output\": output,\n                        \"result\": result\n                    }\n                )\n        if TEST_COT_GREEDY:\n            r_cot_greedy, cot_episodes = cot_direct_output(\n                args,\n                problem_inst,\n                stop=tokenizer.eos_token_id,\n                max_new_tokens=args.max_new_tokens,\n                temperature=args.temperature,\n                top_k=1,\n            )\n            save_fn(cot_writer, cot_episodes, r_cot_greedy)\n            results[\"cot-greedy\"] = r_cot_greedy\n        if TEST_WITH_TERMINAL:\n            mcts, r_with_terminal, with_terminal_episodes = mcts_multi_search(\n                args, problem_inst, False\n            )\n            # save_tree_path = save_dir / f\"tmp_tree\"\n            # if not save_tree_path.exists():\n            #     save_tree_path.mkdir(parents=True)\n            # json.dump(\n            #     get_root(mcts.root).to_json(),\n            #     open(save_tree_path / f\"{idx}_r{r_with_terminal}.json\", \"w\"),"
        },
        {
            "comment": "This code is defining a function that evaluates the performance of an LLM (Language Model) in a task involving offline reinforcement learning. It measures the effectiveness with and without terminal episodes, as well as incorporating a special test for the CotSC (Cotton Sequence Claim) scenario. The results are then returned and formatted for clarity.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":339-368",
            "content": "            #     indent=2,\n            # )\n            save_fn(mcts_w_term_writer, with_terminal_episodes, r_with_terminal)\n            results[\"w/-terminal\"] = r_with_terminal\n        if TEST_COT_SC:\n            r_cot_sc, cot_sc_episodes = cot_sc_output(\n                args,\n                problem_inst,\n                stop=tokenizer.eos_token_id,\n                temperature=args.temperature,\n                max_new_tokens=args.max_new_tokens,\n                top_k=50,\n            )\n            save_fn(cot_sc_writer, cot_sc_episodes, r_cot_sc)\n            results[\"cot-sc\"] = r_cot_sc\n        return results\n    def _result_str(results, cnt, join_str=\"\\n\"):\n        res = \"\"\n        for k, v in results.items():\n            if isinstance(v, float):\n                res += f\"{k}: {v/cnt}\"\n            elif isinstance(v, dict):\n                res += f\"{k}: \"\n                res += \", \".join(\n                    [\n                        (\n                            f\"{sub_k}: {sub_v/cnt:.2f}\"\n                            if sub_k == \"#token\""
        },
        {
            "comment": "This code performs a search using specified arguments, handles MCTS sample mode, saves search arguments and possibly Cot information to designated directories for later use.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":369-397",
            "content": "                            else f\"{sub_k}: {sub_v/cnt}\"\n                        )\n                        for sub_k, sub_v in v.items()\n                    ]\n                )\n            else:\n                raise ValueError\n            res += join_str\n        res += f\"cnt: {cnt}\"\n        return res\n    for i_arg, cur_args in enumerate(args_list):\n        args = SearchArgs(**cur_args)\n        if args.mcts_sample:\n            print_with_rank('Use mcts sample mode, Make sure you are using code to generate rollout instead of test')\n        writer_dir = save_dir / (f\"args{i_arg}/\")\n        if local_rank == 0:\n            print(\"Search args: {}\".format(args))\n            if not writer_dir.exists():\n                writer_dir.mkdir(parents=True)\n            json.dump(cur_args, open(writer_dir / \"args.json\", \"w\"))\n        if TEST_COT_GREEDY:\n            cot_save_path = writer_dir / \"cot\"\n            if local_rank == 0 and not cot_save_path.exists():\n                cot_save_path.mkdir(parents=True)\n            dist.barrier()"
        },
        {
            "comment": "This code is setting up file writers for various test results. It creates a writer if the corresponding TEST flag is set, and sets it to None otherwise. The code also ensures that directories are created if they don't already exist, and uses distributed barriers in some cases.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":398-424",
            "content": "            cot_writer = jsonlines.open(cot_save_path / f\"{local_rank}.jsonl\", \"a\")\n        else:\n            cot_writer = None\n        if TEST_COT_SC:\n            cot_sc_save_path = writer_dir / \"cot_sc\"\n            if local_rank == 0 and not cot_sc_save_path.exists():\n                cot_sc_save_path.mkdir(parents=True)\n            dist.barrier()\n            cot_sc_writer = jsonlines.open(\n                cot_sc_save_path / f\"{local_rank}.jsonl\", \"a\"\n            )\n        else:\n            cot_sc_writer = None\n        if TEST_NO_TERMINAL:\n            mcts_no_term_save_path = writer_dir / \"no_terminal_reward\"\n            if local_rank == 0 and not mcts_no_term_save_path.exists():\n                mcts_no_term_save_path.mkdir(parents=True)\n            dist.barrier()\n            mcts_no_term_writer = jsonlines.open(\n                mcts_no_term_save_path / f\"{local_rank}.jsonl\", \"a\"\n            )\n        else:\n            mcts_no_term_writer = None\n        if TEST_WITH_TERMINAL:\n            mcts_w_term_save_path = writer_dir / \"with_terminal_reward\""
        },
        {
            "comment": "This code sets up the mcts_w_term_writer for saving game data and initializes a reward_dict. It then iterates through test_ds range, calling the test_problem function on each iteration. The results from this function are added to the reward_dict if they're floats.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":425-452",
            "content": "            if local_rank == 0 and not mcts_w_term_save_path.exists():\n                mcts_w_term_save_path.mkdir(parents=True)\n            dist.barrier()\n            mcts_w_term_writer = jsonlines.open(\n                mcts_w_term_save_path / f\"{local_rank}.jsonl\", \"a\"\n            )\n        else:\n            mcts_w_term_writer = None\n        cnt = 0\n        reward_dict = dict()\n        t0 = time.time()\n        for i in (pbar := tqdm(range(len(test_ds)), disable=(local_rank != 0))):\n            if i % world_size == local_rank:\n                results = test_problem(\n                    args,\n                    i,\n                    test_ds[i],\n                    cot_writer,\n                    cot_sc_writer,\n                    mcts_no_term_writer,\n                    mcts_w_term_writer,\n                )\n                for k, v in results.items():\n                    if isinstance(v, float):\n                        if k not in reward_dict:\n                            reward_dict[k] = 0\n                        reward_dict[k] += v"
        },
        {
            "comment": "This code handles rewards data structure. It checks the type of each value in the reward dictionary, adding it to the appropriate key or creating new keys as needed. It also handles gathering and summarizing results across multiple processes for accurate calculations.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":453-477",
            "content": "                    elif isinstance(v, dict):\n                        if k not in reward_dict:\n                            reward_dict[k] = dict()\n                        for sub_k, sub_v in v.items():\n                            if sub_k not in reward_dict[k]:\n                                reward_dict[k][sub_k] = 0\n                            reward_dict[k][sub_k] += sub_v\n                    else:\n                        raise NotImplementedError\n                cnt += 1\n                results_strs = _result_str(reward_dict, cnt, join_str=\"; \")\n                pbar.set_description(results_strs)\n        print_with_rank(results_strs)\n        cnt_list = gather_scalar(cnt, local_rank, world_size)\n        gathered_results = {}\n        for k, v in reward_dict.items():\n            if isinstance(v, float):\n                gathered_list = gather_scalar(v, local_rank, world_size)\n                if local_rank == 0:\n                    gathered_results[k] = sum(gathered_list)\n            elif isinstance(v, dict):"
        },
        {
            "comment": "This code gathers results from multiple processes, sums them if the local rank is 0, and prints the total results with time taken. It uses `gather_scalar` to collect float values from different sub-values in a dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/LLM_Tree_Search/docs/src/tsllm/offline_rl/test_sft_and_v_rlhf.py\":478-493",
            "content": "                gathered_results[k] = {}\n                for sub_k, sub_v in v.items():\n                    gathered_list = gather_scalar(float(sub_v), local_rank, world_size)\n                    if local_rank == 0:\n                        gathered_results[k][sub_k] = sum(gathered_list)\n            else:\n                raise ValueError\n        if local_rank == 0:\n            total_cnt = sum(cnt_list)\n            t1 = time.time()\n            total_results_strs = _result_str(gathered_results, total_cnt)\n            print(cur_args)\n            print(\"TOTAL RESULTS:\\n\", total_results_strs)\n            print(\"Time: {}\".format(t1 - t0))"
        }
    ]
}